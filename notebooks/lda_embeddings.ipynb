{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/daniil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from navec import Navec\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from deepxml.dataset import MultiLabelDataset\n",
    "from deepxml.evaluation import get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10\n",
    "from deepxml.models import Model\n",
    "\n",
    "from models.preprocessing import texts_preprocessing\n",
    "from models.lda_correction_network import LDACorrectionNet, CorNetLDACorrectionNet, LDACorrectionNetLarge, CorNetLDACorrectionNetLarge\n",
    "from models.lda_encoders import LDAEmbeddings\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/habr_posts_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807711</td>\n",
       "      <td>Kaspersky_Lab</td>\n",
       "      <td>Security Week 2416: уязвимость в серверных мат...</td>\n",
       "      <td>[Блог компании «Лаборатория Касперского», Инфо...</td>\n",
       "      <td>На прошлой неделе исследователи компании Binar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>807709</td>\n",
       "      <td>markshevchenko</td>\n",
       "      <td>Вычислительные выражения: Подробнее про типы-о...</td>\n",
       "      <td>[.NET, Функциональное программирование, F#]</td>\n",
       "      <td>В предыдущем посте мы познакомились с концепци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>807707</td>\n",
       "      <td>ru_vds</td>\n",
       "      <td>Угадай местоположение льдины с арктическим ЦОД...</td>\n",
       "      <td>[Блог компании RUVDS.com, Хостинг, Системное а...</td>\n",
       "      <td>Как вы наверняка знаете, 12 апреля RUVDS успеш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>807705</td>\n",
       "      <td>shaddyk</td>\n",
       "      <td>Запустили проект с НСИС по повышению качества ...</td>\n",
       "      <td>[Блог компании HFLabs, Открытые данные, IT-ком...</td>\n",
       "      <td>НСИС — оператор единой автоматизированной инфо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>807703</td>\n",
       "      <td>VokaMut</td>\n",
       "      <td>Тестируем AI на создании прикладного приложения</td>\n",
       "      <td>[Веб-разработка, Искусственный интеллект, Natu...</td>\n",
       "      <td>Всем привет, я Григорий Тумаков, CTO в Моризо ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id          author                                              title  \\\n",
       "0   807711   Kaspersky_Lab  Security Week 2416: уязвимость в серверных мат...   \n",
       "1   807709  markshevchenko  Вычислительные выражения: Подробнее про типы-о...   \n",
       "2   807707          ru_vds  Угадай местоположение льдины с арктическим ЦОД...   \n",
       "3   807705         shaddyk  Запустили проект с НСИС по повышению качества ...   \n",
       "4   807703         VokaMut    Тестируем AI на создании прикладного приложения   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [Блог компании «Лаборатория Касперского», Инфо...   \n",
       "1        [.NET, Функциональное программирование, F#]   \n",
       "2  [Блог компании RUVDS.com, Хостинг, Системное а...   \n",
       "3  [Блог компании HFLabs, Открытые данные, IT-ком...   \n",
       "4  [Веб-разработка, Искусственный интеллект, Natu...   \n",
       "\n",
       "                                                text  \n",
       "0  На прошлой неделе исследователи компании Binar...  \n",
       "1  В предыдущем посте мы познакомились с концепци...  \n",
       "2  Как вы наверняка знаете, 12 апреля RUVDS успеш...  \n",
       "3  НСИС — оператор единой автоматизированной инфо...  \n",
       "4  Всем привет, я Григорий Тумаков, CTO в Моризо ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = texts_preprocessing(train_df[\"text\"].to_list())\n",
    "val_texts = texts_preprocessing(val_df[\"text\"].to_list())\n",
    "test_texts = texts_preprocessing(test_df[\"text\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/PapersTagsPrediction/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['Haskell', 'I2P', 'MongoDB', 'MySQL', 'NestJS', 'Блог компании Garage Eight', 'Блог компании RDP', 'Блог компании SL Soft', 'Блог компании SOFTPOINT', 'Блог компании VAS Experts', 'Блог компании Xeovo VPN', 'Блог компании Леруа Мерлен', 'Блог компании ООО «СМАРТС-Кванттелеком»', 'Блог компании Самолет', 'Блог компании Страховой Дом ВСК', 'Кодобред', 'Медгаджеты'] will be ignored\n",
      "  warnings.warn(\n",
      "/home/daniil/programming/PapersTagsPrediction/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['ASP', 'CGI (графика)', 'IIS', 'Lua', 'MongoDB', 'MySQL', 'UEFI', 'Блог компании DataLine', 'Блог компании Deiteriy Lab', 'Блог компании Garage Eight', 'Блог компании Headz.io', 'Блог компании ITT Solutions', 'Блог компании Monq', 'Блог компании PERCo', 'Блог компании Sapiens solutions', 'Блог компании Start X (EX Антифишинг)', 'Блог компании documentat.io', 'Блог компании Ænix', 'Блог компании АйПиМатика', 'Блог компании ЕАЕ-Консалт', 'Блог компании ИТМО', 'Блог компании Лига Цифровой Экономики', 'Блог компании МосТрансПроект', 'Блог компании Окама', 'Блог компании РТЛабс', 'Блог компании Самолет', 'Блог компании Северсталь', 'Блог компании Ситидрайв', 'Блог компании Сравни', 'Блог компании Страховой Дом ВСК', 'Верстка писем', 'Графические оболочки', 'Медгаджеты', 'Типографика'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "train_labels = mlb.fit_transform(train_df[\"tags\"].to_list())\n",
    "val_labels = mlb.transform(val_df[\"tags\"].to_list())\n",
    "test_labels = mlb.transform(test_df[\"tags\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Embeddings (num_topic=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_embs = LDAEmbeddings(train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 37s, sys: 14min 8s, total: 22min 45s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_embs = lda_embs.fit_transform(train_texts)\n",
    "val_embs = lda_embs.transform(val_texts)\n",
    "test_embs = lda_embs.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MultiLabelDataset(train_embs, train_labels),\n",
    "                          8, shuffle=True)\n",
    "val_loader = DataLoader(MultiLabelDataset(val_embs, val_labels),\n",
    "                          8, shuffle=False)\n",
    "test_loader = DataLoader(MultiLabelDataset(test_embs, test_labels),\n",
    "                          8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=LDACorrectionNet,\n",
    "              emb_size=300, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/PapersTagsPrediction/src/deepxml/optimizers.py:97: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1578.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0937443 valid loss: 0.0331649 P@5: 0.07286 N@5: 0.10954 early stop: 0\n",
      "0 1600 train loss: 0.0317167 valid loss: 0.0307127 P@5: 0.07371 N@5: 0.10701 early stop: 0\n",
      "0 2400 train loss: 0.0297573 valid loss: 0.0301985 P@5: 0.08114 N@5: 0.11558 early stop: 0\n",
      "1 400 train loss: 0.0302892 valid loss: 0.0299662 P@5: 0.08114 N@5: 0.11212 early stop: 0\n",
      "1 1200 train loss: 0.0301741 valid loss: 0.0300037 P@5: 0.06229 N@5: 0.09790 early stop: 0\n",
      "1 2000 train loss: 0.0293873 valid loss: 0.0299897 P@5: 0.07514 N@5: 0.11006 early stop: 0\n",
      "1 2800 train loss: 0.0300107 valid loss: 0.0299394 P@5: 0.07257 N@5: 0.10384 early stop: 0\n",
      "2 800 train loss: 0.0297223 valid loss: 0.0299269 P@5: 0.07629 N@5: 0.11371 early stop: 0\n",
      "2 1600 train loss: 0.0296573 valid loss: 0.0299116 P@5: 0.07686 N@5: 0.11344 early stop: 0\n",
      "2 2400 train loss: 0.0301420 valid loss: 0.0299866 P@5: 0.08343 N@5: 0.11638 early stop: 0\n",
      "3 400 train loss: 0.0297882 valid loss: 0.0299826 P@5: 0.07600 N@5: 0.11478 early stop: 0\n",
      "3 1200 train loss: 0.0303319 valid loss: 0.0299018 P@5: 0.07371 N@5: 0.10723 early stop: 0\n",
      "3 2000 train loss: 0.0304138 valid loss: 0.0300059 P@5: 0.07486 N@5: 0.09711 early stop: 0\n",
      "3 2800 train loss: 0.0291087 valid loss: 0.0298659 P@5: 0.08314 N@5: 0.12011 early stop: 0\n",
      "4 800 train loss: 0.0299391 valid loss: 0.0299617 P@5: 0.07914 N@5: 0.12103 early stop: 0\n",
      "4 1600 train loss: 0.0297878 valid loss: 0.0299160 P@5: 0.07114 N@5: 0.10793 early stop: 0\n",
      "4 2400 train loss: 0.0297833 valid loss: 0.0300425 P@5: 0.07114 N@5: 0.10765 early stop: 0\n",
      "5 400 train loss: 0.0296118 valid loss: 0.0299073 P@5: 0.08514 N@5: 0.11221 early stop: 0\n",
      "5 1200 train loss: 0.0293579 valid loss: 0.0299842 P@5: 0.07400 N@5: 0.11215 early stop: 0\n",
      "5 2000 train loss: 0.0303234 valid loss: 0.0298967 P@5: 0.07971 N@5: 0.11516 early stop: 0\n",
      "5 2800 train loss: 0.0303771 valid loss: 0.0300478 P@5: 0.07200 N@5: 0.10765 early stop: 0\n",
      "6 800 train loss: 0.0296371 valid loss: 0.0298575 P@5: 0.07314 N@5: 0.10964 early stop: 0\n",
      "6 1600 train loss: 0.0301173 valid loss: 0.0300308 P@5: 0.06971 N@5: 0.09632 early stop: 0\n",
      "6 2400 train loss: 0.0295845 valid loss: 0.0300174 P@5: 0.07857 N@5: 0.11674 early stop: 0\n",
      "7 400 train loss: 0.0303452 valid loss: 0.0298845 P@5: 0.08143 N@5: 0.11602 early stop: 0\n",
      "7 1200 train loss: 0.0296484 valid loss: 0.0299956 P@5: 0.07629 N@5: 0.11514 early stop: 0\n",
      "7 2000 train loss: 0.0298717 valid loss: 0.0298221 P@5: 0.07657 N@5: 0.10861 early stop: 0\n",
      "7 2800 train loss: 0.0296621 valid loss: 0.0298149 P@5: 0.09286 N@5: 0.13088 early stop: 0\n",
      "8 800 train loss: 0.0296446 valid loss: 0.0299555 P@5: 0.07200 N@5: 0.10860 early stop: 0\n",
      "8 1600 train loss: 0.0296023 valid loss: 0.0299434 P@5: 0.08571 N@5: 0.11814 early stop: 0\n",
      "8 2400 train loss: 0.0299689 valid loss: 0.0297500 P@5: 0.08600 N@5: 0.13023 early stop: 0\n",
      "9 400 train loss: 0.0295011 valid loss: 0.0297836 P@5: 0.07857 N@5: 0.11904 early stop: 0\n",
      "9 1200 train loss: 0.0294335 valid loss: 0.0297819 P@5: 0.07914 N@5: 0.11957 early stop: 0\n",
      "9 2000 train loss: 0.0298235 valid loss: 0.0297217 P@5: 0.09571 N@5: 0.12837 early stop: 0\n",
      "9 2800 train loss: 0.0297195 valid loss: 0.0297344 P@5: 0.09514 N@5: 0.12908 early stop: 0\n",
      "10 800 train loss: 0.0299243 valid loss: 0.0296227 P@5: 0.09914 N@5: 0.14128 early stop: 0\n",
      "10 1600 train loss: 0.0299945 valid loss: 0.0296566 P@5: 0.09000 N@5: 0.13045 early stop: 0\n",
      "10 2400 train loss: 0.0285707 valid loss: 0.0296396 P@5: 0.07771 N@5: 0.11690 early stop: 0\n",
      "11 400 train loss: 0.0290571 valid loss: 0.0295883 P@5: 0.08771 N@5: 0.13496 early stop: 0\n",
      "11 1200 train loss: 0.0293513 valid loss: 0.0294879 P@5: 0.08457 N@5: 0.12807 early stop: 0\n",
      "11 2000 train loss: 0.0297243 valid loss: 0.0294603 P@5: 0.10086 N@5: 0.15040 early stop: 0\n",
      "11 2800 train loss: 0.0293742 valid loss: 0.0294747 P@5: 0.09371 N@5: 0.15245 early stop: 0\n",
      "12 800 train loss: 0.0291101 valid loss: 0.0294976 P@5: 0.08943 N@5: 0.14114 early stop: 0\n",
      "12 1600 train loss: 0.0291679 valid loss: 0.0292736 P@5: 0.11571 N@5: 0.18160 early stop: 0\n",
      "12 2400 train loss: 0.0289559 valid loss: 0.0292116 P@5: 0.11229 N@5: 0.17642 early stop: 0\n",
      "13 400 train loss: 0.0289432 valid loss: 0.0290190 P@5: 0.11200 N@5: 0.17094 early stop: 0\n",
      "13 1200 train loss: 0.0282556 valid loss: 0.0290502 P@5: 0.10714 N@5: 0.16730 early stop: 0\n",
      "13 2000 train loss: 0.0288137 valid loss: 0.0291218 P@5: 0.11343 N@5: 0.18523 early stop: 0\n",
      "13 2800 train loss: 0.0294025 valid loss: 0.0290316 P@5: 0.10971 N@5: 0.17166 early stop: 0\n",
      "14 800 train loss: 0.0279641 valid loss: 0.0288433 P@5: 0.11857 N@5: 0.19026 early stop: 0\n",
      "14 1600 train loss: 0.0289777 valid loss: 0.0287758 P@5: 0.12486 N@5: 0.18091 early stop: 0\n",
      "14 2400 train loss: 0.0287980 valid loss: 0.0287075 P@5: 0.12200 N@5: 0.20455 early stop: 0\n",
      "15 400 train loss: 0.0279217 valid loss: 0.0289390 P@5: 0.11829 N@5: 0.18078 early stop: 0\n",
      "15 1200 train loss: 0.0284492 valid loss: 0.0285382 P@5: 0.11229 N@5: 0.18761 early stop: 0\n",
      "15 2000 train loss: 0.0282493 valid loss: 0.0286848 P@5: 0.10657 N@5: 0.16598 early stop: 0\n",
      "15 2800 train loss: 0.0283508 valid loss: 0.0284266 P@5: 0.12657 N@5: 0.19866 early stop: 0\n",
      "16 800 train loss: 0.0277998 valid loss: 0.0285330 P@5: 0.11657 N@5: 0.19036 early stop: 0\n",
      "16 1600 train loss: 0.0280692 valid loss: 0.0282298 P@5: 0.13829 N@5: 0.21274 early stop: 0\n",
      "16 2400 train loss: 0.0275812 valid loss: 0.0282172 P@5: 0.14314 N@5: 0.22859 early stop: 0\n",
      "17 400 train loss: 0.0279501 valid loss: 0.0281474 P@5: 0.13343 N@5: 0.20980 early stop: 0\n",
      "17 1200 train loss: 0.0276477 valid loss: 0.0281213 P@5: 0.12743 N@5: 0.21498 early stop: 0\n",
      "17 2000 train loss: 0.0275379 valid loss: 0.0279870 P@5: 0.13457 N@5: 0.21610 early stop: 0\n",
      "17 2800 train loss: 0.0277353 valid loss: 0.0279681 P@5: 0.14314 N@5: 0.23687 early stop: 0\n",
      "18 800 train loss: 0.0267406 valid loss: 0.0279655 P@5: 0.14257 N@5: 0.23057 early stop: 0\n",
      "18 1600 train loss: 0.0274774 valid loss: 0.0278974 P@5: 0.14486 N@5: 0.23325 early stop: 0\n",
      "18 2400 train loss: 0.0273996 valid loss: 0.0277684 P@5: 0.14286 N@5: 0.23632 early stop: 0\n",
      "19 400 train loss: 0.0272386 valid loss: 0.0278544 P@5: 0.13543 N@5: 0.21312 early stop: 0\n",
      "19 1200 train loss: 0.0270027 valid loss: 0.0277155 P@5: 0.15143 N@5: 0.23717 early stop: 0\n",
      "19 2000 train loss: 0.0269205 valid loss: 0.0275751 P@5: 0.14943 N@5: 0.23841 early stop: 0\n",
      "19 2800 train loss: 0.0274974 valid loss: 0.0273487 P@5: 0.14543 N@5: 0.23290 early stop: 0\n",
      "20 800 train loss: 0.0267735 valid loss: 0.0275398 P@5: 0.14371 N@5: 0.23563 early stop: 0\n",
      "20 1600 train loss: 0.0266891 valid loss: 0.0273716 P@5: 0.15829 N@5: 0.25014 early stop: 0\n",
      "20 2400 train loss: 0.0267688 valid loss: 0.0274594 P@5: 0.15857 N@5: 0.25192 early stop: 0\n",
      "21 400 train loss: 0.0263528 valid loss: 0.0272667 P@5: 0.15514 N@5: 0.24883 early stop: 0\n",
      "21 1200 train loss: 0.0271123 valid loss: 0.0273240 P@5: 0.15743 N@5: 0.24794 early stop: 0\n",
      "21 2000 train loss: 0.0263159 valid loss: 0.0273418 P@5: 0.15086 N@5: 0.24195 early stop: 0\n",
      "21 2800 train loss: 0.0263983 valid loss: 0.0270787 P@5: 0.16743 N@5: 0.26857 early stop: 0\n",
      "22 800 train loss: 0.0258408 valid loss: 0.0272341 P@5: 0.16171 N@5: 0.25836 early stop: 0\n",
      "22 1600 train loss: 0.0265749 valid loss: 0.0270299 P@5: 0.16029 N@5: 0.25347 early stop: 0\n",
      "22 2400 train loss: 0.0260414 valid loss: 0.0269874 P@5: 0.16257 N@5: 0.26622 early stop: 0\n",
      "23 400 train loss: 0.0263361 valid loss: 0.0270227 P@5: 0.15457 N@5: 0.24333 early stop: 0\n",
      "23 1200 train loss: 0.0263200 valid loss: 0.0270237 P@5: 0.17143 N@5: 0.26952 early stop: 0\n",
      "23 2000 train loss: 0.0258744 valid loss: 0.0269779 P@5: 0.15771 N@5: 0.24996 early stop: 0\n",
      "23 2800 train loss: 0.0258441 valid loss: 0.0267271 P@5: 0.16714 N@5: 0.26849 early stop: 0\n",
      "24 800 train loss: 0.0259534 valid loss: 0.0268000 P@5: 0.16971 N@5: 0.26763 early stop: 0\n",
      "24 1600 train loss: 0.0259995 valid loss: 0.0268821 P@5: 0.16429 N@5: 0.26528 early stop: 0\n",
      "24 2400 train loss: 0.0256774 valid loss: 0.0266771 P@5: 0.17457 N@5: 0.28235 early stop: 0\n",
      "25 400 train loss: 0.0252400 valid loss: 0.0266290 P@5: 0.17229 N@5: 0.27839 early stop: 0\n",
      "25 1200 train loss: 0.0254596 valid loss: 0.0266949 P@5: 0.16514 N@5: 0.26851 early stop: 0\n",
      "25 2000 train loss: 0.0255244 valid loss: 0.0265963 P@5: 0.16800 N@5: 0.26964 early stop: 0\n",
      "25 2800 train loss: 0.0256866 valid loss: 0.0266663 P@5: 0.16857 N@5: 0.27207 early stop: 0\n",
      "26 800 train loss: 0.0249206 valid loss: 0.0267205 P@5: 0.15171 N@5: 0.24508 early stop: 0\n",
      "26 1600 train loss: 0.0254255 valid loss: 0.0263204 P@5: 0.18000 N@5: 0.29168 early stop: 0\n",
      "26 2400 train loss: 0.0252510 valid loss: 0.0263787 P@5: 0.17200 N@5: 0.27928 early stop: 0\n",
      "27 400 train loss: 0.0253888 valid loss: 0.0265342 P@5: 0.17000 N@5: 0.27713 early stop: 0\n",
      "27 1200 train loss: 0.0254268 valid loss: 0.0263309 P@5: 0.17914 N@5: 0.28664 early stop: 0\n",
      "27 2000 train loss: 0.0250836 valid loss: 0.0264375 P@5: 0.17629 N@5: 0.27849 early stop: 0\n",
      "27 2800 train loss: 0.0249102 valid loss: 0.0262658 P@5: 0.18000 N@5: 0.28645 early stop: 0\n",
      "28 800 train loss: 0.0244649 valid loss: 0.0262103 P@5: 0.17857 N@5: 0.29285 early stop: 0\n",
      "28 1600 train loss: 0.0249100 valid loss: 0.0262409 P@5: 0.17829 N@5: 0.28399 early stop: 0\n",
      "28 2400 train loss: 0.0250345 valid loss: 0.0263156 P@5: 0.18057 N@5: 0.28835 early stop: 0\n",
      "29 400 train loss: 0.0246359 valid loss: 0.0262459 P@5: 0.17886 N@5: 0.29089 early stop: 0\n",
      "29 1200 train loss: 0.0248024 valid loss: 0.0263204 P@5: 0.17771 N@5: 0.28652 early stop: 0\n",
      "29 2000 train loss: 0.0244899 valid loss: 0.0260183 P@5: 0.17686 N@5: 0.28563 early stop: 0\n",
      "29 2800 train loss: 0.0251134 valid loss: 0.0259213 P@5: 0.18086 N@5: 0.29019 early stop: 0\n",
      "30 800 train loss: 0.0241796 valid loss: 0.0260391 P@5: 0.18714 N@5: 0.30471 early stop: 0\n",
      "30 1600 train loss: 0.0243144 valid loss: 0.0259784 P@5: 0.18514 N@5: 0.29518 early stop: 0\n",
      "30 2400 train loss: 0.0252072 valid loss: 0.0258909 P@5: 0.18800 N@5: 0.30287 early stop: 0\n",
      "31 400 train loss: 0.0240795 valid loss: 0.0260961 P@5: 0.17743 N@5: 0.28829 early stop: 0\n",
      "31 1200 train loss: 0.0245497 valid loss: 0.0259332 P@5: 0.18029 N@5: 0.29399 early stop: 0\n",
      "31 2000 train loss: 0.0237587 valid loss: 0.0258618 P@5: 0.18857 N@5: 0.30678 early stop: 0\n",
      "31 2800 train loss: 0.0245406 valid loss: 0.0257027 P@5: 0.18629 N@5: 0.30164 early stop: 0\n",
      "32 800 train loss: 0.0239688 valid loss: 0.0259356 P@5: 0.18229 N@5: 0.29503 early stop: 0\n",
      "32 1600 train loss: 0.0242768 valid loss: 0.0258453 P@5: 0.18657 N@5: 0.30359 early stop: 0\n",
      "32 2400 train loss: 0.0243627 valid loss: 0.0257669 P@5: 0.18857 N@5: 0.30929 early stop: 0\n",
      "33 400 train loss: 0.0237611 valid loss: 0.0258603 P@5: 0.18971 N@5: 0.30822 early stop: 0\n",
      "33 1200 train loss: 0.0238458 valid loss: 0.0258439 P@5: 0.18114 N@5: 0.29872 early stop: 0\n",
      "33 2000 train loss: 0.0240240 valid loss: 0.0257082 P@5: 0.18771 N@5: 0.30156 early stop: 0\n",
      "33 2800 train loss: 0.0238298 valid loss: 0.0255794 P@5: 0.18286 N@5: 0.30062 early stop: 0\n",
      "34 800 train loss: 0.0236903 valid loss: 0.0257610 P@5: 0.18886 N@5: 0.30077 early stop: 0\n",
      "34 1600 train loss: 0.0233020 valid loss: 0.0257569 P@5: 0.19514 N@5: 0.31884 early stop: 0\n",
      "34 2400 train loss: 0.0239610 valid loss: 0.0255670 P@5: 0.19571 N@5: 0.31680 early stop: 0\n",
      "35 400 train loss: 0.0234436 valid loss: 0.0256220 P@5: 0.19629 N@5: 0.32350 early stop: 0\n",
      "35 1200 train loss: 0.0226775 valid loss: 0.0255629 P@5: 0.19286 N@5: 0.31438 early stop: 0\n",
      "35 2000 train loss: 0.0244962 valid loss: 0.0255323 P@5: 0.18571 N@5: 0.29775 early stop: 0\n",
      "35 2800 train loss: 0.0238169 valid loss: 0.0255791 P@5: 0.18257 N@5: 0.29872 early stop: 0\n",
      "36 800 train loss: 0.0229422 valid loss: 0.0256236 P@5: 0.18800 N@5: 0.30613 early stop: 0\n",
      "36 1600 train loss: 0.0235090 valid loss: 0.0256282 P@5: 0.18829 N@5: 0.30166 early stop: 0\n",
      "36 2400 train loss: 0.0231323 valid loss: 0.0254541 P@5: 0.18657 N@5: 0.30343 early stop: 0\n",
      "37 400 train loss: 0.0233406 valid loss: 0.0254928 P@5: 0.20371 N@5: 0.33313 early stop: 0\n",
      "37 1200 train loss: 0.0237378 valid loss: 0.0255810 P@5: 0.18800 N@5: 0.30327 early stop: 0\n",
      "37 2000 train loss: 0.0232742 valid loss: 0.0254935 P@5: 0.18400 N@5: 0.29924 early stop: 0\n",
      "37 2800 train loss: 0.0228123 valid loss: 0.0256428 P@5: 0.18114 N@5: 0.28666 early stop: 0\n",
      "38 800 train loss: 0.0234464 valid loss: 0.0253828 P@5: 0.19400 N@5: 0.31718 early stop: 0\n",
      "38 1600 train loss: 0.0228293 valid loss: 0.0253599 P@5: 0.19857 N@5: 0.32771 early stop: 0\n",
      "38 2400 train loss: 0.0228034 valid loss: 0.0254675 P@5: 0.18429 N@5: 0.29907 early stop: 0\n",
      "39 400 train loss: 0.0231187 valid loss: 0.0254353 P@5: 0.19771 N@5: 0.31816 early stop: 0\n",
      "39 1200 train loss: 0.0221553 valid loss: 0.0253563 P@5: 0.19314 N@5: 0.31909 early stop: 0\n",
      "39 2000 train loss: 0.0233887 valid loss: 0.0253229 P@5: 0.18971 N@5: 0.31268 early stop: 0\n",
      "39 2800 train loss: 0.0228169 valid loss: 0.0253770 P@5: 0.17686 N@5: 0.29497 early stop: 0\n",
      "40 800 train loss: 0.0225591 valid loss: 0.0251171 P@5: 0.19829 N@5: 0.33015 early stop: 0\n",
      "40 1600 train loss: 0.0226277 valid loss: 0.0253494 P@5: 0.19257 N@5: 0.30839 early stop: 0\n",
      "40 2400 train loss: 0.0229491 valid loss: 0.0252840 P@5: 0.19629 N@5: 0.31182 early stop: 0\n",
      "41 400 train loss: 0.0224833 valid loss: 0.0252085 P@5: 0.20057 N@5: 0.32390 early stop: 0\n",
      "41 1200 train loss: 0.0227974 valid loss: 0.0251759 P@5: 0.18943 N@5: 0.31337 early stop: 0\n",
      "41 2000 train loss: 0.0220344 valid loss: 0.0251828 P@5: 0.19571 N@5: 0.31983 early stop: 0\n",
      "41 2800 train loss: 0.0227803 valid loss: 0.0252400 P@5: 0.19771 N@5: 0.32566 early stop: 0\n",
      "42 800 train loss: 0.0222272 valid loss: 0.0252457 P@5: 0.19914 N@5: 0.32616 early stop: 0\n",
      "42 1600 train loss: 0.0221226 valid loss: 0.0250524 P@5: 0.19886 N@5: 0.32321 early stop: 0\n",
      "42 2400 train loss: 0.0225517 valid loss: 0.0251198 P@5: 0.20057 N@5: 0.33208 early stop: 0\n",
      "43 400 train loss: 0.0225659 valid loss: 0.0252424 P@5: 0.19886 N@5: 0.31466 early stop: 0\n",
      "43 1200 train loss: 0.0223566 valid loss: 0.0251780 P@5: 0.19114 N@5: 0.30942 early stop: 0\n",
      "43 2000 train loss: 0.0222129 valid loss: 0.0249381 P@5: 0.19486 N@5: 0.32207 early stop: 0\n",
      "43 2800 train loss: 0.0220231 valid loss: 0.0250290 P@5: 0.20086 N@5: 0.32674 early stop: 0\n",
      "44 800 train loss: 0.0222253 valid loss: 0.0250813 P@5: 0.19943 N@5: 0.32546 early stop: 0\n",
      "44 1600 train loss: 0.0214315 valid loss: 0.0249986 P@5: 0.19743 N@5: 0.32929 early stop: 0\n",
      "44 2400 train loss: 0.0221018 valid loss: 0.0250487 P@5: 0.19371 N@5: 0.31559 early stop: 0\n",
      "45 400 train loss: 0.0218474 valid loss: 0.0250257 P@5: 0.20229 N@5: 0.32961 early stop: 0\n",
      "45 1200 train loss: 0.0218206 valid loss: 0.0249726 P@5: 0.19857 N@5: 0.32025 early stop: 0\n",
      "45 2000 train loss: 0.0224993 valid loss: 0.0248601 P@5: 0.20200 N@5: 0.32528 early stop: 0\n",
      "45 2800 train loss: 0.0218520 valid loss: 0.0249363 P@5: 0.21057 N@5: 0.34502 early stop: 0\n",
      "46 800 train loss: 0.0221617 valid loss: 0.0249402 P@5: 0.20371 N@5: 0.33380 early stop: 0\n",
      "46 1600 train loss: 0.0211730 valid loss: 0.0250594 P@5: 0.19800 N@5: 0.32201 early stop: 0\n",
      "46 2400 train loss: 0.0218046 valid loss: 0.0249847 P@5: 0.20029 N@5: 0.33549 early stop: 0\n",
      "47 400 train loss: 0.0215956 valid loss: 0.0249136 P@5: 0.19771 N@5: 0.33273 early stop: 0\n",
      "47 1200 train loss: 0.0218604 valid loss: 0.0249208 P@5: 0.20514 N@5: 0.33531 early stop: 0\n",
      "47 2000 train loss: 0.0215801 valid loss: 0.0249456 P@5: 0.20771 N@5: 0.34330 early stop: 0\n",
      "47 2800 train loss: 0.0216536 valid loss: 0.0248913 P@5: 0.20057 N@5: 0.32104 early stop: 0\n",
      "48 800 train loss: 0.0214402 valid loss: 0.0248766 P@5: 0.20114 N@5: 0.32577 early stop: 0\n",
      "48 1600 train loss: 0.0220029 valid loss: 0.0249214 P@5: 0.19343 N@5: 0.31515 early stop: 0\n",
      "48 2400 train loss: 0.0212234 valid loss: 0.0247525 P@5: 0.21114 N@5: 0.34058 early stop: 0\n",
      "49 400 train loss: 0.0211005 valid loss: 0.0247555 P@5: 0.20600 N@5: 0.33998 early stop: 0\n",
      "49 1200 train loss: 0.0218067 valid loss: 0.0249609 P@5: 0.19829 N@5: 0.32640 early stop: 0\n",
      "49 2000 train loss: 0.0210241 valid loss: 0.0250142 P@5: 0.21057 N@5: 0.34155 early stop: 0\n",
      "49 2800 train loss: 0.0211953 valid loss: 0.0247823 P@5: 0.20971 N@5: 0.34502 early stop: 0\n",
      "50 800 train loss: 0.0211875 valid loss: 0.0246487 P@5: 0.21257 N@5: 0.34539 early stop: 0\n",
      "50 1600 train loss: 0.0210312 valid loss: 0.0247513 P@5: 0.21057 N@5: 0.34300 early stop: 0\n",
      "50 2400 train loss: 0.0210484 valid loss: 0.0247004 P@5: 0.20771 N@5: 0.33889 early stop: 0\n",
      "51 400 train loss: 0.0212731 valid loss: 0.0247910 P@5: 0.20514 N@5: 0.33031 early stop: 0\n",
      "51 1200 train loss: 0.0214912 valid loss: 0.0248911 P@5: 0.20971 N@5: 0.33604 early stop: 0\n",
      "51 2000 train loss: 0.0213846 valid loss: 0.0246912 P@5: 0.20743 N@5: 0.33779 early stop: 0\n",
      "51 2800 train loss: 0.0205686 valid loss: 0.0246840 P@5: 0.20543 N@5: 0.33562 early stop: 0\n",
      "52 800 train loss: 0.0205570 valid loss: 0.0246532 P@5: 0.21171 N@5: 0.34413 early stop: 0\n",
      "52 1600 train loss: 0.0217796 valid loss: 0.0246570 P@5: 0.21543 N@5: 0.35181 early stop: 0\n",
      "52 2400 train loss: 0.0204044 valid loss: 0.0247068 P@5: 0.21371 N@5: 0.34201 early stop: 0\n",
      "53 400 train loss: 0.0206753 valid loss: 0.0246545 P@5: 0.20571 N@5: 0.34184 early stop: 0\n",
      "53 1200 train loss: 0.0205684 valid loss: 0.0246734 P@5: 0.20029 N@5: 0.33332 early stop: 0\n",
      "53 2000 train loss: 0.0210568 valid loss: 0.0245735 P@5: 0.21571 N@5: 0.35008 early stop: 0\n",
      "53 2800 train loss: 0.0211314 valid loss: 0.0245516 P@5: 0.21086 N@5: 0.34026 early stop: 0\n",
      "54 800 train loss: 0.0205363 valid loss: 0.0246647 P@5: 0.21286 N@5: 0.34532 early stop: 0\n",
      "54 1600 train loss: 0.0212593 valid loss: 0.0246522 P@5: 0.20257 N@5: 0.33075 early stop: 0\n",
      "54 2400 train loss: 0.0204933 valid loss: 0.0245338 P@5: 0.21086 N@5: 0.34959 early stop: 0\n",
      "55 400 train loss: 0.0202502 valid loss: 0.0247109 P@5: 0.20486 N@5: 0.33975 early stop: 0\n",
      "55 1200 train loss: 0.0204680 valid loss: 0.0245829 P@5: 0.21143 N@5: 0.34891 early stop: 0\n",
      "55 2000 train loss: 0.0207471 valid loss: 0.0245068 P@5: 0.21714 N@5: 0.35109 early stop: 0\n",
      "55 2800 train loss: 0.0207996 valid loss: 0.0245245 P@5: 0.21229 N@5: 0.34724 early stop: 0\n",
      "56 800 train loss: 0.0207162 valid loss: 0.0244865 P@5: 0.21914 N@5: 0.35692 early stop: 0\n",
      "56 1600 train loss: 0.0202291 valid loss: 0.0245707 P@5: 0.21457 N@5: 0.35083 early stop: 0\n",
      "56 2400 train loss: 0.0204490 valid loss: 0.0245158 P@5: 0.20143 N@5: 0.32666 early stop: 0\n",
      "57 400 train loss: 0.0199656 valid loss: 0.0245962 P@5: 0.20286 N@5: 0.32722 early stop: 0\n",
      "57 1200 train loss: 0.0204844 valid loss: 0.0245791 P@5: 0.21000 N@5: 0.34621 early stop: 0\n",
      "57 2000 train loss: 0.0205733 valid loss: 0.0245360 P@5: 0.21886 N@5: 0.35276 early stop: 0\n",
      "57 2800 train loss: 0.0204581 valid loss: 0.0244614 P@5: 0.21486 N@5: 0.35181 early stop: 0\n",
      "58 800 train loss: 0.0203431 valid loss: 0.0243848 P@5: 0.21143 N@5: 0.35103 early stop: 0\n",
      "58 1600 train loss: 0.0203459 valid loss: 0.0243450 P@5: 0.21600 N@5: 0.35188 early stop: 0\n",
      "58 2400 train loss: 0.0199303 valid loss: 0.0244666 P@5: 0.20971 N@5: 0.33951 early stop: 0\n",
      "59 400 train loss: 0.0205792 valid loss: 0.0243617 P@5: 0.21400 N@5: 0.34806 early stop: 0\n",
      "59 1200 train loss: 0.0198068 valid loss: 0.0243948 P@5: 0.21114 N@5: 0.34320 early stop: 0\n",
      "59 2000 train loss: 0.0204794 valid loss: 0.0244308 P@5: 0.20029 N@5: 0.33626 early stop: 0\n",
      "59 2800 train loss: 0.0197150 valid loss: 0.0244919 P@5: 0.21857 N@5: 0.35108 early stop: 0\n",
      "60 800 train loss: 0.0196300 valid loss: 0.0245417 P@5: 0.21714 N@5: 0.35152 early stop: 0\n",
      "60 1600 train loss: 0.0203636 valid loss: 0.0242627 P@5: 0.21914 N@5: 0.35601 early stop: 0\n",
      "60 2400 train loss: 0.0197576 valid loss: 0.0244734 P@5: 0.20714 N@5: 0.34105 early stop: 0\n",
      "61 400 train loss: 0.0204246 valid loss: 0.0244473 P@5: 0.21543 N@5: 0.35273 early stop: 0\n",
      "61 1200 train loss: 0.0191407 valid loss: 0.0244932 P@5: 0.21029 N@5: 0.34404 early stop: 0\n",
      "61 2000 train loss: 0.0206531 valid loss: 0.0241669 P@5: 0.21543 N@5: 0.35026 early stop: 0\n",
      "61 2800 train loss: 0.0197426 valid loss: 0.0245876 P@5: 0.20200 N@5: 0.32989 early stop: 0\n",
      "62 800 train loss: 0.0195081 valid loss: 0.0244091 P@5: 0.22171 N@5: 0.36708 early stop: 0\n",
      "62 1600 train loss: 0.0196779 valid loss: 0.0243170 P@5: 0.21371 N@5: 0.35339 early stop: 0\n",
      "62 2400 train loss: 0.0201513 valid loss: 0.0243859 P@5: 0.21429 N@5: 0.35501 early stop: 0\n",
      "63 400 train loss: 0.0197311 valid loss: 0.0243539 P@5: 0.21686 N@5: 0.35633 early stop: 0\n",
      "63 1200 train loss: 0.0195712 valid loss: 0.0242663 P@5: 0.22229 N@5: 0.36291 early stop: 0\n",
      "63 2000 train loss: 0.0197923 valid loss: 0.0243555 P@5: 0.20829 N@5: 0.33912 early stop: 0\n",
      "63 2800 train loss: 0.0196869 valid loss: 0.0242054 P@5: 0.21857 N@5: 0.35612 early stop: 0\n",
      "64 800 train loss: 0.0193523 valid loss: 0.0243202 P@5: 0.22200 N@5: 0.35895 early stop: 0\n",
      "64 1600 train loss: 0.0197958 valid loss: 0.0243784 P@5: 0.21771 N@5: 0.36254 early stop: 0\n",
      "64 2400 train loss: 0.0198673 valid loss: 0.0242838 P@5: 0.21400 N@5: 0.34473 early stop: 0\n",
      "65 400 train loss: 0.0189075 valid loss: 0.0244445 P@5: 0.21343 N@5: 0.35277 early stop: 0\n",
      "65 1200 train loss: 0.0193141 valid loss: 0.0242021 P@5: 0.21000 N@5: 0.34665 early stop: 0\n",
      "65 2000 train loss: 0.0201055 valid loss: 0.0243417 P@5: 0.21857 N@5: 0.34767 early stop: 0\n",
      "65 2800 train loss: 0.0193751 valid loss: 0.0242988 P@5: 0.21371 N@5: 0.34864 early stop: 0\n",
      "66 800 train loss: 0.0188924 valid loss: 0.0245093 P@5: 0.21257 N@5: 0.34685 early stop: 0\n",
      "66 1600 train loss: 0.0192246 valid loss: 0.0241557 P@5: 0.21886 N@5: 0.35731 early stop: 0\n",
      "66 2400 train loss: 0.0195245 valid loss: 0.0243246 P@5: 0.21457 N@5: 0.35160 early stop: 0\n",
      "67 400 train loss: 0.0198292 valid loss: 0.0241228 P@5: 0.22257 N@5: 0.35801 early stop: 0\n",
      "67 1200 train loss: 0.0196273 valid loss: 0.0242098 P@5: 0.21886 N@5: 0.35618 early stop: 0\n",
      "67 2000 train loss: 0.0191527 valid loss: 0.0243086 P@5: 0.22171 N@5: 0.35987 early stop: 0\n",
      "67 2800 train loss: 0.0191299 valid loss: 0.0242291 P@5: 0.21629 N@5: 0.35810 early stop: 0\n",
      "68 800 train loss: 0.0189455 valid loss: 0.0241855 P@5: 0.21486 N@5: 0.35172 early stop: 0\n",
      "68 1600 train loss: 0.0195949 valid loss: 0.0242438 P@5: 0.21886 N@5: 0.36054 early stop: 0\n",
      "68 2400 train loss: 0.0188268 valid loss: 0.0239995 P@5: 0.22200 N@5: 0.36404 early stop: 0\n",
      "69 400 train loss: 0.0189266 valid loss: 0.0242512 P@5: 0.22000 N@5: 0.36371 early stop: 0\n",
      "69 1200 train loss: 0.0194367 valid loss: 0.0241523 P@5: 0.21657 N@5: 0.35673 early stop: 0\n",
      "69 2000 train loss: 0.0187130 valid loss: 0.0243411 P@5: 0.21714 N@5: 0.35380 early stop: 0\n",
      "69 2800 train loss: 0.0195924 valid loss: 0.0240899 P@5: 0.21600 N@5: 0.35039 early stop: 0\n",
      "70 800 train loss: 0.0183752 valid loss: 0.0242162 P@5: 0.22800 N@5: 0.36505 early stop: 0\n",
      "70 1600 train loss: 0.0188936 valid loss: 0.0241671 P@5: 0.22257 N@5: 0.36395 early stop: 0\n",
      "70 2400 train loss: 0.0196023 valid loss: 0.0242517 P@5: 0.21486 N@5: 0.34946 early stop: 0\n",
      "71 400 train loss: 0.0184300 valid loss: 0.0242308 P@5: 0.22229 N@5: 0.36423 early stop: 0\n",
      "71 1200 train loss: 0.0188088 valid loss: 0.0241940 P@5: 0.21400 N@5: 0.35466 early stop: 0\n",
      "71 2000 train loss: 0.0196252 valid loss: 0.0241188 P@5: 0.22000 N@5: 0.35826 early stop: 0\n",
      "71 2800 train loss: 0.0189345 valid loss: 0.0240788 P@5: 0.21886 N@5: 0.36077 early stop: 0\n",
      "72 800 train loss: 0.0190095 valid loss: 0.0241810 P@5: 0.22029 N@5: 0.35594 early stop: 0\n",
      "72 1600 train loss: 0.0183874 valid loss: 0.0242507 P@5: 0.22229 N@5: 0.36396 early stop: 0\n",
      "72 2400 train loss: 0.0190201 valid loss: 0.0240287 P@5: 0.22714 N@5: 0.36507 early stop: 0\n",
      "73 400 train loss: 0.0189230 valid loss: 0.0241294 P@5: 0.22600 N@5: 0.36989 early stop: 0\n",
      "73 1200 train loss: 0.0180654 valid loss: 0.0240311 P@5: 0.21914 N@5: 0.35743 early stop: 0\n",
      "73 2000 train loss: 0.0189329 valid loss: 0.0240736 P@5: 0.21629 N@5: 0.35789 early stop: 0\n",
      "73 2800 train loss: 0.0189894 valid loss: 0.0239722 P@5: 0.23429 N@5: 0.38268 early stop: 0\n",
      "74 800 train loss: 0.0189136 valid loss: 0.0240150 P@5: 0.22314 N@5: 0.36466 early stop: 0\n",
      "74 1600 train loss: 0.0186687 valid loss: 0.0241361 P@5: 0.21914 N@5: 0.35890 early stop: 0\n",
      "74 2400 train loss: 0.0180523 valid loss: 0.0240804 P@5: 0.22400 N@5: 0.36239 early stop: 0\n",
      "75 400 train loss: 0.0189311 valid loss: 0.0242277 P@5: 0.21886 N@5: 0.35665 early stop: 0\n",
      "75 1200 train loss: 0.0187045 valid loss: 0.0239418 P@5: 0.22029 N@5: 0.35935 early stop: 0\n",
      "75 2000 train loss: 0.0179509 valid loss: 0.0243068 P@5: 0.21857 N@5: 0.35463 early stop: 0\n",
      "75 2800 train loss: 0.0189500 valid loss: 0.0240679 P@5: 0.22314 N@5: 0.36647 early stop: 0\n",
      "76 800 train loss: 0.0183590 valid loss: 0.0239812 P@5: 0.22371 N@5: 0.36773 early stop: 0\n",
      "76 1600 train loss: 0.0178605 valid loss: 0.0239968 P@5: 0.22257 N@5: 0.36606 early stop: 0\n",
      "76 2400 train loss: 0.0187491 valid loss: 0.0239979 P@5: 0.22000 N@5: 0.35952 early stop: 0\n",
      "77 400 train loss: 0.0188921 valid loss: 0.0239165 P@5: 0.22543 N@5: 0.36658 early stop: 0\n",
      "77 1200 train loss: 0.0182965 valid loss: 0.0240453 P@5: 0.21943 N@5: 0.36293 early stop: 0\n",
      "77 2000 train loss: 0.0183981 valid loss: 0.0240607 P@5: 0.22114 N@5: 0.36787 early stop: 0\n",
      "77 2800 train loss: 0.0182027 valid loss: 0.0238332 P@5: 0.23086 N@5: 0.37453 early stop: 0\n",
      "78 800 train loss: 0.0180028 valid loss: 0.0240321 P@5: 0.22143 N@5: 0.35839 early stop: 0\n",
      "78 1600 train loss: 0.0187399 valid loss: 0.0239171 P@5: 0.22114 N@5: 0.35688 early stop: 0\n",
      "78 2400 train loss: 0.0176684 valid loss: 0.0240171 P@5: 0.22657 N@5: 0.37119 early stop: 0\n",
      "79 400 train loss: 0.0186482 valid loss: 0.0240862 P@5: 0.22457 N@5: 0.36876 early stop: 0\n",
      "79 1200 train loss: 0.0179694 valid loss: 0.0240248 P@5: 0.22143 N@5: 0.36463 early stop: 0\n",
      "79 2000 train loss: 0.0182229 valid loss: 0.0239715 P@5: 0.22886 N@5: 0.37437 early stop: 0\n",
      "79 2800 train loss: 0.0182886 valid loss: 0.0239492 P@5: 0.22514 N@5: 0.37218 early stop: 0\n",
      "80 800 train loss: 0.0173897 valid loss: 0.0240008 P@5: 0.22486 N@5: 0.36651 early stop: 0\n",
      "80 1600 train loss: 0.0184215 valid loss: 0.0239794 P@5: 0.21457 N@5: 0.35394 early stop: 0\n",
      "80 2400 train loss: 0.0183586 valid loss: 0.0238260 P@5: 0.22457 N@5: 0.36132 early stop: 0\n",
      "81 400 train loss: 0.0179050 valid loss: 0.0240413 P@5: 0.22143 N@5: 0.35351 early stop: 0\n",
      "81 1200 train loss: 0.0183292 valid loss: 0.0238752 P@5: 0.22943 N@5: 0.37646 early stop: 0\n",
      "81 2000 train loss: 0.0181721 valid loss: 0.0239747 P@5: 0.22514 N@5: 0.36325 early stop: 0\n",
      "81 2800 train loss: 0.0177102 valid loss: 0.0239927 P@5: 0.22457 N@5: 0.36880 early stop: 0\n",
      "82 800 train loss: 0.0175990 valid loss: 0.0238948 P@5: 0.22829 N@5: 0.37400 early stop: 0\n",
      "82 1600 train loss: 0.0181899 valid loss: 0.0239421 P@5: 0.22571 N@5: 0.36785 early stop: 0\n",
      "82 2400 train loss: 0.0175573 valid loss: 0.0240083 P@5: 0.22257 N@5: 0.36857 early stop: 0\n",
      "83 400 train loss: 0.0183043 valid loss: 0.0239583 P@5: 0.22657 N@5: 0.36979 early stop: 0\n",
      "83 1200 train loss: 0.0173969 valid loss: 0.0239454 P@5: 0.22714 N@5: 0.37253 early stop: 0\n",
      "83 2000 train loss: 0.0183269 valid loss: 0.0239720 P@5: 0.22400 N@5: 0.36685 early stop: 0\n",
      "83 2800 train loss: 0.0178532 valid loss: 0.0238490 P@5: 0.22600 N@5: 0.36399 early stop: 0\n",
      "84 800 train loss: 0.0174826 valid loss: 0.0240009 P@5: 0.22971 N@5: 0.37665 early stop: 0\n",
      "84 1600 train loss: 0.0178684 valid loss: 0.0240061 P@5: 0.22429 N@5: 0.36710 early stop: 0\n",
      "84 2400 train loss: 0.0181560 valid loss: 0.0238403 P@5: 0.22514 N@5: 0.37016 early stop: 0\n",
      "85 400 train loss: 0.0174304 valid loss: 0.0239082 P@5: 0.23057 N@5: 0.37509 early stop: 0\n",
      "85 1200 train loss: 0.0172448 valid loss: 0.0241197 P@5: 0.21486 N@5: 0.35776 early stop: 0\n",
      "85 2000 train loss: 0.0180823 valid loss: 0.0238710 P@5: 0.22657 N@5: 0.36558 early stop: 0\n",
      "85 2800 train loss: 0.0177239 valid loss: 0.0237990 P@5: 0.23200 N@5: 0.37254 early stop: 0\n",
      "86 800 train loss: 0.0174220 valid loss: 0.0239271 P@5: 0.22514 N@5: 0.36870 early stop: 0\n",
      "86 1600 train loss: 0.0180880 valid loss: 0.0238169 P@5: 0.22514 N@5: 0.36865 early stop: 0\n",
      "86 2400 train loss: 0.0174079 valid loss: 0.0238658 P@5: 0.22743 N@5: 0.37033 early stop: 0\n",
      "87 400 train loss: 0.0173004 valid loss: 0.0239444 P@5: 0.22257 N@5: 0.36348 early stop: 0\n",
      "87 1200 train loss: 0.0171240 valid loss: 0.0238597 P@5: 0.22743 N@5: 0.36842 early stop: 0\n",
      "87 2000 train loss: 0.0174934 valid loss: 0.0239421 P@5: 0.23229 N@5: 0.37241 early stop: 0\n",
      "87 2800 train loss: 0.0179277 valid loss: 0.0238870 P@5: 0.22629 N@5: 0.37305 early stop: 0\n",
      "88 800 train loss: 0.0168895 valid loss: 0.0239840 P@5: 0.22314 N@5: 0.36479 early stop: 0\n",
      "88 1600 train loss: 0.0179772 valid loss: 0.0238077 P@5: 0.22886 N@5: 0.37498 early stop: 0\n",
      "88 2400 train loss: 0.0174529 valid loss: 0.0238554 P@5: 0.22257 N@5: 0.36969 early stop: 0\n",
      "89 400 train loss: 0.0171099 valid loss: 0.0239628 P@5: 0.22286 N@5: 0.36190 early stop: 0\n",
      "89 1200 train loss: 0.0173190 valid loss: 0.0239673 P@5: 0.22429 N@5: 0.36726 early stop: 0\n",
      "89 2000 train loss: 0.0172325 valid loss: 0.0237342 P@5: 0.23114 N@5: 0.37355 early stop: 0\n",
      "89 2800 train loss: 0.0175450 valid loss: 0.0237399 P@5: 0.23400 N@5: 0.38493 early stop: 0\n",
      "90 800 train loss: 0.0174801 valid loss: 0.0237736 P@5: 0.23486 N@5: 0.38397 early stop: 0\n",
      "90 1600 train loss: 0.0168031 valid loss: 0.0237869 P@5: 0.22971 N@5: 0.37450 early stop: 0\n",
      "90 2400 train loss: 0.0169519 valid loss: 0.0238991 P@5: 0.23143 N@5: 0.37497 early stop: 0\n",
      "91 400 train loss: 0.0174636 valid loss: 0.0238289 P@5: 0.23057 N@5: 0.37658 early stop: 0\n",
      "91 1200 train loss: 0.0169738 valid loss: 0.0239403 P@5: 0.22800 N@5: 0.37984 early stop: 0\n",
      "91 2000 train loss: 0.0168427 valid loss: 0.0237114 P@5: 0.22857 N@5: 0.37284 early stop: 0\n",
      "91 2800 train loss: 0.0177493 valid loss: 0.0238633 P@5: 0.22343 N@5: 0.36826 early stop: 0\n",
      "92 800 train loss: 0.0165169 valid loss: 0.0237823 P@5: 0.22657 N@5: 0.37401 early stop: 0\n",
      "92 1600 train loss: 0.0167719 valid loss: 0.0238506 P@5: 0.23086 N@5: 0.37496 early stop: 0\n",
      "92 2400 train loss: 0.0176155 valid loss: 0.0237521 P@5: 0.22914 N@5: 0.37399 early stop: 0\n",
      "93 400 train loss: 0.0170421 valid loss: 0.0237329 P@5: 0.23886 N@5: 0.38342 early stop: 0\n",
      "93 1200 train loss: 0.0167947 valid loss: 0.0238146 P@5: 0.22857 N@5: 0.37925 early stop: 0\n",
      "93 2000 train loss: 0.0175844 valid loss: 0.0237512 P@5: 0.22743 N@5: 0.37205 early stop: 0\n",
      "93 2800 train loss: 0.0170521 valid loss: 0.0238723 P@5: 0.22229 N@5: 0.36649 early stop: 0\n",
      "94 800 train loss: 0.0166573 valid loss: 0.0237800 P@5: 0.22886 N@5: 0.37537 early stop: 0\n",
      "94 1600 train loss: 0.0170236 valid loss: 0.0238126 P@5: 0.22886 N@5: 0.37873 early stop: 0\n",
      "94 2400 train loss: 0.0171488 valid loss: 0.0236831 P@5: 0.23086 N@5: 0.37439 early stop: 0\n",
      "95 400 train loss: 0.0163192 valid loss: 0.0239673 P@5: 0.23314 N@5: 0.37781 early stop: 0\n",
      "95 1200 train loss: 0.0166876 valid loss: 0.0237809 P@5: 0.22429 N@5: 0.36706 early stop: 0\n",
      "95 2000 train loss: 0.0170492 valid loss: 0.0237055 P@5: 0.23429 N@5: 0.38146 early stop: 0\n",
      "95 2800 train loss: 0.0172151 valid loss: 0.0237265 P@5: 0.23286 N@5: 0.37847 early stop: 0\n",
      "96 800 train loss: 0.0163887 valid loss: 0.0238075 P@5: 0.22943 N@5: 0.37542 early stop: 0\n",
      "96 1600 train loss: 0.0167169 valid loss: 0.0237900 P@5: 0.22943 N@5: 0.37310 early stop: 0\n",
      "96 2400 train loss: 0.0166150 valid loss: 0.0237719 P@5: 0.22086 N@5: 0.36445 early stop: 0\n",
      "97 400 train loss: 0.0167362 valid loss: 0.0236712 P@5: 0.23629 N@5: 0.38970 early stop: 0\n",
      "97 1200 train loss: 0.0168623 valid loss: 0.0237173 P@5: 0.23286 N@5: 0.37645 early stop: 0\n",
      "97 2000 train loss: 0.0165818 valid loss: 0.0237948 P@5: 0.23314 N@5: 0.37912 early stop: 0\n",
      "97 2800 train loss: 0.0170561 valid loss: 0.0238375 P@5: 0.22886 N@5: 0.37313 early stop: 0\n",
      "98 800 train loss: 0.0169009 valid loss: 0.0237503 P@5: 0.23543 N@5: 0.38059 early stop: 0\n",
      "98 1600 train loss: 0.0159417 valid loss: 0.0236631 P@5: 0.22971 N@5: 0.37607 early stop: 0\n",
      "98 2400 train loss: 0.0170121 valid loss: 0.0237736 P@5: 0.22829 N@5: 0.37434 early stop: 0\n",
      "99 400 train loss: 0.0161966 valid loss: 0.0237090 P@5: 0.23286 N@5: 0.37867 early stop: 0\n",
      "99 1200 train loss: 0.0164962 valid loss: 0.0237446 P@5: 0.22629 N@5: 0.37707 early stop: 0\n",
      "99 2000 train loss: 0.0164096 valid loss: 0.0237631 P@5: 0.23286 N@5: 0.37910 early stop: 0\n",
      "99 2800 train loss: 0.0169268 valid loss: 0.0237331 P@5: 0.23286 N@5: 0.37683 early stop: 0\n",
      "CPU times: user 2min 53s, sys: 1.16 s, total: 2min 54s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, optim_params={\"lr\":1e-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict:   0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3447488584474886,\n",
       " 0.20753424657534247,\n",
       " 0.13858447488584474,\n",
       " 0.3447488584474886,\n",
       " 0.33397402221971173,\n",
       " 0.3848880820359636]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(lda_embs, model, name):\n",
    "    os.mkdir(f\"../data/models/{name}/\")\n",
    "    with open(Path(f\"../data/models/{name}/model.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(Path(f\"../data/models/{name}/lda_embs.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(lda_embs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LDA(num_topics=num_labrls)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_982171/103318190.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df,\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetLDACorrectionNet\"]+metrics+[\"3min 56s + 2min 57s\"]+[\"1.7 Gb + 7.8 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetLDACorrectionNet,\n",
    "              emb_size=300, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0688915 valid loss: 0.0309646 P@5: 0.07771 N@5: 0.11519 early stop: 0\n",
      "0 1600 train loss: 0.0304047 valid loss: 0.0303425 P@5: 0.07886 N@5: 0.11676 early stop: 0\n",
      "0 2400 train loss: 0.0302113 valid loss: 0.0304084 P@5: 0.06743 N@5: 0.08246 early stop: 0\n",
      "1 400 train loss: 0.0303187 valid loss: 0.0306888 P@5: 0.06371 N@5: 0.09674 early stop: 0\n",
      "1 1200 train loss: 0.0306011 valid loss: 0.0302712 P@5: 0.07114 N@5: 0.10336 early stop: 0\n",
      "1 2000 train loss: 0.0298866 valid loss: 0.0303687 P@5: 0.06800 N@5: 0.10002 early stop: 0\n",
      "1 2800 train loss: 0.0306319 valid loss: 0.0300972 P@5: 0.07600 N@5: 0.11225 early stop: 0\n",
      "2 800 train loss: 0.0300871 valid loss: 0.0300718 P@5: 0.07800 N@5: 0.11066 early stop: 0\n",
      "2 1600 train loss: 0.0298875 valid loss: 0.0302885 P@5: 0.06886 N@5: 0.09713 early stop: 0\n",
      "2 2400 train loss: 0.0310915 valid loss: 0.0302050 P@5: 0.06743 N@5: 0.10119 early stop: 0\n",
      "3 400 train loss: 0.0297334 valid loss: 0.0302089 P@5: 0.07829 N@5: 0.10966 early stop: 0\n",
      "3 1200 train loss: 0.0302662 valid loss: 0.0302417 P@5: 0.08114 N@5: 0.11866 early stop: 0\n",
      "3 2000 train loss: 0.0301298 valid loss: 0.0302098 P@5: 0.07771 N@5: 0.11552 early stop: 0\n",
      "3 2800 train loss: 0.0302952 valid loss: 0.0300883 P@5: 0.07743 N@5: 0.11060 early stop: 0\n",
      "4 800 train loss: 0.0305592 valid loss: 0.0302304 P@5: 0.08800 N@5: 0.13016 early stop: 0\n",
      "4 1600 train loss: 0.0300000 valid loss: 0.0302928 P@5: 0.08914 N@5: 0.11587 early stop: 0\n",
      "4 2400 train loss: 0.0299571 valid loss: 0.0299335 P@5: 0.08429 N@5: 0.12234 early stop: 0\n",
      "5 400 train loss: 0.0298110 valid loss: 0.0301086 P@5: 0.06829 N@5: 0.10503 early stop: 0\n",
      "5 1200 train loss: 0.0299877 valid loss: 0.0300378 P@5: 0.08086 N@5: 0.11534 early stop: 0\n",
      "5 2000 train loss: 0.0298839 valid loss: 0.0303127 P@5: 0.08286 N@5: 0.11991 early stop: 0\n",
      "5 2800 train loss: 0.0298352 valid loss: 0.0297728 P@5: 0.08229 N@5: 0.13290 early stop: 0\n",
      "6 800 train loss: 0.0303781 valid loss: 0.0298917 P@5: 0.09829 N@5: 0.13685 early stop: 0\n",
      "6 1600 train loss: 0.0296312 valid loss: 0.0298370 P@5: 0.10371 N@5: 0.14133 early stop: 0\n",
      "6 2400 train loss: 0.0295608 valid loss: 0.0297930 P@5: 0.09514 N@5: 0.13450 early stop: 0\n",
      "7 400 train loss: 0.0293484 valid loss: 0.0296954 P@5: 0.10457 N@5: 0.15276 early stop: 0\n",
      "7 1200 train loss: 0.0300745 valid loss: 0.0297584 P@5: 0.09800 N@5: 0.13481 early stop: 0\n",
      "7 2000 train loss: 0.0290613 valid loss: 0.0296387 P@5: 0.08800 N@5: 0.15084 early stop: 0\n",
      "7 2800 train loss: 0.0300640 valid loss: 0.0297832 P@5: 0.09029 N@5: 0.13518 early stop: 0\n",
      "8 800 train loss: 0.0289284 valid loss: 0.0297220 P@5: 0.09571 N@5: 0.13722 early stop: 0\n",
      "8 1600 train loss: 0.0295140 valid loss: 0.0296784 P@5: 0.08886 N@5: 0.14059 early stop: 0\n",
      "8 2400 train loss: 0.0297359 valid loss: 0.0295219 P@5: 0.09743 N@5: 0.13734 early stop: 0\n",
      "9 400 train loss: 0.0289482 valid loss: 0.0293288 P@5: 0.10857 N@5: 0.15748 early stop: 0\n",
      "9 1200 train loss: 0.0292481 valid loss: 0.0292980 P@5: 0.09714 N@5: 0.15484 early stop: 0\n",
      "9 2000 train loss: 0.0294194 valid loss: 0.0293118 P@5: 0.11000 N@5: 0.17141 early stop: 0\n",
      "9 2800 train loss: 0.0291673 valid loss: 0.0291746 P@5: 0.09514 N@5: 0.17567 early stop: 0\n",
      "10 800 train loss: 0.0291398 valid loss: 0.0291749 P@5: 0.11457 N@5: 0.17956 early stop: 0\n",
      "10 1600 train loss: 0.0286936 valid loss: 0.0288321 P@5: 0.10800 N@5: 0.17305 early stop: 0\n",
      "10 2400 train loss: 0.0290554 valid loss: 0.0288466 P@5: 0.11400 N@5: 0.16847 early stop: 0\n",
      "11 400 train loss: 0.0278232 valid loss: 0.0287537 P@5: 0.12057 N@5: 0.19016 early stop: 0\n",
      "11 1200 train loss: 0.0278652 valid loss: 0.0286631 P@5: 0.11971 N@5: 0.19105 early stop: 0\n",
      "11 2000 train loss: 0.0287501 valid loss: 0.0286578 P@5: 0.12171 N@5: 0.18045 early stop: 0\n",
      "11 2800 train loss: 0.0285491 valid loss: 0.0283274 P@5: 0.12857 N@5: 0.19570 early stop: 0\n",
      "12 800 train loss: 0.0278265 valid loss: 0.0282187 P@5: 0.12286 N@5: 0.18097 early stop: 0\n",
      "12 1600 train loss: 0.0279688 valid loss: 0.0282181 P@5: 0.12886 N@5: 0.19707 early stop: 0\n",
      "12 2400 train loss: 0.0277939 valid loss: 0.0279854 P@5: 0.12829 N@5: 0.20429 early stop: 0\n",
      "13 400 train loss: 0.0271666 valid loss: 0.0279231 P@5: 0.13343 N@5: 0.20341 early stop: 0\n",
      "13 1200 train loss: 0.0270917 valid loss: 0.0278806 P@5: 0.13400 N@5: 0.21185 early stop: 0\n",
      "13 2000 train loss: 0.0278166 valid loss: 0.0273129 P@5: 0.15314 N@5: 0.24650 early stop: 0\n",
      "13 2800 train loss: 0.0273188 valid loss: 0.0274084 P@5: 0.14371 N@5: 0.22979 early stop: 0\n",
      "14 800 train loss: 0.0272591 valid loss: 0.0272128 P@5: 0.15286 N@5: 0.23103 early stop: 0\n",
      "14 1600 train loss: 0.0266601 valid loss: 0.0274773 P@5: 0.13429 N@5: 0.20685 early stop: 0\n",
      "14 2400 train loss: 0.0264141 valid loss: 0.0269664 P@5: 0.15629 N@5: 0.24807 early stop: 0\n",
      "15 400 train loss: 0.0264896 valid loss: 0.0268255 P@5: 0.15657 N@5: 0.24972 early stop: 0\n",
      "15 1200 train loss: 0.0260830 valid loss: 0.0266484 P@5: 0.16114 N@5: 0.25971 early stop: 0\n",
      "15 2000 train loss: 0.0266148 valid loss: 0.0267319 P@5: 0.15714 N@5: 0.25174 early stop: 0\n",
      "15 2800 train loss: 0.0263954 valid loss: 0.0265897 P@5: 0.16229 N@5: 0.26215 early stop: 0\n",
      "16 800 train loss: 0.0257060 valid loss: 0.0264888 P@5: 0.16229 N@5: 0.25796 early stop: 0\n",
      "16 1600 train loss: 0.0257377 valid loss: 0.0266647 P@5: 0.15457 N@5: 0.24575 early stop: 0\n",
      "16 2400 train loss: 0.0257122 valid loss: 0.0269367 P@5: 0.15057 N@5: 0.24352 early stop: 0\n",
      "17 400 train loss: 0.0256452 valid loss: 0.0265312 P@5: 0.15714 N@5: 0.25354 early stop: 0\n",
      "17 1200 train loss: 0.0253587 valid loss: 0.0263310 P@5: 0.15771 N@5: 0.25510 early stop: 0\n",
      "17 2000 train loss: 0.0262214 valid loss: 0.0263819 P@5: 0.16543 N@5: 0.26172 early stop: 0\n",
      "17 2800 train loss: 0.0257503 valid loss: 0.0265130 P@5: 0.16314 N@5: 0.25779 early stop: 0\n",
      "18 800 train loss: 0.0250377 valid loss: 0.0265178 P@5: 0.16200 N@5: 0.25299 early stop: 0\n",
      "18 1600 train loss: 0.0250906 valid loss: 0.0260337 P@5: 0.16886 N@5: 0.27396 early stop: 0\n",
      "18 2400 train loss: 0.0253646 valid loss: 0.0261057 P@5: 0.16914 N@5: 0.26768 early stop: 0\n",
      "19 400 train loss: 0.0250255 valid loss: 0.0262121 P@5: 0.16571 N@5: 0.26694 early stop: 0\n",
      "19 1200 train loss: 0.0247806 valid loss: 0.0260683 P@5: 0.16686 N@5: 0.27340 early stop: 0\n",
      "19 2000 train loss: 0.0247541 valid loss: 0.0258417 P@5: 0.17514 N@5: 0.28503 early stop: 0\n",
      "19 2800 train loss: 0.0252707 valid loss: 0.0260122 P@5: 0.16743 N@5: 0.26927 early stop: 0\n",
      "20 800 train loss: 0.0244898 valid loss: 0.0257220 P@5: 0.17657 N@5: 0.29067 early stop: 0\n",
      "20 1600 train loss: 0.0239397 valid loss: 0.0257544 P@5: 0.18086 N@5: 0.29168 early stop: 0\n",
      "20 2400 train loss: 0.0249766 valid loss: 0.0258833 P@5: 0.17314 N@5: 0.28312 early stop: 0\n",
      "21 400 train loss: 0.0248390 valid loss: 0.0260458 P@5: 0.17086 N@5: 0.27201 early stop: 0\n",
      "21 1200 train loss: 0.0241832 valid loss: 0.0260867 P@5: 0.16743 N@5: 0.27528 early stop: 0\n",
      "21 2000 train loss: 0.0238036 valid loss: 0.0260274 P@5: 0.16514 N@5: 0.26380 early stop: 0\n",
      "21 2800 train loss: 0.0248988 valid loss: 0.0254050 P@5: 0.18400 N@5: 0.30216 early stop: 0\n",
      "22 800 train loss: 0.0239722 valid loss: 0.0254052 P@5: 0.18029 N@5: 0.29623 early stop: 0\n",
      "22 1600 train loss: 0.0241399 valid loss: 0.0256207 P@5: 0.18657 N@5: 0.30690 early stop: 0\n",
      "22 2400 train loss: 0.0238365 valid loss: 0.0254358 P@5: 0.18371 N@5: 0.29856 early stop: 0\n",
      "23 400 train loss: 0.0236528 valid loss: 0.0252698 P@5: 0.18800 N@5: 0.30322 early stop: 0\n",
      "23 1200 train loss: 0.0235558 valid loss: 0.0255582 P@5: 0.18343 N@5: 0.29814 early stop: 0\n",
      "23 2000 train loss: 0.0240064 valid loss: 0.0254877 P@5: 0.17914 N@5: 0.28974 early stop: 0\n",
      "23 2800 train loss: 0.0240101 valid loss: 0.0256933 P@5: 0.17943 N@5: 0.29032 early stop: 0\n",
      "24 800 train loss: 0.0231884 valid loss: 0.0255028 P@5: 0.18743 N@5: 0.29788 early stop: 0\n",
      "24 1600 train loss: 0.0231590 valid loss: 0.0251876 P@5: 0.19200 N@5: 0.31208 early stop: 0\n",
      "24 2400 train loss: 0.0236008 valid loss: 0.0250572 P@5: 0.19029 N@5: 0.30299 early stop: 0\n",
      "25 400 train loss: 0.0238850 valid loss: 0.0251497 P@5: 0.19371 N@5: 0.30922 early stop: 0\n",
      "25 1200 train loss: 0.0232149 valid loss: 0.0255748 P@5: 0.18429 N@5: 0.28825 early stop: 0\n",
      "25 2000 train loss: 0.0225167 valid loss: 0.0253403 P@5: 0.18343 N@5: 0.29438 early stop: 0\n",
      "25 2800 train loss: 0.0239824 valid loss: 0.0251164 P@5: 0.17943 N@5: 0.29939 early stop: 0\n",
      "26 800 train loss: 0.0231981 valid loss: 0.0250548 P@5: 0.19543 N@5: 0.31283 early stop: 0\n",
      "26 1600 train loss: 0.0228716 valid loss: 0.0250434 P@5: 0.19000 N@5: 0.30836 early stop: 0\n",
      "26 2400 train loss: 0.0232309 valid loss: 0.0254458 P@5: 0.18743 N@5: 0.30328 early stop: 0\n",
      "27 400 train loss: 0.0225284 valid loss: 0.0253798 P@5: 0.18400 N@5: 0.29619 early stop: 0\n",
      "27 1200 train loss: 0.0223335 valid loss: 0.0249212 P@5: 0.20371 N@5: 0.32824 early stop: 0\n",
      "27 2000 train loss: 0.0231019 valid loss: 0.0252011 P@5: 0.18257 N@5: 0.29317 early stop: 0\n",
      "27 2800 train loss: 0.0226144 valid loss: 0.0249074 P@5: 0.19514 N@5: 0.31909 early stop: 0\n",
      "28 800 train loss: 0.0226337 valid loss: 0.0252951 P@5: 0.18543 N@5: 0.30213 early stop: 0\n",
      "28 1600 train loss: 0.0224478 valid loss: 0.0250910 P@5: 0.19543 N@5: 0.31397 early stop: 0\n",
      "28 2400 train loss: 0.0219746 valid loss: 0.0251001 P@5: 0.19057 N@5: 0.30391 early stop: 0\n",
      "29 400 train loss: 0.0226950 valid loss: 0.0254503 P@5: 0.19400 N@5: 0.30986 early stop: 0\n",
      "29 1200 train loss: 0.0222646 valid loss: 0.0249793 P@5: 0.20086 N@5: 0.31995 early stop: 0\n",
      "29 2000 train loss: 0.0225840 valid loss: 0.0251489 P@5: 0.18971 N@5: 0.30989 early stop: 0\n",
      "29 2800 train loss: 0.0219978 valid loss: 0.0250722 P@5: 0.19143 N@5: 0.30886 early stop: 0\n",
      "30 800 train loss: 0.0215330 valid loss: 0.0251731 P@5: 0.17971 N@5: 0.28484 early stop: 0\n",
      "30 1600 train loss: 0.0223558 valid loss: 0.0247218 P@5: 0.20886 N@5: 0.33211 early stop: 0\n",
      "30 2400 train loss: 0.0218963 valid loss: 0.0248959 P@5: 0.19457 N@5: 0.31447 early stop: 0\n",
      "31 400 train loss: 0.0215239 valid loss: 0.0249021 P@5: 0.20114 N@5: 0.32028 early stop: 0\n",
      "31 1200 train loss: 0.0218609 valid loss: 0.0246571 P@5: 0.20429 N@5: 0.32483 early stop: 0\n",
      "31 2000 train loss: 0.0219751 valid loss: 0.0246484 P@5: 0.20371 N@5: 0.33004 early stop: 0\n",
      "31 2800 train loss: 0.0221302 valid loss: 0.0250602 P@5: 0.19200 N@5: 0.31400 early stop: 0\n",
      "32 800 train loss: 0.0214106 valid loss: 0.0247205 P@5: 0.20343 N@5: 0.32753 early stop: 0\n",
      "32 1600 train loss: 0.0216679 valid loss: 0.0246215 P@5: 0.20314 N@5: 0.32600 early stop: 0\n",
      "32 2400 train loss: 0.0217438 valid loss: 0.0246775 P@5: 0.20286 N@5: 0.32591 early stop: 0\n",
      "33 400 train loss: 0.0208545 valid loss: 0.0247639 P@5: 0.19914 N@5: 0.32899 early stop: 0\n",
      "33 1200 train loss: 0.0215486 valid loss: 0.0251144 P@5: 0.19457 N@5: 0.30971 early stop: 0\n",
      "33 2000 train loss: 0.0205019 valid loss: 0.0251000 P@5: 0.19057 N@5: 0.31112 early stop: 0\n",
      "33 2800 train loss: 0.0218601 valid loss: 0.0250779 P@5: 0.20200 N@5: 0.32542 early stop: 0\n",
      "34 800 train loss: 0.0214011 valid loss: 0.0248211 P@5: 0.20286 N@5: 0.32146 early stop: 0\n",
      "34 1600 train loss: 0.0214596 valid loss: 0.0243793 P@5: 0.20971 N@5: 0.34176 early stop: 0\n",
      "34 2400 train loss: 0.0205806 valid loss: 0.0245649 P@5: 0.20057 N@5: 0.32333 early stop: 0\n",
      "35 400 train loss: 0.0199327 valid loss: 0.0245451 P@5: 0.20400 N@5: 0.32564 early stop: 0\n",
      "35 1200 train loss: 0.0209723 valid loss: 0.0246660 P@5: 0.20600 N@5: 0.32498 early stop: 0\n",
      "35 2000 train loss: 0.0204119 valid loss: 0.0246924 P@5: 0.20657 N@5: 0.32763 early stop: 0\n",
      "35 2800 train loss: 0.0213700 valid loss: 0.0244034 P@5: 0.21314 N@5: 0.33698 early stop: 0\n",
      "36 800 train loss: 0.0205642 valid loss: 0.0248687 P@5: 0.19743 N@5: 0.31653 early stop: 0\n",
      "36 1600 train loss: 0.0206026 valid loss: 0.0246326 P@5: 0.20286 N@5: 0.33091 early stop: 0\n",
      "36 2400 train loss: 0.0207171 valid loss: 0.0247272 P@5: 0.21029 N@5: 0.33515 early stop: 0\n",
      "37 400 train loss: 0.0195114 valid loss: 0.0247820 P@5: 0.19343 N@5: 0.31679 early stop: 0\n",
      "37 1200 train loss: 0.0203367 valid loss: 0.0246572 P@5: 0.20686 N@5: 0.33079 early stop: 0\n",
      "37 2000 train loss: 0.0202709 valid loss: 0.0247914 P@5: 0.20057 N@5: 0.31996 early stop: 0\n",
      "37 2800 train loss: 0.0207411 valid loss: 0.0244560 P@5: 0.20943 N@5: 0.33478 early stop: 0\n",
      "38 800 train loss: 0.0191363 valid loss: 0.0250753 P@5: 0.19629 N@5: 0.30933 early stop: 0\n",
      "38 1600 train loss: 0.0202870 valid loss: 0.0248174 P@5: 0.20486 N@5: 0.32458 early stop: 0\n",
      "38 2400 train loss: 0.0206270 valid loss: 0.0246599 P@5: 0.20629 N@5: 0.33280 early stop: 0\n",
      "39 400 train loss: 0.0198144 valid loss: 0.0247257 P@5: 0.20914 N@5: 0.33051 early stop: 0\n",
      "39 1200 train loss: 0.0196859 valid loss: 0.0248851 P@5: 0.20543 N@5: 0.33218 early stop: 0\n",
      "39 2000 train loss: 0.0198571 valid loss: 0.0247550 P@5: 0.20686 N@5: 0.33041 early stop: 0\n",
      "39 2800 train loss: 0.0197220 valid loss: 0.0246511 P@5: 0.20857 N@5: 0.33402 early stop: 0\n",
      "40 800 train loss: 0.0193225 valid loss: 0.0247144 P@5: 0.21343 N@5: 0.33864 early stop: 0\n",
      "40 1600 train loss: 0.0198498 valid loss: 0.0248783 P@5: 0.20257 N@5: 0.33194 early stop: 0\n",
      "40 2400 train loss: 0.0198264 valid loss: 0.0249267 P@5: 0.20800 N@5: 0.33286 early stop: 0\n",
      "41 400 train loss: 0.0190150 valid loss: 0.0250506 P@5: 0.19429 N@5: 0.31900 early stop: 0\n",
      "41 1200 train loss: 0.0193787 valid loss: 0.0247920 P@5: 0.20429 N@5: 0.33032 early stop: 0\n",
      "41 2000 train loss: 0.0195642 valid loss: 0.0253822 P@5: 0.19914 N@5: 0.31329 early stop: 0\n",
      "41 2800 train loss: 0.0189900 valid loss: 0.0247523 P@5: 0.21200 N@5: 0.34016 early stop: 0\n",
      "42 800 train loss: 0.0188893 valid loss: 0.0245962 P@5: 0.20971 N@5: 0.33536 early stop: 0\n",
      "42 1600 train loss: 0.0189472 valid loss: 0.0249500 P@5: 0.21514 N@5: 0.34924 early stop: 0\n",
      "42 2400 train loss: 0.0195362 valid loss: 0.0252661 P@5: 0.20543 N@5: 0.33067 early stop: 0\n",
      "43 400 train loss: 0.0186826 valid loss: 0.0246686 P@5: 0.20800 N@5: 0.33723 early stop: 0\n",
      "43 1200 train loss: 0.0184383 valid loss: 0.0246028 P@5: 0.21314 N@5: 0.34251 early stop: 0\n",
      "43 2000 train loss: 0.0187491 valid loss: 0.0254405 P@5: 0.19057 N@5: 0.30617 early stop: 0\n",
      "43 2800 train loss: 0.0194246 valid loss: 0.0248934 P@5: 0.21257 N@5: 0.33766 early stop: 0\n",
      "44 800 train loss: 0.0183646 valid loss: 0.0249399 P@5: 0.21086 N@5: 0.32931 early stop: 0\n",
      "44 1600 train loss: 0.0183890 valid loss: 0.0248245 P@5: 0.21086 N@5: 0.33923 early stop: 0\n",
      "44 2400 train loss: 0.0189694 valid loss: 0.0250449 P@5: 0.20286 N@5: 0.32819 early stop: 0\n",
      "45 400 train loss: 0.0181428 valid loss: 0.0248840 P@5: 0.21257 N@5: 0.34518 early stop: 0\n",
      "45 1200 train loss: 0.0177726 valid loss: 0.0249753 P@5: 0.21571 N@5: 0.34787 early stop: 0\n",
      "45 2000 train loss: 0.0187270 valid loss: 0.0249674 P@5: 0.20629 N@5: 0.33578 early stop: 0\n",
      "45 2800 train loss: 0.0192912 valid loss: 0.0247696 P@5: 0.21943 N@5: 0.34622 early stop: 0\n",
      "46 800 train loss: 0.0174398 valid loss: 0.0248201 P@5: 0.20886 N@5: 0.34589 early stop: 0\n",
      "46 1600 train loss: 0.0182886 valid loss: 0.0249581 P@5: 0.20714 N@5: 0.33691 early stop: 0\n",
      "46 2400 train loss: 0.0187732 valid loss: 0.0253059 P@5: 0.21143 N@5: 0.33347 early stop: 0\n",
      "47 400 train loss: 0.0179799 valid loss: 0.0249101 P@5: 0.21629 N@5: 0.34085 early stop: 0\n",
      "47 1200 train loss: 0.0178984 valid loss: 0.0249313 P@5: 0.20800 N@5: 0.33784 early stop: 0\n",
      "47 2000 train loss: 0.0178998 valid loss: 0.0247965 P@5: 0.21286 N@5: 0.34358 early stop: 0\n",
      "47 2800 train loss: 0.0179576 valid loss: 0.0251278 P@5: 0.21314 N@5: 0.34525 early stop: 0\n",
      "48 800 train loss: 0.0170102 valid loss: 0.0253517 P@5: 0.20571 N@5: 0.33552 early stop: 0\n",
      "48 1600 train loss: 0.0176737 valid loss: 0.0251422 P@5: 0.21200 N@5: 0.34091 early stop: 0\n",
      "48 2400 train loss: 0.0184011 valid loss: 0.0247483 P@5: 0.21457 N@5: 0.34785 early stop: 0\n",
      "49 400 train loss: 0.0172802 valid loss: 0.0249247 P@5: 0.21229 N@5: 0.34584 early stop: 0\n",
      "49 1200 train loss: 0.0173438 valid loss: 0.0252449 P@5: 0.21629 N@5: 0.34396 early stop: 0\n",
      "49 2000 train loss: 0.0174809 valid loss: 0.0251828 P@5: 0.20943 N@5: 0.33845 early stop: 0\n",
      "49 2800 train loss: 0.0177885 valid loss: 0.0248929 P@5: 0.21657 N@5: 0.35002 early stop: 0\n",
      "50 800 train loss: 0.0166084 valid loss: 0.0252256 P@5: 0.21200 N@5: 0.34172 early stop: 0\n",
      "50 1600 train loss: 0.0177261 valid loss: 0.0253738 P@5: 0.20829 N@5: 0.33601 early stop: 0\n",
      "50 2400 train loss: 0.0172066 valid loss: 0.0255294 P@5: 0.20343 N@5: 0.33204 early stop: 0\n",
      "51 400 train loss: 0.0169583 valid loss: 0.0253150 P@5: 0.21543 N@5: 0.35163 early stop: 0\n",
      "51 1200 train loss: 0.0170327 valid loss: 0.0255316 P@5: 0.21114 N@5: 0.34007 early stop: 0\n",
      "51 2000 train loss: 0.0174115 valid loss: 0.0252218 P@5: 0.21143 N@5: 0.34071 early stop: 0\n",
      "51 2800 train loss: 0.0169662 valid loss: 0.0250689 P@5: 0.22286 N@5: 0.35796 early stop: 0\n",
      "52 800 train loss: 0.0165775 valid loss: 0.0250186 P@5: 0.21429 N@5: 0.34747 early stop: 0\n",
      "52 1600 train loss: 0.0170686 valid loss: 0.0250694 P@5: 0.21771 N@5: 0.35474 early stop: 0\n",
      "52 2400 train loss: 0.0167655 valid loss: 0.0253615 P@5: 0.20829 N@5: 0.33920 early stop: 0\n",
      "53 400 train loss: 0.0167148 valid loss: 0.0253746 P@5: 0.20743 N@5: 0.33797 early stop: 0\n",
      "53 1200 train loss: 0.0163401 valid loss: 0.0257948 P@5: 0.21086 N@5: 0.34333 early stop: 0\n",
      "53 2000 train loss: 0.0167476 valid loss: 0.0256164 P@5: 0.20514 N@5: 0.33515 early stop: 0\n",
      "53 2800 train loss: 0.0171243 valid loss: 0.0251219 P@5: 0.21800 N@5: 0.35333 early stop: 0\n",
      "54 800 train loss: 0.0159134 valid loss: 0.0257096 P@5: 0.20829 N@5: 0.33582 early stop: 0\n",
      "54 1600 train loss: 0.0165951 valid loss: 0.0259035 P@5: 0.21057 N@5: 0.34521 early stop: 0\n",
      "54 2400 train loss: 0.0163007 valid loss: 0.0264428 P@5: 0.20000 N@5: 0.32030 early stop: 0\n",
      "55 400 train loss: 0.0164637 valid loss: 0.0254998 P@5: 0.20943 N@5: 0.34116 early stop: 0\n",
      "55 1200 train loss: 0.0163856 valid loss: 0.0254265 P@5: 0.21057 N@5: 0.34257 early stop: 0\n",
      "55 2000 train loss: 0.0159302 valid loss: 0.0254081 P@5: 0.21686 N@5: 0.35146 early stop: 0\n",
      "55 2800 train loss: 0.0164500 valid loss: 0.0256312 P@5: 0.20971 N@5: 0.34354 early stop: 0\n",
      "56 800 train loss: 0.0154681 valid loss: 0.0263803 P@5: 0.20829 N@5: 0.32872 early stop: 0\n",
      "56 1600 train loss: 0.0158138 valid loss: 0.0254023 P@5: 0.22286 N@5: 0.35842 early stop: 0\n",
      "56 2400 train loss: 0.0164947 valid loss: 0.0256494 P@5: 0.21400 N@5: 0.34127 early stop: 0\n",
      "57 400 train loss: 0.0156520 valid loss: 0.0255651 P@5: 0.21286 N@5: 0.34093 early stop: 0\n",
      "57 1200 train loss: 0.0154861 valid loss: 0.0258324 P@5: 0.21343 N@5: 0.33810 early stop: 0\n",
      "57 2000 train loss: 0.0160570 valid loss: 0.0258396 P@5: 0.21600 N@5: 0.34733 early stop: 0\n",
      "57 2800 train loss: 0.0160128 valid loss: 0.0256218 P@5: 0.21571 N@5: 0.35500 early stop: 0\n",
      "58 800 train loss: 0.0151193 valid loss: 0.0260742 P@5: 0.21486 N@5: 0.35142 early stop: 0\n",
      "58 1600 train loss: 0.0157010 valid loss: 0.0257842 P@5: 0.21314 N@5: 0.35292 early stop: 0\n",
      "58 2400 train loss: 0.0157430 valid loss: 0.0266333 P@5: 0.20457 N@5: 0.32783 early stop: 0\n",
      "59 400 train loss: 0.0157396 valid loss: 0.0261963 P@5: 0.20371 N@5: 0.33419 early stop: 0\n",
      "59 1200 train loss: 0.0148657 valid loss: 0.0260514 P@5: 0.20800 N@5: 0.34043 early stop: 0\n",
      "59 2000 train loss: 0.0153579 valid loss: 0.0258914 P@5: 0.21457 N@5: 0.34698 early stop: 0\n",
      "59 2800 train loss: 0.0157926 valid loss: 0.0265401 P@5: 0.20286 N@5: 0.32480 early stop: 0\n",
      "60 800 train loss: 0.0145102 valid loss: 0.0261779 P@5: 0.21486 N@5: 0.34766 early stop: 0\n",
      "60 1600 train loss: 0.0149426 valid loss: 0.0257677 P@5: 0.22371 N@5: 0.36117 early stop: 0\n",
      "60 2400 train loss: 0.0153374 valid loss: 0.0261067 P@5: 0.21429 N@5: 0.34971 early stop: 0\n",
      "61 400 train loss: 0.0151372 valid loss: 0.0260213 P@5: 0.21343 N@5: 0.35280 early stop: 0\n",
      "61 1200 train loss: 0.0149233 valid loss: 0.0263990 P@5: 0.20829 N@5: 0.33055 early stop: 0\n",
      "61 2000 train loss: 0.0147154 valid loss: 0.0261541 P@5: 0.21543 N@5: 0.35453 early stop: 0\n",
      "61 2800 train loss: 0.0154525 valid loss: 0.0262223 P@5: 0.22114 N@5: 0.35631 early stop: 0\n",
      "62 800 train loss: 0.0142803 valid loss: 0.0267398 P@5: 0.21143 N@5: 0.33751 early stop: 0\n",
      "62 1600 train loss: 0.0149106 valid loss: 0.0267354 P@5: 0.20857 N@5: 0.34150 early stop: 0\n",
      "62 2400 train loss: 0.0146736 valid loss: 0.0262402 P@5: 0.21314 N@5: 0.34726 early stop: 0\n",
      "63 400 train loss: 0.0142226 valid loss: 0.0266500 P@5: 0.21286 N@5: 0.34578 early stop: 0\n",
      "63 1200 train loss: 0.0141851 valid loss: 0.0268169 P@5: 0.20657 N@5: 0.33369 early stop: 0\n",
      "63 2000 train loss: 0.0147571 valid loss: 0.0263760 P@5: 0.21457 N@5: 0.35153 early stop: 0\n",
      "63 2800 train loss: 0.0151246 valid loss: 0.0269298 P@5: 0.21171 N@5: 0.33655 early stop: 0\n",
      "64 800 train loss: 0.0138364 valid loss: 0.0267783 P@5: 0.20743 N@5: 0.33849 early stop: 0\n",
      "64 1600 train loss: 0.0142984 valid loss: 0.0265736 P@5: 0.21857 N@5: 0.34908 early stop: 0\n",
      "64 2400 train loss: 0.0147074 valid loss: 0.0269846 P@5: 0.21029 N@5: 0.34807 early stop: 0\n",
      "65 400 train loss: 0.0140773 valid loss: 0.0269220 P@5: 0.20743 N@5: 0.33476 early stop: 0\n",
      "65 1200 train loss: 0.0138686 valid loss: 0.0272921 P@5: 0.20429 N@5: 0.33257 early stop: 0\n",
      "65 2000 train loss: 0.0141172 valid loss: 0.0268640 P@5: 0.21543 N@5: 0.35176 early stop: 0\n",
      "65 2800 train loss: 0.0146944 valid loss: 0.0270191 P@5: 0.21257 N@5: 0.34157 early stop: 0\n",
      "66 800 train loss: 0.0136735 valid loss: 0.0275401 P@5: 0.20486 N@5: 0.33089 early stop: 0\n",
      "66 1600 train loss: 0.0138265 valid loss: 0.0272440 P@5: 0.21000 N@5: 0.33306 early stop: 0\n",
      "66 2400 train loss: 0.0139888 valid loss: 0.0270819 P@5: 0.21200 N@5: 0.34451 early stop: 0\n",
      "67 400 train loss: 0.0136657 valid loss: 0.0272436 P@5: 0.20771 N@5: 0.34263 early stop: 0\n",
      "67 1200 train loss: 0.0134277 valid loss: 0.0272672 P@5: 0.20886 N@5: 0.33997 early stop: 0\n",
      "67 2000 train loss: 0.0138713 valid loss: 0.0271482 P@5: 0.21400 N@5: 0.34751 early stop: 0\n",
      "67 2800 train loss: 0.0140289 valid loss: 0.0273346 P@5: 0.20743 N@5: 0.34073 early stop: 0\n",
      "68 800 train loss: 0.0125807 valid loss: 0.0279217 P@5: 0.20743 N@5: 0.33382 early stop: 0\n",
      "68 1600 train loss: 0.0134759 valid loss: 0.0274025 P@5: 0.21114 N@5: 0.34221 early stop: 0\n",
      "68 2400 train loss: 0.0144895 valid loss: 0.0275380 P@5: 0.20971 N@5: 0.34274 early stop: 0\n",
      "69 400 train loss: 0.0135371 valid loss: 0.0273570 P@5: 0.21743 N@5: 0.35102 early stop: 0\n",
      "69 1200 train loss: 0.0130647 valid loss: 0.0281301 P@5: 0.20057 N@5: 0.32241 early stop: 0\n",
      "69 2000 train loss: 0.0128299 valid loss: 0.0276288 P@5: 0.21429 N@5: 0.34925 early stop: 0\n",
      "69 2800 train loss: 0.0141635 valid loss: 0.0277095 P@5: 0.21314 N@5: 0.35133 early stop: 0\n",
      "70 800 train loss: 0.0124117 valid loss: 0.0282244 P@5: 0.20629 N@5: 0.33544 early stop: 0\n",
      "70 1600 train loss: 0.0130704 valid loss: 0.0278009 P@5: 0.21029 N@5: 0.34619 early stop: 0\n",
      "70 2400 train loss: 0.0136134 valid loss: 0.0277911 P@5: 0.21771 N@5: 0.35286 early stop: 0\n",
      "71 400 train loss: 0.0130577 valid loss: 0.0281079 P@5: 0.20600 N@5: 0.33304 early stop: 0\n",
      "71 1200 train loss: 0.0123032 valid loss: 0.0281041 P@5: 0.21600 N@5: 0.34951 early stop: 0\n",
      "71 2000 train loss: 0.0130989 valid loss: 0.0279540 P@5: 0.21286 N@5: 0.34950 early stop: 0\n",
      "71 2800 train loss: 0.0136423 valid loss: 0.0283112 P@5: 0.20257 N@5: 0.32911 early stop: 0\n",
      "72 800 train loss: 0.0122381 valid loss: 0.0282984 P@5: 0.20486 N@5: 0.33701 early stop: 0\n",
      "72 1600 train loss: 0.0128458 valid loss: 0.0284278 P@5: 0.20857 N@5: 0.33585 early stop: 0\n",
      "72 2400 train loss: 0.0129132 valid loss: 0.0282418 P@5: 0.20971 N@5: 0.33593 early stop: 0\n",
      "73 400 train loss: 0.0123394 valid loss: 0.0297646 P@5: 0.19714 N@5: 0.31099 early stop: 0\n",
      "73 1200 train loss: 0.0122358 valid loss: 0.0288300 P@5: 0.19971 N@5: 0.33149 early stop: 0\n",
      "73 2000 train loss: 0.0125616 valid loss: 0.0282496 P@5: 0.21171 N@5: 0.34118 early stop: 0\n",
      "73 2800 train loss: 0.0129550 valid loss: 0.0284833 P@5: 0.20829 N@5: 0.33888 early stop: 0\n",
      "74 800 train loss: 0.0117750 valid loss: 0.0289180 P@5: 0.20771 N@5: 0.33182 early stop: 0\n",
      "74 1600 train loss: 0.0123918 valid loss: 0.0290903 P@5: 0.19714 N@5: 0.32120 early stop: 0\n",
      "74 2400 train loss: 0.0123041 valid loss: 0.0283254 P@5: 0.21257 N@5: 0.34254 early stop: 0\n",
      "75 400 train loss: 0.0123817 valid loss: 0.0290067 P@5: 0.20629 N@5: 0.33756 early stop: 0\n",
      "75 1200 train loss: 0.0117440 valid loss: 0.0294249 P@5: 0.20286 N@5: 0.33001 early stop: 0\n",
      "75 2000 train loss: 0.0116487 valid loss: 0.0289005 P@5: 0.20771 N@5: 0.33471 early stop: 0\n",
      "75 2800 train loss: 0.0129724 valid loss: 0.0291196 P@5: 0.20371 N@5: 0.32985 early stop: 0\n",
      "76 800 train loss: 0.0116389 valid loss: 0.0296533 P@5: 0.19371 N@5: 0.31990 early stop: 0\n",
      "76 1600 train loss: 0.0115249 valid loss: 0.0295066 P@5: 0.20686 N@5: 0.34098 early stop: 0\n",
      "76 2400 train loss: 0.0122453 valid loss: 0.0290472 P@5: 0.20114 N@5: 0.32801 early stop: 0\n",
      "77 400 train loss: 0.0113233 valid loss: 0.0295031 P@5: 0.20600 N@5: 0.33316 early stop: 0\n",
      "77 1200 train loss: 0.0118169 valid loss: 0.0295590 P@5: 0.20400 N@5: 0.33108 early stop: 0\n",
      "77 2000 train loss: 0.0115420 valid loss: 0.0291795 P@5: 0.21486 N@5: 0.34318 early stop: 0\n",
      "77 2800 train loss: 0.0125304 valid loss: 0.0291799 P@5: 0.21029 N@5: 0.34318 early stop: 0\n",
      "78 800 train loss: 0.0111835 valid loss: 0.0301483 P@5: 0.20429 N@5: 0.32683 early stop: 0\n",
      "78 1600 train loss: 0.0114166 valid loss: 0.0297551 P@5: 0.20429 N@5: 0.32514 early stop: 0\n",
      "78 2400 train loss: 0.0118105 valid loss: 0.0298992 P@5: 0.20571 N@5: 0.33074 early stop: 0\n",
      "79 400 train loss: 0.0109723 valid loss: 0.0299120 P@5: 0.20486 N@5: 0.32927 early stop: 0\n",
      "79 1200 train loss: 0.0105864 valid loss: 0.0302126 P@5: 0.20714 N@5: 0.33544 early stop: 0\n",
      "79 2000 train loss: 0.0113404 valid loss: 0.0298625 P@5: 0.20857 N@5: 0.33270 early stop: 0\n",
      "79 2800 train loss: 0.0120491 valid loss: 0.0299145 P@5: 0.21029 N@5: 0.34398 early stop: 0\n",
      "80 800 train loss: 0.0109490 valid loss: 0.0300383 P@5: 0.20771 N@5: 0.33209 early stop: 0\n",
      "80 1600 train loss: 0.0104914 valid loss: 0.0302335 P@5: 0.21000 N@5: 0.34278 early stop: 0\n",
      "80 2400 train loss: 0.0116334 valid loss: 0.0305332 P@5: 0.20657 N@5: 0.33678 early stop: 0\n",
      "81 400 train loss: 0.0108378 valid loss: 0.0302570 P@5: 0.21229 N@5: 0.33925 early stop: 0\n",
      "81 1200 train loss: 0.0101275 valid loss: 0.0309220 P@5: 0.21114 N@5: 0.33412 early stop: 0\n",
      "81 2000 train loss: 0.0115548 valid loss: 0.0307275 P@5: 0.20343 N@5: 0.32963 early stop: 0\n",
      "81 2800 train loss: 0.0112519 valid loss: 0.0307217 P@5: 0.20143 N@5: 0.33015 early stop: 0\n",
      "82 800 train loss: 0.0103044 valid loss: 0.0304135 P@5: 0.21114 N@5: 0.33950 early stop: 0\n",
      "82 1600 train loss: 0.0108012 valid loss: 0.0308077 P@5: 0.20514 N@5: 0.33456 early stop: 0\n",
      "82 2400 train loss: 0.0104185 valid loss: 0.0310100 P@5: 0.20429 N@5: 0.32856 early stop: 0\n",
      "83 400 train loss: 0.0108537 valid loss: 0.0310797 P@5: 0.21171 N@5: 0.34013 early stop: 0\n",
      "83 1200 train loss: 0.0102021 valid loss: 0.0304247 P@5: 0.21000 N@5: 0.33695 early stop: 0\n",
      "83 2000 train loss: 0.0107854 valid loss: 0.0305775 P@5: 0.20886 N@5: 0.33996 early stop: 0\n",
      "83 2800 train loss: 0.0105801 valid loss: 0.0306624 P@5: 0.21114 N@5: 0.34552 early stop: 0\n",
      "84 800 train loss: 0.0101063 valid loss: 0.0308800 P@5: 0.20457 N@5: 0.33283 early stop: 0\n",
      "84 1600 train loss: 0.0102812 valid loss: 0.0315024 P@5: 0.20943 N@5: 0.32960 early stop: 0\n",
      "84 2400 train loss: 0.0103514 valid loss: 0.0312922 P@5: 0.21200 N@5: 0.34234 early stop: 0\n",
      "85 400 train loss: 0.0101064 valid loss: 0.0313544 P@5: 0.21286 N@5: 0.34562 early stop: 0\n",
      "85 1200 train loss: 0.0097292 valid loss: 0.0310591 P@5: 0.20914 N@5: 0.33835 early stop: 0\n",
      "85 2000 train loss: 0.0103237 valid loss: 0.0317413 P@5: 0.20971 N@5: 0.33978 early stop: 0\n",
      "85 2800 train loss: 0.0104301 valid loss: 0.0311113 P@5: 0.20200 N@5: 0.33057 early stop: 0\n",
      "86 800 train loss: 0.0100319 valid loss: 0.0322077 P@5: 0.20057 N@5: 0.32815 early stop: 0\n",
      "86 1600 train loss: 0.0103182 valid loss: 0.0320513 P@5: 0.20229 N@5: 0.33060 early stop: 0\n",
      "86 2400 train loss: 0.0099522 valid loss: 0.0316915 P@5: 0.20600 N@5: 0.33135 early stop: 0\n",
      "87 400 train loss: 0.0095044 valid loss: 0.0322513 P@5: 0.20343 N@5: 0.32249 early stop: 0\n",
      "87 1200 train loss: 0.0092608 valid loss: 0.0320237 P@5: 0.20600 N@5: 0.34348 early stop: 0\n",
      "87 2000 train loss: 0.0097840 valid loss: 0.0321213 P@5: 0.20086 N@5: 0.33019 early stop: 0\n",
      "87 2800 train loss: 0.0103967 valid loss: 0.0319204 P@5: 0.20257 N@5: 0.32748 early stop: 0\n",
      "88 800 train loss: 0.0095286 valid loss: 0.0322487 P@5: 0.20171 N@5: 0.32897 early stop: 0\n",
      "88 1600 train loss: 0.0092503 valid loss: 0.0332473 P@5: 0.20200 N@5: 0.32817 early stop: 0\n",
      "88 2400 train loss: 0.0097469 valid loss: 0.0332499 P@5: 0.20257 N@5: 0.33213 early stop: 0\n",
      "89 400 train loss: 0.0093687 valid loss: 0.0326779 P@5: 0.20371 N@5: 0.33192 early stop: 0\n",
      "89 1200 train loss: 0.0091591 valid loss: 0.0332985 P@5: 0.20171 N@5: 0.32452 early stop: 0\n",
      "89 2000 train loss: 0.0092562 valid loss: 0.0328860 P@5: 0.19286 N@5: 0.32178 early stop: 0\n",
      "89 2800 train loss: 0.0098799 valid loss: 0.0324007 P@5: 0.20914 N@5: 0.34110 early stop: 0\n",
      "90 800 train loss: 0.0087209 valid loss: 0.0336335 P@5: 0.20629 N@5: 0.33213 early stop: 0\n",
      "90 1600 train loss: 0.0087981 valid loss: 0.0332738 P@5: 0.19886 N@5: 0.32645 early stop: 0\n",
      "90 2400 train loss: 0.0101174 valid loss: 0.0332570 P@5: 0.20143 N@5: 0.32546 early stop: 0\n",
      "91 400 train loss: 0.0089236 valid loss: 0.0340557 P@5: 0.19971 N@5: 0.32392 early stop: 0\n",
      "91 1200 train loss: 0.0090160 valid loss: 0.0331140 P@5: 0.20143 N@5: 0.32719 early stop: 0\n",
      "91 2000 train loss: 0.0089629 valid loss: 0.0337873 P@5: 0.20371 N@5: 0.33143 early stop: 0\n",
      "91 2800 train loss: 0.0096057 valid loss: 0.0334367 P@5: 0.19886 N@5: 0.32276 early stop: 0\n",
      "92 800 train loss: 0.0078708 valid loss: 0.0340594 P@5: 0.20429 N@5: 0.32749 early stop: 0\n",
      "92 1600 train loss: 0.0088040 valid loss: 0.0338195 P@5: 0.19971 N@5: 0.32788 early stop: 0\n",
      "92 2400 train loss: 0.0095015 valid loss: 0.0338156 P@5: 0.20229 N@5: 0.32987 early stop: 0\n",
      "93 400 train loss: 0.0088473 valid loss: 0.0337738 P@5: 0.20714 N@5: 0.33719 early stop: 0\n",
      "93 1200 train loss: 0.0084060 valid loss: 0.0343281 P@5: 0.20229 N@5: 0.32707 early stop: 0\n",
      "93 2000 train loss: 0.0084050 valid loss: 0.0344586 P@5: 0.19571 N@5: 0.31851 early stop: 0\n",
      "93 2800 train loss: 0.0095307 valid loss: 0.0337267 P@5: 0.20543 N@5: 0.33072 early stop: 0\n",
      "94 800 train loss: 0.0083318 valid loss: 0.0348127 P@5: 0.19714 N@5: 0.32181 early stop: 0\n",
      "94 1600 train loss: 0.0083530 valid loss: 0.0345777 P@5: 0.19343 N@5: 0.31669 early stop: 0\n",
      "94 2400 train loss: 0.0087043 valid loss: 0.0345279 P@5: 0.19771 N@5: 0.32131 early stop: 0\n",
      "95 400 train loss: 0.0082247 valid loss: 0.0346841 P@5: 0.20200 N@5: 0.32466 early stop: 0\n",
      "95 1200 train loss: 0.0076879 valid loss: 0.0353189 P@5: 0.19771 N@5: 0.31416 early stop: 0\n",
      "95 2000 train loss: 0.0082702 valid loss: 0.0354498 P@5: 0.19743 N@5: 0.32096 early stop: 0\n",
      "95 2800 train loss: 0.0093178 valid loss: 0.0349361 P@5: 0.19029 N@5: 0.30868 early stop: 0\n",
      "96 800 train loss: 0.0081123 valid loss: 0.0362660 P@5: 0.20143 N@5: 0.31908 early stop: 0\n",
      "96 1600 train loss: 0.0081188 valid loss: 0.0355037 P@5: 0.19886 N@5: 0.32274 early stop: 0\n",
      "96 2400 train loss: 0.0080838 valid loss: 0.0347436 P@5: 0.20771 N@5: 0.32803 early stop: 0\n",
      "97 400 train loss: 0.0080584 valid loss: 0.0351560 P@5: 0.20257 N@5: 0.33135 early stop: 0\n",
      "97 1200 train loss: 0.0077420 valid loss: 0.0363935 P@5: 0.19514 N@5: 0.31174 early stop: 0\n",
      "97 2000 train loss: 0.0080914 valid loss: 0.0357133 P@5: 0.20257 N@5: 0.32946 early stop: 0\n",
      "97 2800 train loss: 0.0081942 valid loss: 0.0353677 P@5: 0.20229 N@5: 0.32263 early stop: 0\n",
      "98 800 train loss: 0.0072523 valid loss: 0.0361774 P@5: 0.20000 N@5: 0.31746 early stop: 0\n",
      "98 1600 train loss: 0.0080455 valid loss: 0.0360637 P@5: 0.20257 N@5: 0.32924 early stop: 0\n",
      "98 2400 train loss: 0.0078951 valid loss: 0.0357736 P@5: 0.20257 N@5: 0.32270 early stop: 0\n",
      "99 400 train loss: 0.0074936 valid loss: 0.0366892 P@5: 0.19800 N@5: 0.31997 early stop: 0\n",
      "99 1200 train loss: 0.0075082 valid loss: 0.0364842 P@5: 0.19800 N@5: 0.32431 early stop: 0\n",
      "99 2000 train loss: 0.0076959 valid loss: 0.0363086 P@5: 0.19657 N@5: 0.32171 early stop: 0\n",
      "99 2800 train loss: 0.0082333 valid loss: 0.0359716 P@5: 0.20029 N@5: 0.31785 early stop: 0\n",
      "CPU times: user 2min 8s, sys: 992 ms, total: 2min 9s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, optim_params={\"lr\":1e-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2888127853881279,\n",
       " 0.17990867579908676,\n",
       " 0.12773972602739725,\n",
       " 0.2888127853881279,\n",
       " 0.2868693589903636,\n",
       " 0.3424568924843919]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LDA(num_topics=num_labrls) + CorNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetLDACorrectionNet\"]+metrics+[\"3min 17s + 2min 11s\"]+[\"1.7 Gb + 10 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA(num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 4min 24s, total: 7min 16s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_embs = LDAEmbeddings(300)\n",
    "\n",
    "train_embs = lda_embs.fit_transform(train_texts)\n",
    "val_embs = lda_embs.transform(val_texts)\n",
    "test_embs = lda_embs.transform(test_texts)\n",
    "\n",
    "train_loader = DataLoader(MultiLabelDataset(train_embs, train_labels),\n",
    "                          8, shuffle=True)\n",
    "val_loader = DataLoader(MultiLabelDataset(val_embs, val_labels),\n",
    "                          8, shuffle=False)\n",
    "test_loader = DataLoader(MultiLabelDataset(test_embs, test_labels),\n",
    "                          8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=LDACorrectionNet, \n",
    "              emb_size=300, num_labels=train_labels.shape[1], num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0922166 valid loss: 0.0330804 P@5: 0.08343 N@5: 0.11780 early stop: 0\n",
      "0 1600 train loss: 0.0319736 valid loss: 0.0307707 P@5: 0.07629 N@5: 0.11121 early stop: 0\n",
      "0 2400 train loss: 0.0303546 valid loss: 0.0301470 P@5: 0.08029 N@5: 0.11029 early stop: 0\n",
      "1 400 train loss: 0.0299066 valid loss: 0.0301522 P@5: 0.08057 N@5: 0.11473 early stop: 0\n",
      "1 1200 train loss: 0.0297687 valid loss: 0.0300357 P@5: 0.07514 N@5: 0.11416 early stop: 0\n",
      "1 2000 train loss: 0.0298948 valid loss: 0.0298915 P@5: 0.08114 N@5: 0.10794 early stop: 0\n",
      "1 2800 train loss: 0.0302858 valid loss: 0.0299329 P@5: 0.07114 N@5: 0.10819 early stop: 0\n",
      "2 800 train loss: 0.0293881 valid loss: 0.0301162 P@5: 0.07600 N@5: 0.10943 early stop: 0\n",
      "2 1600 train loss: 0.0292202 valid loss: 0.0300098 P@5: 0.07914 N@5: 0.10702 early stop: 0\n",
      "2 2400 train loss: 0.0305586 valid loss: 0.0299587 P@5: 0.06914 N@5: 0.10423 early stop: 0\n",
      "3 400 train loss: 0.0308814 valid loss: 0.0298849 P@5: 0.08143 N@5: 0.11231 early stop: 0\n",
      "3 1200 train loss: 0.0296178 valid loss: 0.0299862 P@5: 0.07800 N@5: 0.11603 early stop: 0\n",
      "3 2000 train loss: 0.0300236 valid loss: 0.0300489 P@5: 0.07829 N@5: 0.11558 early stop: 0\n",
      "3 2800 train loss: 0.0295428 valid loss: 0.0300001 P@5: 0.07829 N@5: 0.11838 early stop: 0\n",
      "4 800 train loss: 0.0301762 valid loss: 0.0300758 P@5: 0.08486 N@5: 0.11654 early stop: 0\n",
      "4 1600 train loss: 0.0295664 valid loss: 0.0299804 P@5: 0.08829 N@5: 0.12387 early stop: 0\n",
      "4 2400 train loss: 0.0298845 valid loss: 0.0299034 P@5: 0.06943 N@5: 0.10623 early stop: 0\n",
      "5 400 train loss: 0.0297873 valid loss: 0.0299646 P@5: 0.07600 N@5: 0.11100 early stop: 0\n",
      "5 1200 train loss: 0.0298129 valid loss: 0.0298695 P@5: 0.07114 N@5: 0.10945 early stop: 0\n",
      "5 2000 train loss: 0.0294640 valid loss: 0.0300044 P@5: 0.08543 N@5: 0.11556 early stop: 0\n",
      "5 2800 train loss: 0.0305828 valid loss: 0.0299340 P@5: 0.07486 N@5: 0.10904 early stop: 0\n",
      "6 800 train loss: 0.0301457 valid loss: 0.0298940 P@5: 0.08486 N@5: 0.12747 early stop: 0\n",
      "6 1600 train loss: 0.0295341 valid loss: 0.0298836 P@5: 0.08400 N@5: 0.12606 early stop: 0\n",
      "6 2400 train loss: 0.0297515 valid loss: 0.0298842 P@5: 0.07829 N@5: 0.12377 early stop: 0\n",
      "7 400 train loss: 0.0299974 valid loss: 0.0298531 P@5: 0.08943 N@5: 0.12536 early stop: 0\n",
      "7 1200 train loss: 0.0304240 valid loss: 0.0299791 P@5: 0.08171 N@5: 0.11923 early stop: 0\n",
      "7 2000 train loss: 0.0292946 valid loss: 0.0298767 P@5: 0.08029 N@5: 0.12686 early stop: 0\n",
      "7 2800 train loss: 0.0295844 valid loss: 0.0296963 P@5: 0.09229 N@5: 0.12946 early stop: 0\n",
      "8 800 train loss: 0.0301698 valid loss: 0.0298898 P@5: 0.08571 N@5: 0.13164 early stop: 0\n",
      "8 1600 train loss: 0.0289625 valid loss: 0.0297066 P@5: 0.08200 N@5: 0.13317 early stop: 0\n",
      "8 2400 train loss: 0.0299557 valid loss: 0.0297293 P@5: 0.08829 N@5: 0.12899 early stop: 0\n",
      "9 400 train loss: 0.0297271 valid loss: 0.0298754 P@5: 0.09114 N@5: 0.14059 early stop: 0\n",
      "9 1200 train loss: 0.0294241 valid loss: 0.0295852 P@5: 0.09857 N@5: 0.13982 early stop: 0\n",
      "9 2000 train loss: 0.0297445 valid loss: 0.0295560 P@5: 0.08771 N@5: 0.14271 early stop: 0\n",
      "9 2800 train loss: 0.0289631 valid loss: 0.0293197 P@5: 0.09943 N@5: 0.14863 early stop: 0\n",
      "10 800 train loss: 0.0288304 valid loss: 0.0293410 P@5: 0.10143 N@5: 0.17233 early stop: 0\n",
      "10 1600 train loss: 0.0291768 valid loss: 0.0292245 P@5: 0.12086 N@5: 0.18860 early stop: 0\n",
      "10 2400 train loss: 0.0295904 valid loss: 0.0291778 P@5: 0.10086 N@5: 0.15676 early stop: 0\n",
      "11 400 train loss: 0.0293514 valid loss: 0.0292982 P@5: 0.09914 N@5: 0.15100 early stop: 0\n",
      "11 1200 train loss: 0.0287155 valid loss: 0.0291006 P@5: 0.10057 N@5: 0.15019 early stop: 0\n",
      "11 2000 train loss: 0.0291462 valid loss: 0.0289004 P@5: 0.11400 N@5: 0.18218 early stop: 0\n",
      "11 2800 train loss: 0.0292439 valid loss: 0.0288866 P@5: 0.11714 N@5: 0.17170 early stop: 0\n",
      "12 800 train loss: 0.0289339 valid loss: 0.0288320 P@5: 0.11486 N@5: 0.18072 early stop: 0\n",
      "12 1600 train loss: 0.0281425 valid loss: 0.0288144 P@5: 0.10657 N@5: 0.16537 early stop: 0\n",
      "12 2400 train loss: 0.0288679 valid loss: 0.0287059 P@5: 0.11514 N@5: 0.17762 early stop: 0\n",
      "13 400 train loss: 0.0288378 valid loss: 0.0286402 P@5: 0.11486 N@5: 0.18154 early stop: 0\n",
      "13 1200 train loss: 0.0281913 valid loss: 0.0285078 P@5: 0.12400 N@5: 0.18726 early stop: 0\n",
      "13 2000 train loss: 0.0282481 valid loss: 0.0283915 P@5: 0.11029 N@5: 0.18281 early stop: 0\n",
      "13 2800 train loss: 0.0286995 valid loss: 0.0283590 P@5: 0.12114 N@5: 0.18999 early stop: 0\n",
      "14 800 train loss: 0.0281027 valid loss: 0.0284172 P@5: 0.12143 N@5: 0.19123 early stop: 0\n",
      "14 1600 train loss: 0.0280681 valid loss: 0.0281769 P@5: 0.13000 N@5: 0.19392 early stop: 0\n",
      "14 2400 train loss: 0.0281705 valid loss: 0.0281848 P@5: 0.12114 N@5: 0.19430 early stop: 0\n",
      "15 400 train loss: 0.0280171 valid loss: 0.0281655 P@5: 0.12057 N@5: 0.18787 early stop: 0\n",
      "15 1200 train loss: 0.0283715 valid loss: 0.0279751 P@5: 0.12971 N@5: 0.20399 early stop: 0\n",
      "15 2000 train loss: 0.0277037 valid loss: 0.0280588 P@5: 0.12771 N@5: 0.19571 early stop: 0\n",
      "15 2800 train loss: 0.0277754 valid loss: 0.0279881 P@5: 0.12943 N@5: 0.20480 early stop: 0\n",
      "16 800 train loss: 0.0280151 valid loss: 0.0280375 P@5: 0.13314 N@5: 0.20203 early stop: 0\n",
      "16 1600 train loss: 0.0276430 valid loss: 0.0277778 P@5: 0.13000 N@5: 0.20139 early stop: 0\n",
      "16 2400 train loss: 0.0276970 valid loss: 0.0276963 P@5: 0.13343 N@5: 0.20664 early stop: 0\n",
      "17 400 train loss: 0.0271636 valid loss: 0.0277250 P@5: 0.13286 N@5: 0.20796 early stop: 0\n",
      "17 1200 train loss: 0.0271897 valid loss: 0.0276356 P@5: 0.13800 N@5: 0.22445 early stop: 0\n",
      "17 2000 train loss: 0.0279545 valid loss: 0.0277461 P@5: 0.13600 N@5: 0.21340 early stop: 0\n",
      "17 2800 train loss: 0.0272983 valid loss: 0.0274947 P@5: 0.13086 N@5: 0.19826 early stop: 0\n",
      "18 800 train loss: 0.0266348 valid loss: 0.0277700 P@5: 0.13371 N@5: 0.20787 early stop: 0\n",
      "18 1600 train loss: 0.0273776 valid loss: 0.0275199 P@5: 0.14057 N@5: 0.21894 early stop: 0\n",
      "18 2400 train loss: 0.0272682 valid loss: 0.0273829 P@5: 0.14114 N@5: 0.21318 early stop: 0\n",
      "19 400 train loss: 0.0271110 valid loss: 0.0274633 P@5: 0.14486 N@5: 0.22582 early stop: 0\n",
      "19 1200 train loss: 0.0270555 valid loss: 0.0272142 P@5: 0.14057 N@5: 0.22508 early stop: 0\n",
      "19 2000 train loss: 0.0273123 valid loss: 0.0274476 P@5: 0.13229 N@5: 0.21516 early stop: 0\n",
      "19 2800 train loss: 0.0269974 valid loss: 0.0272404 P@5: 0.14000 N@5: 0.22517 early stop: 0\n",
      "20 800 train loss: 0.0260915 valid loss: 0.0273433 P@5: 0.14029 N@5: 0.21708 early stop: 0\n",
      "20 1600 train loss: 0.0271653 valid loss: 0.0272647 P@5: 0.14286 N@5: 0.22004 early stop: 0\n",
      "20 2400 train loss: 0.0272521 valid loss: 0.0271351 P@5: 0.14886 N@5: 0.23374 early stop: 0\n",
      "21 400 train loss: 0.0261742 valid loss: 0.0271613 P@5: 0.14257 N@5: 0.22671 early stop: 0\n",
      "21 1200 train loss: 0.0264264 valid loss: 0.0271094 P@5: 0.14543 N@5: 0.22704 early stop: 0\n",
      "21 2000 train loss: 0.0271652 valid loss: 0.0271081 P@5: 0.13714 N@5: 0.21842 early stop: 0\n",
      "21 2800 train loss: 0.0264990 valid loss: 0.0269797 P@5: 0.14057 N@5: 0.22069 early stop: 0\n",
      "22 800 train loss: 0.0271317 valid loss: 0.0269964 P@5: 0.14343 N@5: 0.22170 early stop: 0\n",
      "22 1600 train loss: 0.0266953 valid loss: 0.0267600 P@5: 0.15257 N@5: 0.23962 early stop: 0\n",
      "22 2400 train loss: 0.0260588 valid loss: 0.0269486 P@5: 0.14629 N@5: 0.23126 early stop: 0\n",
      "23 400 train loss: 0.0256584 valid loss: 0.0270391 P@5: 0.14514 N@5: 0.23045 early stop: 0\n",
      "23 1200 train loss: 0.0266884 valid loss: 0.0267518 P@5: 0.15400 N@5: 0.24416 early stop: 0\n",
      "23 2000 train loss: 0.0261543 valid loss: 0.0268170 P@5: 0.14971 N@5: 0.23143 early stop: 0\n",
      "23 2800 train loss: 0.0258787 valid loss: 0.0267890 P@5: 0.14457 N@5: 0.23383 early stop: 0\n",
      "24 800 train loss: 0.0262451 valid loss: 0.0269036 P@5: 0.14486 N@5: 0.22455 early stop: 0\n",
      "24 1600 train loss: 0.0258213 valid loss: 0.0267311 P@5: 0.15371 N@5: 0.24140 early stop: 0\n",
      "24 2400 train loss: 0.0256917 valid loss: 0.0266648 P@5: 0.15543 N@5: 0.24943 early stop: 0\n",
      "25 400 train loss: 0.0265118 valid loss: 0.0266573 P@5: 0.15343 N@5: 0.24923 early stop: 0\n",
      "25 1200 train loss: 0.0254983 valid loss: 0.0266409 P@5: 0.15714 N@5: 0.24921 early stop: 0\n",
      "25 2000 train loss: 0.0255977 valid loss: 0.0268059 P@5: 0.14086 N@5: 0.22997 early stop: 0\n",
      "25 2800 train loss: 0.0263646 valid loss: 0.0266700 P@5: 0.14886 N@5: 0.24064 early stop: 0\n",
      "26 800 train loss: 0.0252514 valid loss: 0.0265518 P@5: 0.15343 N@5: 0.23737 early stop: 0\n",
      "26 1600 train loss: 0.0255878 valid loss: 0.0265156 P@5: 0.15971 N@5: 0.25175 early stop: 0\n",
      "26 2400 train loss: 0.0258793 valid loss: 0.0266273 P@5: 0.15029 N@5: 0.24144 early stop: 0\n",
      "27 400 train loss: 0.0263403 valid loss: 0.0265595 P@5: 0.15743 N@5: 0.24643 early stop: 0\n",
      "27 1200 train loss: 0.0249243 valid loss: 0.0264826 P@5: 0.15857 N@5: 0.25182 early stop: 0\n",
      "27 2000 train loss: 0.0256795 valid loss: 0.0263851 P@5: 0.15429 N@5: 0.25186 early stop: 0\n",
      "27 2800 train loss: 0.0257241 valid loss: 0.0264319 P@5: 0.16143 N@5: 0.25614 early stop: 0\n",
      "28 800 train loss: 0.0258451 valid loss: 0.0265618 P@5: 0.15029 N@5: 0.24009 early stop: 0\n",
      "28 1600 train loss: 0.0253049 valid loss: 0.0265084 P@5: 0.15686 N@5: 0.24927 early stop: 0\n",
      "28 2400 train loss: 0.0252104 valid loss: 0.0264237 P@5: 0.15229 N@5: 0.23771 early stop: 0\n",
      "29 400 train loss: 0.0254048 valid loss: 0.0265114 P@5: 0.15343 N@5: 0.24500 early stop: 0\n",
      "29 1200 train loss: 0.0251296 valid loss: 0.0262702 P@5: 0.16314 N@5: 0.25231 early stop: 0\n",
      "29 2000 train loss: 0.0249033 valid loss: 0.0263761 P@5: 0.15857 N@5: 0.25274 early stop: 0\n",
      "29 2800 train loss: 0.0253483 valid loss: 0.0263438 P@5: 0.15514 N@5: 0.24977 early stop: 0\n",
      "30 800 train loss: 0.0244692 valid loss: 0.0262955 P@5: 0.16229 N@5: 0.25638 early stop: 0\n",
      "30 1600 train loss: 0.0257399 valid loss: 0.0262891 P@5: 0.16829 N@5: 0.26583 early stop: 0\n",
      "30 2400 train loss: 0.0251622 valid loss: 0.0262488 P@5: 0.16286 N@5: 0.26019 early stop: 0\n",
      "31 400 train loss: 0.0252358 valid loss: 0.0261854 P@5: 0.16743 N@5: 0.26499 early stop: 0\n",
      "31 1200 train loss: 0.0250394 valid loss: 0.0263402 P@5: 0.16829 N@5: 0.27037 early stop: 0\n",
      "31 2000 train loss: 0.0244750 valid loss: 0.0262698 P@5: 0.16886 N@5: 0.26979 early stop: 0\n",
      "31 2800 train loss: 0.0248789 valid loss: 0.0261433 P@5: 0.16171 N@5: 0.25750 early stop: 0\n",
      "32 800 train loss: 0.0246360 valid loss: 0.0260954 P@5: 0.15971 N@5: 0.25204 early stop: 0\n",
      "32 1600 train loss: 0.0251021 valid loss: 0.0261173 P@5: 0.16171 N@5: 0.25456 early stop: 0\n",
      "32 2400 train loss: 0.0250001 valid loss: 0.0263514 P@5: 0.15143 N@5: 0.24162 early stop: 0\n",
      "33 400 train loss: 0.0237809 valid loss: 0.0262449 P@5: 0.16686 N@5: 0.26785 early stop: 0\n",
      "33 1200 train loss: 0.0249904 valid loss: 0.0261849 P@5: 0.17000 N@5: 0.26806 early stop: 0\n",
      "33 2000 train loss: 0.0241916 valid loss: 0.0261881 P@5: 0.15600 N@5: 0.25178 early stop: 0\n",
      "33 2800 train loss: 0.0251533 valid loss: 0.0260206 P@5: 0.16714 N@5: 0.26119 early stop: 0\n",
      "34 800 train loss: 0.0245681 valid loss: 0.0261960 P@5: 0.16743 N@5: 0.25892 early stop: 0\n",
      "34 1600 train loss: 0.0249738 valid loss: 0.0259790 P@5: 0.16600 N@5: 0.26946 early stop: 0\n",
      "34 2400 train loss: 0.0240760 valid loss: 0.0259615 P@5: 0.16543 N@5: 0.26694 early stop: 0\n",
      "35 400 train loss: 0.0241867 valid loss: 0.0262989 P@5: 0.16543 N@5: 0.26662 early stop: 0\n",
      "35 1200 train loss: 0.0237005 valid loss: 0.0260203 P@5: 0.17257 N@5: 0.27421 early stop: 0\n",
      "35 2000 train loss: 0.0240436 valid loss: 0.0259485 P@5: 0.17486 N@5: 0.27546 early stop: 0\n",
      "35 2800 train loss: 0.0252874 valid loss: 0.0259903 P@5: 0.16857 N@5: 0.26981 early stop: 0\n",
      "36 800 train loss: 0.0237283 valid loss: 0.0259948 P@5: 0.16943 N@5: 0.26650 early stop: 0\n",
      "36 1600 train loss: 0.0248915 valid loss: 0.0259066 P@5: 0.16857 N@5: 0.26823 early stop: 0\n",
      "36 2400 train loss: 0.0242616 valid loss: 0.0261099 P@5: 0.15886 N@5: 0.25136 early stop: 0\n",
      "37 400 train loss: 0.0234202 valid loss: 0.0259609 P@5: 0.16943 N@5: 0.26503 early stop: 0\n",
      "37 1200 train loss: 0.0240909 valid loss: 0.0261038 P@5: 0.16314 N@5: 0.26188 early stop: 0\n",
      "37 2000 train loss: 0.0241166 valid loss: 0.0258870 P@5: 0.17229 N@5: 0.27613 early stop: 0\n",
      "37 2800 train loss: 0.0244251 valid loss: 0.0258812 P@5: 0.17000 N@5: 0.26911 early stop: 0\n",
      "38 800 train loss: 0.0249193 valid loss: 0.0257892 P@5: 0.17400 N@5: 0.27763 early stop: 0\n",
      "38 1600 train loss: 0.0235436 valid loss: 0.0260021 P@5: 0.17143 N@5: 0.27738 early stop: 0\n",
      "38 2400 train loss: 0.0234649 valid loss: 0.0258524 P@5: 0.17371 N@5: 0.27226 early stop: 0\n",
      "39 400 train loss: 0.0235945 valid loss: 0.0257276 P@5: 0.17057 N@5: 0.27918 early stop: 0\n",
      "39 1200 train loss: 0.0239571 valid loss: 0.0259855 P@5: 0.16486 N@5: 0.25943 early stop: 0\n",
      "39 2000 train loss: 0.0237287 valid loss: 0.0259469 P@5: 0.16743 N@5: 0.26624 early stop: 0\n",
      "39 2800 train loss: 0.0238427 valid loss: 0.0258747 P@5: 0.16457 N@5: 0.26980 early stop: 0\n",
      "40 800 train loss: 0.0237284 valid loss: 0.0259218 P@5: 0.17086 N@5: 0.26738 early stop: 0\n",
      "40 1600 train loss: 0.0234528 valid loss: 0.0258061 P@5: 0.17371 N@5: 0.27777 early stop: 0\n",
      "40 2400 train loss: 0.0232681 valid loss: 0.0257215 P@5: 0.17171 N@5: 0.27593 early stop: 0\n",
      "41 400 train loss: 0.0242675 valid loss: 0.0257980 P@5: 0.17829 N@5: 0.27226 early stop: 0\n",
      "41 1200 train loss: 0.0238833 valid loss: 0.0258465 P@5: 0.16429 N@5: 0.25771 early stop: 0\n",
      "41 2000 train loss: 0.0234989 valid loss: 0.0256838 P@5: 0.16800 N@5: 0.26977 early stop: 0\n",
      "41 2800 train loss: 0.0231149 valid loss: 0.0256169 P@5: 0.17971 N@5: 0.28644 early stop: 0\n",
      "42 800 train loss: 0.0233351 valid loss: 0.0257628 P@5: 0.17314 N@5: 0.28113 early stop: 0\n",
      "42 1600 train loss: 0.0235001 valid loss: 0.0256979 P@5: 0.17400 N@5: 0.27759 early stop: 0\n",
      "42 2400 train loss: 0.0235194 valid loss: 0.0256272 P@5: 0.17314 N@5: 0.27791 early stop: 0\n",
      "43 400 train loss: 0.0228703 valid loss: 0.0258443 P@5: 0.16543 N@5: 0.25997 early stop: 0\n",
      "43 1200 train loss: 0.0236276 valid loss: 0.0257637 P@5: 0.17286 N@5: 0.27129 early stop: 0\n",
      "43 2000 train loss: 0.0237763 valid loss: 0.0257342 P@5: 0.17200 N@5: 0.27916 early stop: 0\n",
      "43 2800 train loss: 0.0229234 valid loss: 0.0255579 P@5: 0.18057 N@5: 0.28210 early stop: 0\n",
      "44 800 train loss: 0.0227298 valid loss: 0.0256797 P@5: 0.17457 N@5: 0.28048 early stop: 0\n",
      "44 1600 train loss: 0.0233707 valid loss: 0.0256325 P@5: 0.17486 N@5: 0.27935 early stop: 0\n",
      "44 2400 train loss: 0.0234234 valid loss: 0.0257554 P@5: 0.18029 N@5: 0.27835 early stop: 0\n",
      "45 400 train loss: 0.0224703 valid loss: 0.0258371 P@5: 0.17571 N@5: 0.28046 early stop: 0\n",
      "45 1200 train loss: 0.0234866 valid loss: 0.0256513 P@5: 0.17429 N@5: 0.27503 early stop: 0\n",
      "45 2000 train loss: 0.0231219 valid loss: 0.0255532 P@5: 0.17457 N@5: 0.27427 early stop: 0\n",
      "45 2800 train loss: 0.0231924 valid loss: 0.0256167 P@5: 0.17771 N@5: 0.28469 early stop: 0\n",
      "46 800 train loss: 0.0227103 valid loss: 0.0256906 P@5: 0.17314 N@5: 0.28187 early stop: 0\n",
      "46 1600 train loss: 0.0226607 valid loss: 0.0256129 P@5: 0.17486 N@5: 0.27753 early stop: 0\n",
      "46 2400 train loss: 0.0236185 valid loss: 0.0257277 P@5: 0.17714 N@5: 0.28143 early stop: 0\n",
      "47 400 train loss: 0.0224848 valid loss: 0.0255767 P@5: 0.18371 N@5: 0.29056 early stop: 0\n",
      "47 1200 train loss: 0.0229632 valid loss: 0.0255146 P@5: 0.17971 N@5: 0.27529 early stop: 0\n",
      "47 2000 train loss: 0.0228430 valid loss: 0.0255853 P@5: 0.17314 N@5: 0.27314 early stop: 0\n",
      "47 2800 train loss: 0.0229663 valid loss: 0.0255626 P@5: 0.17743 N@5: 0.28710 early stop: 0\n",
      "48 800 train loss: 0.0224512 valid loss: 0.0255278 P@5: 0.18257 N@5: 0.29233 early stop: 0\n",
      "48 1600 train loss: 0.0231028 valid loss: 0.0257049 P@5: 0.17057 N@5: 0.27223 early stop: 0\n",
      "48 2400 train loss: 0.0229549 valid loss: 0.0254798 P@5: 0.18286 N@5: 0.29211 early stop: 0\n",
      "49 400 train loss: 0.0226396 valid loss: 0.0255424 P@5: 0.18400 N@5: 0.29512 early stop: 0\n",
      "49 1200 train loss: 0.0222798 valid loss: 0.0254984 P@5: 0.17657 N@5: 0.27799 early stop: 0\n",
      "49 2000 train loss: 0.0226339 valid loss: 0.0256458 P@5: 0.17800 N@5: 0.28862 early stop: 0\n",
      "49 2800 train loss: 0.0226527 valid loss: 0.0253580 P@5: 0.18429 N@5: 0.29351 early stop: 0\n",
      "50 800 train loss: 0.0223149 valid loss: 0.0254717 P@5: 0.18143 N@5: 0.28768 early stop: 0\n",
      "50 1600 train loss: 0.0227294 valid loss: 0.0255902 P@5: 0.18400 N@5: 0.29035 early stop: 0\n",
      "50 2400 train loss: 0.0223850 valid loss: 0.0253834 P@5: 0.18057 N@5: 0.28897 early stop: 0\n",
      "51 400 train loss: 0.0221540 valid loss: 0.0254042 P@5: 0.18314 N@5: 0.29143 early stop: 0\n",
      "51 1200 train loss: 0.0227003 valid loss: 0.0254917 P@5: 0.18171 N@5: 0.29206 early stop: 0\n",
      "51 2000 train loss: 0.0227053 valid loss: 0.0254810 P@5: 0.19057 N@5: 0.30701 early stop: 0\n",
      "51 2800 train loss: 0.0222292 valid loss: 0.0255452 P@5: 0.17514 N@5: 0.28117 early stop: 0\n",
      "52 800 train loss: 0.0224765 valid loss: 0.0254346 P@5: 0.18571 N@5: 0.29592 early stop: 0\n",
      "52 1600 train loss: 0.0218042 valid loss: 0.0253041 P@5: 0.18457 N@5: 0.29887 early stop: 0\n",
      "52 2400 train loss: 0.0226001 valid loss: 0.0252503 P@5: 0.18457 N@5: 0.29063 early stop: 0\n",
      "53 400 train loss: 0.0214778 valid loss: 0.0255437 P@5: 0.18143 N@5: 0.28759 early stop: 0\n",
      "53 1200 train loss: 0.0223158 valid loss: 0.0254771 P@5: 0.18429 N@5: 0.29101 early stop: 0\n",
      "53 2000 train loss: 0.0225343 valid loss: 0.0254288 P@5: 0.18429 N@5: 0.29807 early stop: 0\n",
      "53 2800 train loss: 0.0225858 valid loss: 0.0252539 P@5: 0.18914 N@5: 0.30006 early stop: 0\n",
      "54 800 train loss: 0.0220829 valid loss: 0.0254726 P@5: 0.18086 N@5: 0.28024 early stop: 0\n",
      "54 1600 train loss: 0.0223332 valid loss: 0.0254070 P@5: 0.18400 N@5: 0.29897 early stop: 0\n",
      "54 2400 train loss: 0.0218288 valid loss: 0.0252922 P@5: 0.18657 N@5: 0.29115 early stop: 0\n",
      "55 400 train loss: 0.0219781 valid loss: 0.0252562 P@5: 0.18457 N@5: 0.29463 early stop: 0\n",
      "55 1200 train loss: 0.0217254 valid loss: 0.0253571 P@5: 0.18314 N@5: 0.28765 early stop: 0\n",
      "55 2000 train loss: 0.0217337 valid loss: 0.0252638 P@5: 0.18171 N@5: 0.28616 early stop: 0\n",
      "55 2800 train loss: 0.0225882 valid loss: 0.0253180 P@5: 0.17886 N@5: 0.29005 early stop: 0\n",
      "56 800 train loss: 0.0215989 valid loss: 0.0253472 P@5: 0.18486 N@5: 0.29861 early stop: 0\n",
      "56 1600 train loss: 0.0217825 valid loss: 0.0254390 P@5: 0.18457 N@5: 0.29434 early stop: 0\n",
      "56 2400 train loss: 0.0219205 valid loss: 0.0251976 P@5: 0.18800 N@5: 0.30331 early stop: 0\n",
      "57 400 train loss: 0.0224458 valid loss: 0.0254282 P@5: 0.18029 N@5: 0.29202 early stop: 0\n",
      "57 1200 train loss: 0.0214131 valid loss: 0.0252701 P@5: 0.18543 N@5: 0.30234 early stop: 0\n",
      "57 2000 train loss: 0.0216798 valid loss: 0.0251676 P@5: 0.19057 N@5: 0.30174 early stop: 0\n",
      "57 2800 train loss: 0.0221365 valid loss: 0.0252996 P@5: 0.18629 N@5: 0.30010 early stop: 0\n",
      "58 800 train loss: 0.0215010 valid loss: 0.0254632 P@5: 0.18429 N@5: 0.29736 early stop: 0\n",
      "58 1600 train loss: 0.0218667 valid loss: 0.0254859 P@5: 0.18257 N@5: 0.29206 early stop: 0\n",
      "58 2400 train loss: 0.0215779 valid loss: 0.0252442 P@5: 0.18143 N@5: 0.28844 early stop: 0\n",
      "59 400 train loss: 0.0216017 valid loss: 0.0252569 P@5: 0.19057 N@5: 0.30554 early stop: 0\n",
      "59 1200 train loss: 0.0217446 valid loss: 0.0252934 P@5: 0.18657 N@5: 0.28878 early stop: 0\n",
      "59 2000 train loss: 0.0213194 valid loss: 0.0252972 P@5: 0.17800 N@5: 0.28418 early stop: 0\n",
      "59 2800 train loss: 0.0218635 valid loss: 0.0253229 P@5: 0.18086 N@5: 0.29563 early stop: 0\n",
      "60 800 train loss: 0.0218409 valid loss: 0.0252298 P@5: 0.18857 N@5: 0.30911 early stop: 0\n",
      "60 1600 train loss: 0.0214089 valid loss: 0.0251997 P@5: 0.18714 N@5: 0.30591 early stop: 0\n",
      "60 2400 train loss: 0.0212714 valid loss: 0.0253421 P@5: 0.18400 N@5: 0.29540 early stop: 0\n",
      "61 400 train loss: 0.0209541 valid loss: 0.0253450 P@5: 0.18486 N@5: 0.29157 early stop: 0\n",
      "61 1200 train loss: 0.0214527 valid loss: 0.0252801 P@5: 0.18657 N@5: 0.29265 early stop: 0\n",
      "61 2000 train loss: 0.0214192 valid loss: 0.0254138 P@5: 0.17743 N@5: 0.28582 early stop: 0\n",
      "61 2800 train loss: 0.0218736 valid loss: 0.0252560 P@5: 0.18400 N@5: 0.30155 early stop: 0\n",
      "62 800 train loss: 0.0206930 valid loss: 0.0252699 P@5: 0.18943 N@5: 0.30830 early stop: 0\n",
      "62 1600 train loss: 0.0217902 valid loss: 0.0251596 P@5: 0.18771 N@5: 0.29714 early stop: 0\n",
      "62 2400 train loss: 0.0213917 valid loss: 0.0251796 P@5: 0.18857 N@5: 0.30374 early stop: 0\n",
      "63 400 train loss: 0.0211153 valid loss: 0.0253058 P@5: 0.18000 N@5: 0.28819 early stop: 0\n",
      "63 1200 train loss: 0.0211725 valid loss: 0.0251300 P@5: 0.19400 N@5: 0.31022 early stop: 0\n",
      "63 2000 train loss: 0.0214389 valid loss: 0.0251259 P@5: 0.19000 N@5: 0.30747 early stop: 0\n",
      "63 2800 train loss: 0.0214389 valid loss: 0.0251990 P@5: 0.18829 N@5: 0.29788 early stop: 0\n",
      "64 800 train loss: 0.0205933 valid loss: 0.0251734 P@5: 0.19143 N@5: 0.30244 early stop: 0\n",
      "64 1600 train loss: 0.0212239 valid loss: 0.0252187 P@5: 0.18286 N@5: 0.29150 early stop: 0\n",
      "64 2400 train loss: 0.0214626 valid loss: 0.0252492 P@5: 0.18457 N@5: 0.29741 early stop: 0\n",
      "65 400 train loss: 0.0211033 valid loss: 0.0251999 P@5: 0.18771 N@5: 0.29603 early stop: 0\n",
      "65 1200 train loss: 0.0209085 valid loss: 0.0251601 P@5: 0.19114 N@5: 0.30241 early stop: 0\n",
      "65 2000 train loss: 0.0213945 valid loss: 0.0251987 P@5: 0.17743 N@5: 0.28727 early stop: 0\n",
      "65 2800 train loss: 0.0209987 valid loss: 0.0251497 P@5: 0.18686 N@5: 0.30374 early stop: 0\n",
      "66 800 train loss: 0.0204181 valid loss: 0.0252622 P@5: 0.18314 N@5: 0.29357 early stop: 0\n",
      "66 1600 train loss: 0.0209826 valid loss: 0.0251251 P@5: 0.18829 N@5: 0.30058 early stop: 0\n",
      "66 2400 train loss: 0.0212984 valid loss: 0.0251804 P@5: 0.18943 N@5: 0.29988 early stop: 0\n",
      "67 400 train loss: 0.0210575 valid loss: 0.0251902 P@5: 0.19314 N@5: 0.30661 early stop: 0\n",
      "67 1200 train loss: 0.0204395 valid loss: 0.0251794 P@5: 0.18571 N@5: 0.30473 early stop: 0\n",
      "67 2000 train loss: 0.0216135 valid loss: 0.0251510 P@5: 0.19086 N@5: 0.30292 early stop: 0\n",
      "67 2800 train loss: 0.0205181 valid loss: 0.0251009 P@5: 0.19657 N@5: 0.30392 early stop: 0\n",
      "68 800 train loss: 0.0204999 valid loss: 0.0251593 P@5: 0.18914 N@5: 0.29565 early stop: 0\n",
      "68 1600 train loss: 0.0209165 valid loss: 0.0251058 P@5: 0.19000 N@5: 0.30807 early stop: 0\n",
      "68 2400 train loss: 0.0207131 valid loss: 0.0251294 P@5: 0.19000 N@5: 0.29688 early stop: 0\n",
      "69 400 train loss: 0.0207529 valid loss: 0.0252236 P@5: 0.18886 N@5: 0.30285 early stop: 0\n",
      "69 1200 train loss: 0.0208706 valid loss: 0.0251289 P@5: 0.19086 N@5: 0.30452 early stop: 0\n",
      "69 2000 train loss: 0.0205905 valid loss: 0.0250551 P@5: 0.20000 N@5: 0.31440 early stop: 0\n",
      "69 2800 train loss: 0.0209666 valid loss: 0.0249701 P@5: 0.19829 N@5: 0.31910 early stop: 0\n",
      "70 800 train loss: 0.0200539 valid loss: 0.0250737 P@5: 0.18943 N@5: 0.30634 early stop: 0\n",
      "70 1600 train loss: 0.0204534 valid loss: 0.0252442 P@5: 0.18457 N@5: 0.30007 early stop: 0\n",
      "70 2400 train loss: 0.0210851 valid loss: 0.0251129 P@5: 0.18600 N@5: 0.30025 early stop: 0\n",
      "71 400 train loss: 0.0210780 valid loss: 0.0252139 P@5: 0.18343 N@5: 0.28996 early stop: 0\n",
      "71 1200 train loss: 0.0199745 valid loss: 0.0251597 P@5: 0.19571 N@5: 0.31116 early stop: 0\n",
      "71 2000 train loss: 0.0212173 valid loss: 0.0250421 P@5: 0.19514 N@5: 0.30967 early stop: 0\n",
      "71 2800 train loss: 0.0201619 valid loss: 0.0250241 P@5: 0.19971 N@5: 0.32189 early stop: 0\n",
      "72 800 train loss: 0.0204666 valid loss: 0.0251511 P@5: 0.19257 N@5: 0.30592 early stop: 0\n",
      "72 1600 train loss: 0.0202787 valid loss: 0.0251340 P@5: 0.19343 N@5: 0.30215 early stop: 0\n",
      "72 2400 train loss: 0.0204368 valid loss: 0.0250842 P@5: 0.19200 N@5: 0.31243 early stop: 0\n",
      "73 400 train loss: 0.0206787 valid loss: 0.0252532 P@5: 0.18686 N@5: 0.30127 early stop: 0\n",
      "73 1200 train loss: 0.0197912 valid loss: 0.0250713 P@5: 0.19057 N@5: 0.31036 early stop: 0\n",
      "73 2000 train loss: 0.0207620 valid loss: 0.0252058 P@5: 0.19086 N@5: 0.30512 early stop: 0\n",
      "73 2800 train loss: 0.0204167 valid loss: 0.0249172 P@5: 0.20571 N@5: 0.32492 early stop: 0\n",
      "74 800 train loss: 0.0197551 valid loss: 0.0250382 P@5: 0.18971 N@5: 0.30773 early stop: 0\n",
      "74 1600 train loss: 0.0204081 valid loss: 0.0249736 P@5: 0.19800 N@5: 0.31285 early stop: 0\n",
      "74 2400 train loss: 0.0205637 valid loss: 0.0250759 P@5: 0.19714 N@5: 0.31228 early stop: 0\n",
      "75 400 train loss: 0.0206906 valid loss: 0.0251725 P@5: 0.19171 N@5: 0.30743 early stop: 0\n",
      "75 1200 train loss: 0.0201516 valid loss: 0.0251196 P@5: 0.19629 N@5: 0.31429 early stop: 0\n",
      "75 2000 train loss: 0.0198986 valid loss: 0.0250353 P@5: 0.19429 N@5: 0.30605 early stop: 0\n",
      "75 2800 train loss: 0.0202657 valid loss: 0.0247979 P@5: 0.20571 N@5: 0.32608 early stop: 0\n",
      "76 800 train loss: 0.0196701 valid loss: 0.0251569 P@5: 0.19029 N@5: 0.30397 early stop: 0\n",
      "76 1600 train loss: 0.0199185 valid loss: 0.0251094 P@5: 0.19057 N@5: 0.30385 early stop: 0\n",
      "76 2400 train loss: 0.0204556 valid loss: 0.0249873 P@5: 0.19714 N@5: 0.31122 early stop: 0\n",
      "77 400 train loss: 0.0204434 valid loss: 0.0252496 P@5: 0.18800 N@5: 0.30058 early stop: 0\n",
      "77 1200 train loss: 0.0199038 valid loss: 0.0250598 P@5: 0.19057 N@5: 0.30588 early stop: 0\n",
      "77 2000 train loss: 0.0198823 valid loss: 0.0249590 P@5: 0.19743 N@5: 0.31550 early stop: 0\n",
      "77 2800 train loss: 0.0202356 valid loss: 0.0249733 P@5: 0.19543 N@5: 0.31021 early stop: 0\n",
      "78 800 train loss: 0.0198321 valid loss: 0.0250718 P@5: 0.19371 N@5: 0.30883 early stop: 0\n",
      "78 1600 train loss: 0.0202133 valid loss: 0.0248806 P@5: 0.19657 N@5: 0.31801 early stop: 0\n",
      "78 2400 train loss: 0.0197491 valid loss: 0.0250066 P@5: 0.19200 N@5: 0.30453 early stop: 0\n",
      "79 400 train loss: 0.0197495 valid loss: 0.0250791 P@5: 0.19571 N@5: 0.31380 early stop: 0\n",
      "79 1200 train loss: 0.0197580 valid loss: 0.0250406 P@5: 0.19971 N@5: 0.31290 early stop: 0\n",
      "79 2000 train loss: 0.0199258 valid loss: 0.0251944 P@5: 0.18486 N@5: 0.30183 early stop: 0\n",
      "79 2800 train loss: 0.0202132 valid loss: 0.0249164 P@5: 0.20000 N@5: 0.31883 early stop: 0\n",
      "80 800 train loss: 0.0199761 valid loss: 0.0249234 P@5: 0.19571 N@5: 0.31417 early stop: 0\n",
      "80 1600 train loss: 0.0190706 valid loss: 0.0250258 P@5: 0.19171 N@5: 0.30419 early stop: 0\n",
      "80 2400 train loss: 0.0198328 valid loss: 0.0250363 P@5: 0.19257 N@5: 0.30834 early stop: 0\n",
      "81 400 train loss: 0.0201390 valid loss: 0.0250056 P@5: 0.19800 N@5: 0.31611 early stop: 0\n",
      "81 1200 train loss: 0.0198689 valid loss: 0.0250751 P@5: 0.19971 N@5: 0.31984 early stop: 0\n",
      "81 2000 train loss: 0.0193280 valid loss: 0.0249167 P@5: 0.19714 N@5: 0.31781 early stop: 0\n",
      "81 2800 train loss: 0.0202138 valid loss: 0.0249729 P@5: 0.20000 N@5: 0.31532 early stop: 0\n",
      "82 800 train loss: 0.0192893 valid loss: 0.0249002 P@5: 0.19600 N@5: 0.31956 early stop: 0\n",
      "82 1600 train loss: 0.0197565 valid loss: 0.0248945 P@5: 0.19886 N@5: 0.31933 early stop: 0\n",
      "82 2400 train loss: 0.0195882 valid loss: 0.0249791 P@5: 0.19943 N@5: 0.32187 early stop: 0\n",
      "83 400 train loss: 0.0197824 valid loss: 0.0249633 P@5: 0.20286 N@5: 0.32466 early stop: 0\n",
      "83 1200 train loss: 0.0192505 valid loss: 0.0250007 P@5: 0.19629 N@5: 0.31950 early stop: 0\n",
      "83 2000 train loss: 0.0199247 valid loss: 0.0249480 P@5: 0.19971 N@5: 0.32000 early stop: 0\n",
      "83 2800 train loss: 0.0196783 valid loss: 0.0249872 P@5: 0.19971 N@5: 0.31076 early stop: 0\n",
      "84 800 train loss: 0.0190688 valid loss: 0.0250102 P@5: 0.19971 N@5: 0.32521 early stop: 0\n",
      "84 1600 train loss: 0.0197242 valid loss: 0.0250044 P@5: 0.19514 N@5: 0.31321 early stop: 0\n",
      "84 2400 train loss: 0.0200103 valid loss: 0.0249542 P@5: 0.19514 N@5: 0.31475 early stop: 0\n",
      "85 400 train loss: 0.0192435 valid loss: 0.0249597 P@5: 0.19314 N@5: 0.31479 early stop: 0\n",
      "85 1200 train loss: 0.0193626 valid loss: 0.0250786 P@5: 0.19600 N@5: 0.31134 early stop: 0\n",
      "85 2000 train loss: 0.0196204 valid loss: 0.0249624 P@5: 0.20057 N@5: 0.31900 early stop: 0\n",
      "85 2800 train loss: 0.0192486 valid loss: 0.0250340 P@5: 0.19457 N@5: 0.30800 early stop: 0\n",
      "86 800 train loss: 0.0190469 valid loss: 0.0250322 P@5: 0.19943 N@5: 0.32210 early stop: 0\n",
      "86 1600 train loss: 0.0191988 valid loss: 0.0248331 P@5: 0.20000 N@5: 0.32747 early stop: 0\n",
      "86 2400 train loss: 0.0196096 valid loss: 0.0248450 P@5: 0.19714 N@5: 0.32145 early stop: 0\n",
      "87 400 train loss: 0.0193222 valid loss: 0.0249233 P@5: 0.20229 N@5: 0.32147 early stop: 0\n",
      "87 1200 train loss: 0.0197604 valid loss: 0.0250531 P@5: 0.19371 N@5: 0.31558 early stop: 0\n",
      "87 2000 train loss: 0.0190928 valid loss: 0.0249827 P@5: 0.19457 N@5: 0.31548 early stop: 0\n",
      "87 2800 train loss: 0.0192839 valid loss: 0.0248851 P@5: 0.19971 N@5: 0.32429 early stop: 0\n",
      "88 800 train loss: 0.0191107 valid loss: 0.0248819 P@5: 0.20057 N@5: 0.32516 early stop: 0\n",
      "88 1600 train loss: 0.0192595 valid loss: 0.0248910 P@5: 0.20114 N@5: 0.32141 early stop: 0\n",
      "88 2400 train loss: 0.0193950 valid loss: 0.0250392 P@5: 0.19629 N@5: 0.32105 early stop: 0\n",
      "89 400 train loss: 0.0192990 valid loss: 0.0249328 P@5: 0.19886 N@5: 0.31831 early stop: 0\n",
      "89 1200 train loss: 0.0190859 valid loss: 0.0249153 P@5: 0.20086 N@5: 0.31124 early stop: 0\n",
      "89 2000 train loss: 0.0193264 valid loss: 0.0250251 P@5: 0.19486 N@5: 0.31452 early stop: 0\n",
      "89 2800 train loss: 0.0187938 valid loss: 0.0249165 P@5: 0.20543 N@5: 0.32383 early stop: 0\n",
      "90 800 train loss: 0.0183985 valid loss: 0.0250956 P@5: 0.18857 N@5: 0.30618 early stop: 0\n",
      "90 1600 train loss: 0.0191644 valid loss: 0.0250003 P@5: 0.19800 N@5: 0.31606 early stop: 0\n",
      "90 2400 train loss: 0.0193050 valid loss: 0.0248604 P@5: 0.20514 N@5: 0.32677 early stop: 0\n",
      "91 400 train loss: 0.0190822 valid loss: 0.0250450 P@5: 0.20171 N@5: 0.32011 early stop: 0\n",
      "91 1200 train loss: 0.0191722 valid loss: 0.0248939 P@5: 0.20514 N@5: 0.32544 early stop: 0\n",
      "91 2000 train loss: 0.0192978 valid loss: 0.0248894 P@5: 0.20457 N@5: 0.32519 early stop: 0\n",
      "91 2800 train loss: 0.0188350 valid loss: 0.0249389 P@5: 0.19886 N@5: 0.32407 early stop: 0\n",
      "92 800 train loss: 0.0184418 valid loss: 0.0248235 P@5: 0.20257 N@5: 0.32908 early stop: 0\n",
      "92 1600 train loss: 0.0191897 valid loss: 0.0249550 P@5: 0.20029 N@5: 0.32694 early stop: 0\n",
      "92 2400 train loss: 0.0190695 valid loss: 0.0250641 P@5: 0.19600 N@5: 0.30967 early stop: 0\n",
      "93 400 train loss: 0.0187492 valid loss: 0.0249442 P@5: 0.19800 N@5: 0.31878 early stop: 0\n",
      "93 1200 train loss: 0.0185063 valid loss: 0.0248886 P@5: 0.20057 N@5: 0.32332 early stop: 0\n",
      "93 2000 train loss: 0.0190710 valid loss: 0.0249282 P@5: 0.19829 N@5: 0.32228 early stop: 0\n",
      "93 2800 train loss: 0.0193592 valid loss: 0.0249099 P@5: 0.20257 N@5: 0.32651 early stop: 0\n",
      "94 800 train loss: 0.0186465 valid loss: 0.0249550 P@5: 0.20343 N@5: 0.32268 early stop: 0\n",
      "94 1600 train loss: 0.0183957 valid loss: 0.0249335 P@5: 0.20200 N@5: 0.32243 early stop: 0\n",
      "94 2400 train loss: 0.0193051 valid loss: 0.0248894 P@5: 0.20000 N@5: 0.32221 early stop: 0\n",
      "95 400 train loss: 0.0184134 valid loss: 0.0248908 P@5: 0.20514 N@5: 0.32235 early stop: 0\n",
      "95 1200 train loss: 0.0190787 valid loss: 0.0249620 P@5: 0.20286 N@5: 0.32091 early stop: 0\n",
      "95 2000 train loss: 0.0184791 valid loss: 0.0248301 P@5: 0.20371 N@5: 0.32682 early stop: 0\n",
      "95 2800 train loss: 0.0189671 valid loss: 0.0249030 P@5: 0.19829 N@5: 0.31864 early stop: 0\n",
      "96 800 train loss: 0.0183734 valid loss: 0.0249775 P@5: 0.20114 N@5: 0.32717 early stop: 0\n",
      "96 1600 train loss: 0.0193066 valid loss: 0.0248881 P@5: 0.20143 N@5: 0.32785 early stop: 0\n",
      "96 2400 train loss: 0.0181950 valid loss: 0.0249459 P@5: 0.19971 N@5: 0.32383 early stop: 0\n",
      "97 400 train loss: 0.0181591 valid loss: 0.0248131 P@5: 0.20600 N@5: 0.33506 early stop: 0\n",
      "97 1200 train loss: 0.0189472 valid loss: 0.0248345 P@5: 0.20600 N@5: 0.32630 early stop: 0\n",
      "97 2000 train loss: 0.0189683 valid loss: 0.0249884 P@5: 0.19914 N@5: 0.32183 early stop: 0\n",
      "97 2800 train loss: 0.0183318 valid loss: 0.0248860 P@5: 0.20486 N@5: 0.33188 early stop: 0\n",
      "98 800 train loss: 0.0188060 valid loss: 0.0249997 P@5: 0.20057 N@5: 0.32336 early stop: 0\n",
      "98 1600 train loss: 0.0187876 valid loss: 0.0248779 P@5: 0.20314 N@5: 0.32706 early stop: 0\n",
      "98 2400 train loss: 0.0182047 valid loss: 0.0248687 P@5: 0.20371 N@5: 0.32596 early stop: 0\n",
      "99 400 train loss: 0.0184041 valid loss: 0.0249358 P@5: 0.20486 N@5: 0.33315 early stop: 0\n",
      "99 1200 train loss: 0.0180693 valid loss: 0.0249092 P@5: 0.20000 N@5: 0.31798 early stop: 0\n",
      "99 2000 train loss: 0.0187147 valid loss: 0.0248306 P@5: 0.21057 N@5: 0.33280 early stop: 0\n",
      "99 2800 train loss: 0.0183553 valid loss: 0.0249927 P@5: 0.19886 N@5: 0.32152 early stop: 0\n",
      "CPU times: user 1min 33s, sys: 867 ms, total: 1min 34s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict:   0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3139269406392694,\n",
       " 0.18881278538812785,\n",
       " 0.12990867579908677,\n",
       " 0.3139269406392694,\n",
       " 0.3007988194140575,\n",
       " 0.3532950178017858]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LDA(num_topics=300)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"LDACorrectionNet(num_topics=300)\"]+metrics+[\"1min 17s + 1min 35s\"]+[\"836 Mb + 8 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDACorrectionNetLarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_embs = LDAEmbeddings(train_labels.shape[1])\n",
    "\n",
    "train_embs = lda_embs.fit_transform(train_texts)\n",
    "val_embs = lda_embs.transform(val_texts)\n",
    "test_embs = lda_embs.transform(test_texts)\n",
    "\n",
    "train_loader = DataLoader(MultiLabelDataset(train_embs, train_labels),\n",
    "                          8, shuffle=True)\n",
    "val_loader = DataLoader(MultiLabelDataset(val_embs, val_labels),\n",
    "                          8, shuffle=False)\n",
    "test_loader = DataLoader(MultiLabelDataset(test_embs, test_labels),\n",
    "                          8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=LDACorrectionNetLarge,\n",
    "              emb_size=600, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1], hidden_states=[900, 1500, 900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.1989133 valid loss: 0.0463999 P@5: 0.07171 N@5: 0.10479 early stop: 0\n",
      "0 1600 train loss: 0.0580269 valid loss: 0.0364868 P@5: 0.06029 N@5: 0.07257 early stop: 0\n",
      "0 2400 train loss: 0.0440270 valid loss: 0.0334983 P@5: 0.06829 N@5: 0.09986 early stop: 0\n",
      "1 400 train loss: 0.0364868 valid loss: 0.0314669 P@5: 0.08771 N@5: 0.12554 early stop: 0\n",
      "1 1200 train loss: 0.0316054 valid loss: 0.0312331 P@5: 0.07200 N@5: 0.11445 early stop: 0\n",
      "1 2000 train loss: 0.0324573 valid loss: 0.0310655 P@5: 0.08771 N@5: 0.12237 early stop: 0\n",
      "1 2800 train loss: 0.0317667 valid loss: 0.0304204 P@5: 0.08971 N@5: 0.13921 early stop: 0\n",
      "2 800 train loss: 0.0303106 valid loss: 0.0302631 P@5: 0.09029 N@5: 0.12829 early stop: 0\n",
      "2 1600 train loss: 0.0303209 valid loss: 0.0299626 P@5: 0.09029 N@5: 0.14422 early stop: 0\n",
      "2 2400 train loss: 0.0303481 valid loss: 0.0297050 P@5: 0.09486 N@5: 0.14484 early stop: 0\n",
      "3 400 train loss: 0.0298457 valid loss: 0.0295077 P@5: 0.09829 N@5: 0.14074 early stop: 0\n",
      "3 1200 train loss: 0.0297452 valid loss: 0.0295666 P@5: 0.10771 N@5: 0.16682 early stop: 0\n",
      "3 2000 train loss: 0.0291906 valid loss: 0.0293506 P@5: 0.10686 N@5: 0.17000 early stop: 0\n",
      "3 2800 train loss: 0.0296997 valid loss: 0.0292623 P@5: 0.10543 N@5: 0.17475 early stop: 0\n",
      "4 800 train loss: 0.0291925 valid loss: 0.0289630 P@5: 0.12200 N@5: 0.18715 early stop: 0\n",
      "4 1600 train loss: 0.0292380 valid loss: 0.0287449 P@5: 0.12000 N@5: 0.18665 early stop: 0\n",
      "4 2400 train loss: 0.0289077 valid loss: 0.0286495 P@5: 0.12686 N@5: 0.19133 early stop: 0\n",
      "5 400 train loss: 0.0277056 valid loss: 0.0284408 P@5: 0.11457 N@5: 0.18039 early stop: 0\n",
      "5 1200 train loss: 0.0283293 valid loss: 0.0282469 P@5: 0.13229 N@5: 0.20453 early stop: 0\n",
      "5 2000 train loss: 0.0285338 valid loss: 0.0278251 P@5: 0.13629 N@5: 0.21045 early stop: 0\n",
      "5 2800 train loss: 0.0277496 valid loss: 0.0279371 P@5: 0.13400 N@5: 0.20784 early stop: 0\n",
      "6 800 train loss: 0.0272961 valid loss: 0.0279125 P@5: 0.14229 N@5: 0.22247 early stop: 0\n",
      "6 1600 train loss: 0.0272714 valid loss: 0.0275386 P@5: 0.14057 N@5: 0.23177 early stop: 0\n",
      "6 2400 train loss: 0.0274802 valid loss: 0.0271484 P@5: 0.15114 N@5: 0.23925 early stop: 0\n",
      "7 400 train loss: 0.0272698 valid loss: 0.0271946 P@5: 0.14914 N@5: 0.23609 early stop: 0\n",
      "7 1200 train loss: 0.0267868 valid loss: 0.0269734 P@5: 0.15629 N@5: 0.24623 early stop: 0\n",
      "7 2000 train loss: 0.0265988 valid loss: 0.0270388 P@5: 0.15086 N@5: 0.24083 early stop: 0\n",
      "7 2800 train loss: 0.0260285 valid loss: 0.0265746 P@5: 0.16343 N@5: 0.26078 early stop: 0\n",
      "8 800 train loss: 0.0253166 valid loss: 0.0265294 P@5: 0.16429 N@5: 0.26438 early stop: 0\n",
      "8 1600 train loss: 0.0254204 valid loss: 0.0264822 P@5: 0.16086 N@5: 0.25887 early stop: 0\n",
      "8 2400 train loss: 0.0269138 valid loss: 0.0260492 P@5: 0.17571 N@5: 0.27700 early stop: 0\n",
      "9 400 train loss: 0.0250101 valid loss: 0.0259424 P@5: 0.17429 N@5: 0.27853 early stop: 0\n",
      "9 1200 train loss: 0.0251194 valid loss: 0.0259425 P@5: 0.17486 N@5: 0.27852 early stop: 0\n",
      "9 2000 train loss: 0.0249494 valid loss: 0.0257183 P@5: 0.17486 N@5: 0.28156 early stop: 0\n",
      "9 2800 train loss: 0.0251256 valid loss: 0.0255138 P@5: 0.17486 N@5: 0.28818 early stop: 0\n",
      "10 800 train loss: 0.0238941 valid loss: 0.0253196 P@5: 0.18571 N@5: 0.29779 early stop: 0\n",
      "10 1600 train loss: 0.0242224 valid loss: 0.0253115 P@5: 0.18286 N@5: 0.29487 early stop: 0\n",
      "10 2400 train loss: 0.0243546 valid loss: 0.0253503 P@5: 0.18514 N@5: 0.29898 early stop: 0\n",
      "11 400 train loss: 0.0238154 valid loss: 0.0251127 P@5: 0.18943 N@5: 0.31091 early stop: 0\n",
      "11 1200 train loss: 0.0234329 valid loss: 0.0252745 P@5: 0.18429 N@5: 0.30158 early stop: 0\n",
      "11 2000 train loss: 0.0234643 valid loss: 0.0246893 P@5: 0.19571 N@5: 0.31709 early stop: 0\n",
      "11 2800 train loss: 0.0233853 valid loss: 0.0247872 P@5: 0.19629 N@5: 0.31424 early stop: 0\n",
      "12 800 train loss: 0.0221947 valid loss: 0.0248752 P@5: 0.19429 N@5: 0.31377 early stop: 0\n",
      "12 1600 train loss: 0.0228670 valid loss: 0.0246107 P@5: 0.20229 N@5: 0.33193 early stop: 0\n",
      "12 2400 train loss: 0.0229058 valid loss: 0.0246687 P@5: 0.20829 N@5: 0.33353 early stop: 0\n",
      "13 400 train loss: 0.0216924 valid loss: 0.0245534 P@5: 0.20771 N@5: 0.33509 early stop: 0\n",
      "13 1200 train loss: 0.0219440 valid loss: 0.0244115 P@5: 0.20886 N@5: 0.33578 early stop: 0\n",
      "13 2000 train loss: 0.0218811 valid loss: 0.0243313 P@5: 0.21257 N@5: 0.34495 early stop: 0\n",
      "13 2800 train loss: 0.0227153 valid loss: 0.0242986 P@5: 0.20971 N@5: 0.33745 early stop: 0\n",
      "14 800 train loss: 0.0205237 valid loss: 0.0243853 P@5: 0.20343 N@5: 0.33608 early stop: 0\n",
      "14 1600 train loss: 0.0216557 valid loss: 0.0243514 P@5: 0.21486 N@5: 0.35009 early stop: 0\n",
      "14 2400 train loss: 0.0211991 valid loss: 0.0240737 P@5: 0.21943 N@5: 0.35383 early stop: 0\n",
      "15 400 train loss: 0.0206666 valid loss: 0.0242783 P@5: 0.21400 N@5: 0.35355 early stop: 0\n",
      "15 1200 train loss: 0.0203660 valid loss: 0.0244092 P@5: 0.21629 N@5: 0.35583 early stop: 0\n",
      "15 2000 train loss: 0.0208020 valid loss: 0.0245339 P@5: 0.21343 N@5: 0.35072 early stop: 0\n",
      "15 2800 train loss: 0.0207002 valid loss: 0.0241115 P@5: 0.22514 N@5: 0.36325 early stop: 0\n",
      "16 800 train loss: 0.0194077 valid loss: 0.0244651 P@5: 0.21857 N@5: 0.35421 early stop: 0\n",
      "16 1600 train loss: 0.0197114 valid loss: 0.0244156 P@5: 0.21829 N@5: 0.34814 early stop: 0\n",
      "16 2400 train loss: 0.0205303 valid loss: 0.0242115 P@5: 0.22400 N@5: 0.37355 early stop: 0\n",
      "17 400 train loss: 0.0191726 valid loss: 0.0240832 P@5: 0.22143 N@5: 0.35923 early stop: 0\n",
      "17 1200 train loss: 0.0190595 valid loss: 0.0243931 P@5: 0.21371 N@5: 0.35499 early stop: 0\n",
      "17 2000 train loss: 0.0194502 valid loss: 0.0243933 P@5: 0.21457 N@5: 0.35325 early stop: 0\n",
      "17 2800 train loss: 0.0200089 valid loss: 0.0242603 P@5: 0.22229 N@5: 0.35838 early stop: 0\n",
      "18 800 train loss: 0.0178630 valid loss: 0.0243399 P@5: 0.22257 N@5: 0.36457 early stop: 0\n",
      "18 1600 train loss: 0.0184031 valid loss: 0.0244138 P@5: 0.22200 N@5: 0.36280 early stop: 0\n",
      "18 2400 train loss: 0.0194204 valid loss: 0.0244333 P@5: 0.21886 N@5: 0.35877 early stop: 0\n",
      "19 400 train loss: 0.0177768 valid loss: 0.0242903 P@5: 0.22543 N@5: 0.36782 early stop: 0\n",
      "19 1200 train loss: 0.0179214 valid loss: 0.0245126 P@5: 0.22486 N@5: 0.36011 early stop: 0\n",
      "19 2000 train loss: 0.0180292 valid loss: 0.0246993 P@5: 0.21686 N@5: 0.35717 early stop: 0\n",
      "19 2800 train loss: 0.0189264 valid loss: 0.0244860 P@5: 0.22171 N@5: 0.36493 early stop: 0\n",
      "20 800 train loss: 0.0168493 valid loss: 0.0246538 P@5: 0.22029 N@5: 0.36677 early stop: 0\n",
      "20 1600 train loss: 0.0173376 valid loss: 0.0244536 P@5: 0.22571 N@5: 0.37315 early stop: 0\n",
      "20 2400 train loss: 0.0178432 valid loss: 0.0247541 P@5: 0.21829 N@5: 0.36241 early stop: 0\n",
      "21 400 train loss: 0.0176553 valid loss: 0.0247031 P@5: 0.22286 N@5: 0.36953 early stop: 0\n",
      "21 1200 train loss: 0.0170847 valid loss: 0.0248420 P@5: 0.22286 N@5: 0.36642 early stop: 0\n",
      "21 2000 train loss: 0.0168563 valid loss: 0.0244767 P@5: 0.22629 N@5: 0.37249 early stop: 0\n",
      "21 2800 train loss: 0.0175622 valid loss: 0.0246150 P@5: 0.22514 N@5: 0.36606 early stop: 0\n",
      "22 800 train loss: 0.0162271 valid loss: 0.0248498 P@5: 0.22600 N@5: 0.37313 early stop: 0\n",
      "22 1600 train loss: 0.0162225 valid loss: 0.0248457 P@5: 0.22943 N@5: 0.37275 early stop: 0\n",
      "22 2400 train loss: 0.0169329 valid loss: 0.0250415 P@5: 0.22257 N@5: 0.36508 early stop: 0\n",
      "23 400 train loss: 0.0159453 valid loss: 0.0252336 P@5: 0.22343 N@5: 0.35962 early stop: 0\n",
      "23 1200 train loss: 0.0158334 valid loss: 0.0252895 P@5: 0.22457 N@5: 0.36618 early stop: 0\n",
      "23 2000 train loss: 0.0161391 valid loss: 0.0250458 P@5: 0.22571 N@5: 0.37030 early stop: 0\n",
      "23 2800 train loss: 0.0164735 valid loss: 0.0250022 P@5: 0.22800 N@5: 0.37554 early stop: 0\n",
      "24 800 train loss: 0.0151687 valid loss: 0.0254159 P@5: 0.22429 N@5: 0.36957 early stop: 0\n",
      "24 1600 train loss: 0.0152072 valid loss: 0.0254316 P@5: 0.22229 N@5: 0.36921 early stop: 0\n",
      "24 2400 train loss: 0.0160100 valid loss: 0.0252261 P@5: 0.22857 N@5: 0.36932 early stop: 0\n",
      "25 400 train loss: 0.0150643 valid loss: 0.0256028 P@5: 0.22943 N@5: 0.37492 early stop: 0\n",
      "25 1200 train loss: 0.0147767 valid loss: 0.0256996 P@5: 0.22600 N@5: 0.36900 early stop: 0\n",
      "25 2000 train loss: 0.0155128 valid loss: 0.0255180 P@5: 0.21914 N@5: 0.36045 early stop: 0\n",
      "25 2800 train loss: 0.0158081 valid loss: 0.0252370 P@5: 0.22686 N@5: 0.37159 early stop: 0\n",
      "26 800 train loss: 0.0138802 valid loss: 0.0256960 P@5: 0.22457 N@5: 0.36488 early stop: 0\n",
      "26 1600 train loss: 0.0147515 valid loss: 0.0260868 P@5: 0.21829 N@5: 0.36251 early stop: 0\n",
      "26 2400 train loss: 0.0150776 valid loss: 0.0257762 P@5: 0.22943 N@5: 0.37430 early stop: 0\n",
      "27 400 train loss: 0.0145605 valid loss: 0.0258702 P@5: 0.22686 N@5: 0.37560 early stop: 0\n",
      "27 1200 train loss: 0.0137164 valid loss: 0.0261756 P@5: 0.22743 N@5: 0.37430 early stop: 0\n",
      "27 2000 train loss: 0.0147006 valid loss: 0.0261073 P@5: 0.22429 N@5: 0.37187 early stop: 0\n",
      "27 2800 train loss: 0.0149989 valid loss: 0.0261036 P@5: 0.22829 N@5: 0.37480 early stop: 0\n",
      "28 800 train loss: 0.0130539 valid loss: 0.0263846 P@5: 0.22543 N@5: 0.36647 early stop: 0\n",
      "28 1600 train loss: 0.0142963 valid loss: 0.0263972 P@5: 0.22429 N@5: 0.37271 early stop: 0\n",
      "28 2400 train loss: 0.0143673 valid loss: 0.0263845 P@5: 0.22800 N@5: 0.37271 early stop: 0\n",
      "29 400 train loss: 0.0135687 valid loss: 0.0263828 P@5: 0.22286 N@5: 0.37088 early stop: 0\n",
      "29 1200 train loss: 0.0135707 valid loss: 0.0266095 P@5: 0.22371 N@5: 0.36565 early stop: 0\n",
      "29 2000 train loss: 0.0140272 valid loss: 0.0266635 P@5: 0.22543 N@5: 0.36902 early stop: 0\n",
      "29 2800 train loss: 0.0139621 valid loss: 0.0267178 P@5: 0.21829 N@5: 0.35978 early stop: 0\n",
      "30 800 train loss: 0.0125370 valid loss: 0.0268628 P@5: 0.22714 N@5: 0.37496 early stop: 0\n",
      "30 1600 train loss: 0.0136831 valid loss: 0.0269236 P@5: 0.21971 N@5: 0.36157 early stop: 0\n",
      "30 2400 train loss: 0.0136310 valid loss: 0.0267903 P@5: 0.22629 N@5: 0.37100 early stop: 0\n",
      "31 400 train loss: 0.0126946 valid loss: 0.0270288 P@5: 0.22257 N@5: 0.36750 early stop: 0\n",
      "31 1200 train loss: 0.0122916 valid loss: 0.0271788 P@5: 0.22686 N@5: 0.37323 early stop: 0\n",
      "31 2000 train loss: 0.0134406 valid loss: 0.0271270 P@5: 0.22886 N@5: 0.37684 early stop: 0\n",
      "31 2800 train loss: 0.0135351 valid loss: 0.0273565 P@5: 0.22314 N@5: 0.36717 early stop: 0\n",
      "32 800 train loss: 0.0117517 valid loss: 0.0275039 P@5: 0.22086 N@5: 0.36535 early stop: 0\n",
      "32 1600 train loss: 0.0127293 valid loss: 0.0276343 P@5: 0.22543 N@5: 0.36780 early stop: 0\n",
      "32 2400 train loss: 0.0128993 valid loss: 0.0277121 P@5: 0.22229 N@5: 0.36539 early stop: 0\n",
      "33 400 train loss: 0.0124402 valid loss: 0.0279611 P@5: 0.21686 N@5: 0.35299 early stop: 0\n",
      "33 1200 train loss: 0.0119353 valid loss: 0.0278894 P@5: 0.22743 N@5: 0.37142 early stop: 0\n",
      "33 2000 train loss: 0.0122238 valid loss: 0.0278859 P@5: 0.22514 N@5: 0.36654 early stop: 0\n",
      "33 2800 train loss: 0.0129647 valid loss: 0.0278955 P@5: 0.22229 N@5: 0.36856 early stop: 0\n",
      "34 800 train loss: 0.0109329 valid loss: 0.0281503 P@5: 0.21771 N@5: 0.35653 early stop: 0\n",
      "34 1600 train loss: 0.0120119 valid loss: 0.0283649 P@5: 0.22286 N@5: 0.36914 early stop: 0\n",
      "34 2400 train loss: 0.0125148 valid loss: 0.0281838 P@5: 0.22229 N@5: 0.36020 early stop: 0\n",
      "35 400 train loss: 0.0115060 valid loss: 0.0286159 P@5: 0.22114 N@5: 0.36387 early stop: 0\n",
      "35 1200 train loss: 0.0110561 valid loss: 0.0285917 P@5: 0.22086 N@5: 0.36001 early stop: 0\n",
      "35 2000 train loss: 0.0118174 valid loss: 0.0287475 P@5: 0.22229 N@5: 0.36600 early stop: 0\n",
      "35 2800 train loss: 0.0127312 valid loss: 0.0284892 P@5: 0.22514 N@5: 0.36949 early stop: 0\n",
      "36 800 train loss: 0.0104283 valid loss: 0.0289386 P@5: 0.22171 N@5: 0.36282 early stop: 0\n",
      "36 1600 train loss: 0.0111669 valid loss: 0.0288491 P@5: 0.22657 N@5: 0.36911 early stop: 0\n",
      "36 2400 train loss: 0.0118084 valid loss: 0.0288534 P@5: 0.22943 N@5: 0.37054 early stop: 0\n",
      "37 400 train loss: 0.0115267 valid loss: 0.0288567 P@5: 0.22514 N@5: 0.36750 early stop: 0\n",
      "37 1200 train loss: 0.0105394 valid loss: 0.0293140 P@5: 0.22429 N@5: 0.36440 early stop: 0\n",
      "37 2000 train loss: 0.0115428 valid loss: 0.0292456 P@5: 0.22314 N@5: 0.36414 early stop: 0\n",
      "37 2800 train loss: 0.0116758 valid loss: 0.0294085 P@5: 0.22371 N@5: 0.36206 early stop: 0\n",
      "38 800 train loss: 0.0100959 valid loss: 0.0294945 P@5: 0.22486 N@5: 0.36767 early stop: 0\n",
      "38 1600 train loss: 0.0107736 valid loss: 0.0296453 P@5: 0.22257 N@5: 0.36130 early stop: 0\n",
      "38 2400 train loss: 0.0113364 valid loss: 0.0295114 P@5: 0.21943 N@5: 0.36556 early stop: 0\n",
      "39 400 train loss: 0.0110107 valid loss: 0.0298900 P@5: 0.22714 N@5: 0.36544 early stop: 0\n",
      "39 1200 train loss: 0.0101318 valid loss: 0.0301428 P@5: 0.22257 N@5: 0.36378 early stop: 0\n",
      "39 2000 train loss: 0.0103680 valid loss: 0.0300674 P@5: 0.22686 N@5: 0.37434 early stop: 0\n",
      "39 2800 train loss: 0.0115033 valid loss: 0.0300511 P@5: 0.22400 N@5: 0.36301 early stop: 0\n",
      "40 800 train loss: 0.0095099 valid loss: 0.0305506 P@5: 0.21914 N@5: 0.36018 early stop: 0\n",
      "40 1600 train loss: 0.0098676 valid loss: 0.0307858 P@5: 0.22343 N@5: 0.36314 early stop: 0\n",
      "40 2400 train loss: 0.0108684 valid loss: 0.0304828 P@5: 0.22286 N@5: 0.36531 early stop: 0\n",
      "41 400 train loss: 0.0109243 valid loss: 0.0306244 P@5: 0.22371 N@5: 0.36833 early stop: 0\n",
      "41 1200 train loss: 0.0099728 valid loss: 0.0310377 P@5: 0.22229 N@5: 0.36210 early stop: 0\n",
      "41 2000 train loss: 0.0102090 valid loss: 0.0310729 P@5: 0.22057 N@5: 0.36055 early stop: 0\n",
      "41 2800 train loss: 0.0104294 valid loss: 0.0306536 P@5: 0.22286 N@5: 0.36606 early stop: 0\n",
      "42 800 train loss: 0.0092607 valid loss: 0.0314164 P@5: 0.21629 N@5: 0.35489 early stop: 0\n",
      "42 1600 train loss: 0.0098442 valid loss: 0.0311199 P@5: 0.22286 N@5: 0.36522 early stop: 0\n",
      "42 2400 train loss: 0.0102788 valid loss: 0.0313219 P@5: 0.22600 N@5: 0.37044 early stop: 0\n",
      "43 400 train loss: 0.0096106 valid loss: 0.0312859 P@5: 0.22371 N@5: 0.36790 early stop: 0\n",
      "43 1200 train loss: 0.0093193 valid loss: 0.0313380 P@5: 0.22457 N@5: 0.37267 early stop: 0\n",
      "43 2000 train loss: 0.0101861 valid loss: 0.0311969 P@5: 0.22114 N@5: 0.36303 early stop: 0\n",
      "43 2800 train loss: 0.0100706 valid loss: 0.0317429 P@5: 0.21857 N@5: 0.36094 early stop: 0\n",
      "44 800 train loss: 0.0087961 valid loss: 0.0319382 P@5: 0.22114 N@5: 0.36471 early stop: 0\n",
      "44 1600 train loss: 0.0092325 valid loss: 0.0322423 P@5: 0.22286 N@5: 0.36236 early stop: 0\n",
      "44 2400 train loss: 0.0098041 valid loss: 0.0317286 P@5: 0.22229 N@5: 0.36531 early stop: 0\n",
      "45 400 train loss: 0.0095171 valid loss: 0.0320666 P@5: 0.22657 N@5: 0.36616 early stop: 0\n",
      "45 1200 train loss: 0.0092743 valid loss: 0.0322923 P@5: 0.22657 N@5: 0.37150 early stop: 0\n",
      "45 2000 train loss: 0.0090814 valid loss: 0.0323754 P@5: 0.22286 N@5: 0.36192 early stop: 0\n",
      "45 2800 train loss: 0.0098961 valid loss: 0.0320986 P@5: 0.22514 N@5: 0.36167 early stop: 0\n",
      "46 800 train loss: 0.0085477 valid loss: 0.0328414 P@5: 0.22457 N@5: 0.36543 early stop: 0\n",
      "46 1600 train loss: 0.0089782 valid loss: 0.0329944 P@5: 0.22400 N@5: 0.36514 early stop: 0\n",
      "46 2400 train loss: 0.0093871 valid loss: 0.0332107 P@5: 0.22000 N@5: 0.35266 early stop: 0\n",
      "47 400 train loss: 0.0088185 valid loss: 0.0330109 P@5: 0.22514 N@5: 0.36823 early stop: 0\n",
      "47 1200 train loss: 0.0086599 valid loss: 0.0333802 P@5: 0.22057 N@5: 0.36228 early stop: 0\n",
      "47 2000 train loss: 0.0092963 valid loss: 0.0332932 P@5: 0.22714 N@5: 0.36535 early stop: 0\n",
      "47 2800 train loss: 0.0095037 valid loss: 0.0327300 P@5: 0.22229 N@5: 0.36516 early stop: 0\n",
      "48 800 train loss: 0.0083017 valid loss: 0.0332925 P@5: 0.21886 N@5: 0.36261 early stop: 0\n",
      "48 1600 train loss: 0.0087211 valid loss: 0.0335355 P@5: 0.22229 N@5: 0.35906 early stop: 0\n",
      "48 2400 train loss: 0.0087918 valid loss: 0.0338299 P@5: 0.22314 N@5: 0.36397 early stop: 0\n",
      "49 400 train loss: 0.0083828 valid loss: 0.0338469 P@5: 0.22400 N@5: 0.36386 early stop: 0\n",
      "49 1200 train loss: 0.0077933 valid loss: 0.0342542 P@5: 0.22314 N@5: 0.36162 early stop: 0\n",
      "49 2000 train loss: 0.0092196 valid loss: 0.0338312 P@5: 0.22200 N@5: 0.36939 early stop: 0\n",
      "49 2800 train loss: 0.0094917 valid loss: 0.0339260 P@5: 0.22400 N@5: 0.36224 early stop: 0\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, val_loader, opt_params={\"lr\": 1e-4}, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict:   0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3698630136986301,\n",
       " 0.21484018264840182,\n",
       " 0.13926940639269406,\n",
       " 0.3698630136986301,\n",
       " 0.3510672088705668,\n",
       " 0.3984654584139152]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LargeLDA(num_topics=num_labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"LDACorrectionNetLarge\"]+metrics+[\"3min 56s + 4min 48s\"]+[\"1.7 Gb + 97 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetLDACorrectionNetLarge,\n",
    "              emb_size=600, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1], hidden_states=[900, 1500, 900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.1602747 valid loss: 0.0383697 P@5: 0.07029 N@5: 0.09374 early stop: 0\n",
      "0 1600 train loss: 0.0405256 valid loss: 0.0339828 P@5: 0.06657 N@5: 0.09521 early stop: 0\n",
      "0 2400 train loss: 0.0369752 valid loss: 0.0315858 P@5: 0.07600 N@5: 0.11549 early stop: 0\n",
      "1 400 train loss: 0.0323199 valid loss: 0.0306272 P@5: 0.08686 N@5: 0.12361 early stop: 0\n",
      "1 1200 train loss: 0.0312475 valid loss: 0.0301715 P@5: 0.08486 N@5: 0.13393 early stop: 0\n",
      "1 2000 train loss: 0.0309072 valid loss: 0.0299855 P@5: 0.09429 N@5: 0.14447 early stop: 0\n",
      "1 2800 train loss: 0.0306725 valid loss: 0.0294978 P@5: 0.10543 N@5: 0.16383 early stop: 0\n",
      "2 800 train loss: 0.0297433 valid loss: 0.0293379 P@5: 0.11029 N@5: 0.17198 early stop: 0\n",
      "2 1600 train loss: 0.0293589 valid loss: 0.0293193 P@5: 0.10029 N@5: 0.15905 early stop: 0\n",
      "2 2400 train loss: 0.0294809 valid loss: 0.0287468 P@5: 0.11429 N@5: 0.18663 early stop: 0\n",
      "3 400 train loss: 0.0287873 valid loss: 0.0287276 P@5: 0.12086 N@5: 0.18757 early stop: 0\n",
      "3 1200 train loss: 0.0284481 valid loss: 0.0282992 P@5: 0.12171 N@5: 0.19913 early stop: 0\n",
      "3 2000 train loss: 0.0288199 valid loss: 0.0281440 P@5: 0.12429 N@5: 0.20359 early stop: 0\n",
      "3 2800 train loss: 0.0280756 valid loss: 0.0279143 P@5: 0.13486 N@5: 0.21124 early stop: 0\n",
      "4 800 train loss: 0.0274220 valid loss: 0.0275322 P@5: 0.13943 N@5: 0.22680 early stop: 0\n",
      "4 1600 train loss: 0.0268137 valid loss: 0.0272982 P@5: 0.15314 N@5: 0.24544 early stop: 0\n",
      "4 2400 train loss: 0.0278064 valid loss: 0.0271500 P@5: 0.15343 N@5: 0.23474 early stop: 0\n",
      "5 400 train loss: 0.0271101 valid loss: 0.0269470 P@5: 0.15429 N@5: 0.24885 early stop: 0\n",
      "5 1200 train loss: 0.0263086 valid loss: 0.0265781 P@5: 0.16771 N@5: 0.26458 early stop: 0\n",
      "5 2000 train loss: 0.0264442 valid loss: 0.0266490 P@5: 0.15714 N@5: 0.25173 early stop: 0\n",
      "5 2800 train loss: 0.0261969 valid loss: 0.0261884 P@5: 0.17486 N@5: 0.27908 early stop: 0\n",
      "6 800 train loss: 0.0255019 valid loss: 0.0261729 P@5: 0.16486 N@5: 0.27124 early stop: 0\n",
      "6 1600 train loss: 0.0251636 valid loss: 0.0258871 P@5: 0.17343 N@5: 0.27266 early stop: 0\n",
      "6 2400 train loss: 0.0253691 valid loss: 0.0256280 P@5: 0.18343 N@5: 0.29599 early stop: 0\n",
      "7 400 train loss: 0.0246820 valid loss: 0.0255887 P@5: 0.17829 N@5: 0.28444 early stop: 0\n",
      "7 1200 train loss: 0.0241901 valid loss: 0.0253049 P@5: 0.18629 N@5: 0.29670 early stop: 0\n",
      "7 2000 train loss: 0.0241555 valid loss: 0.0253444 P@5: 0.18457 N@5: 0.30020 early stop: 0\n",
      "7 2800 train loss: 0.0252060 valid loss: 0.0252425 P@5: 0.18743 N@5: 0.30669 early stop: 0\n",
      "8 800 train loss: 0.0233426 valid loss: 0.0250955 P@5: 0.19086 N@5: 0.30985 early stop: 0\n",
      "8 1600 train loss: 0.0236318 valid loss: 0.0247427 P@5: 0.19771 N@5: 0.31735 early stop: 0\n",
      "8 2400 train loss: 0.0233285 valid loss: 0.0247210 P@5: 0.19343 N@5: 0.30944 early stop: 0\n",
      "9 400 train loss: 0.0231360 valid loss: 0.0245237 P@5: 0.20600 N@5: 0.33484 early stop: 0\n",
      "9 1200 train loss: 0.0228251 valid loss: 0.0248225 P@5: 0.19257 N@5: 0.31342 early stop: 0\n",
      "9 2000 train loss: 0.0228509 valid loss: 0.0246123 P@5: 0.19971 N@5: 0.32437 early stop: 0\n",
      "9 2800 train loss: 0.0224979 valid loss: 0.0242603 P@5: 0.20971 N@5: 0.33772 early stop: 0\n",
      "10 800 train loss: 0.0218556 valid loss: 0.0246462 P@5: 0.20086 N@5: 0.32906 early stop: 0\n",
      "10 1600 train loss: 0.0219635 valid loss: 0.0242737 P@5: 0.21171 N@5: 0.34525 early stop: 0\n",
      "10 2400 train loss: 0.0218970 valid loss: 0.0242820 P@5: 0.20457 N@5: 0.33491 early stop: 0\n",
      "11 400 train loss: 0.0216242 valid loss: 0.0242849 P@5: 0.20829 N@5: 0.33268 early stop: 0\n",
      "11 1200 train loss: 0.0206924 valid loss: 0.0243102 P@5: 0.21114 N@5: 0.34281 early stop: 0\n",
      "11 2000 train loss: 0.0205193 valid loss: 0.0244989 P@5: 0.20114 N@5: 0.33139 early stop: 0\n",
      "11 2800 train loss: 0.0222933 valid loss: 0.0241675 P@5: 0.22086 N@5: 0.36159 early stop: 0\n",
      "12 800 train loss: 0.0198387 valid loss: 0.0242710 P@5: 0.21771 N@5: 0.34952 early stop: 0\n",
      "12 1600 train loss: 0.0204764 valid loss: 0.0240628 P@5: 0.21343 N@5: 0.34652 early stop: 0\n",
      "12 2400 train loss: 0.0205945 valid loss: 0.0238796 P@5: 0.21943 N@5: 0.36027 early stop: 0\n",
      "13 400 train loss: 0.0200669 valid loss: 0.0241180 P@5: 0.22086 N@5: 0.35903 early stop: 0\n",
      "13 1200 train loss: 0.0189619 valid loss: 0.0241769 P@5: 0.21771 N@5: 0.35709 early stop: 0\n",
      "13 2000 train loss: 0.0200879 valid loss: 0.0240562 P@5: 0.22314 N@5: 0.36549 early stop: 0\n",
      "13 2800 train loss: 0.0205204 valid loss: 0.0240415 P@5: 0.22371 N@5: 0.36108 early stop: 0\n",
      "14 800 train loss: 0.0191074 valid loss: 0.0240525 P@5: 0.22400 N@5: 0.36023 early stop: 0\n",
      "14 1600 train loss: 0.0185365 valid loss: 0.0240375 P@5: 0.22457 N@5: 0.36720 early stop: 0\n",
      "14 2400 train loss: 0.0193790 valid loss: 0.0242790 P@5: 0.21943 N@5: 0.36643 early stop: 0\n",
      "15 400 train loss: 0.0184191 valid loss: 0.0242048 P@5: 0.22257 N@5: 0.36453 early stop: 0\n",
      "15 1200 train loss: 0.0184356 valid loss: 0.0241844 P@5: 0.21857 N@5: 0.36093 early stop: 0\n",
      "15 2000 train loss: 0.0183435 valid loss: 0.0244295 P@5: 0.22771 N@5: 0.37012 early stop: 0\n",
      "15 2800 train loss: 0.0190923 valid loss: 0.0242666 P@5: 0.21686 N@5: 0.36264 early stop: 0\n",
      "16 800 train loss: 0.0167366 valid loss: 0.0245993 P@5: 0.22200 N@5: 0.35595 early stop: 0\n",
      "16 1600 train loss: 0.0174002 valid loss: 0.0246065 P@5: 0.22657 N@5: 0.36901 early stop: 0\n",
      "16 2400 train loss: 0.0191555 valid loss: 0.0243202 P@5: 0.22800 N@5: 0.37430 early stop: 0\n",
      "17 400 train loss: 0.0176977 valid loss: 0.0245929 P@5: 0.22371 N@5: 0.36574 early stop: 0\n",
      "17 1200 train loss: 0.0169682 valid loss: 0.0243528 P@5: 0.22914 N@5: 0.37493 early stop: 0\n",
      "17 2000 train loss: 0.0171991 valid loss: 0.0244896 P@5: 0.22543 N@5: 0.37623 early stop: 0\n",
      "17 2800 train loss: 0.0177525 valid loss: 0.0245404 P@5: 0.22457 N@5: 0.36710 early stop: 0\n",
      "18 800 train loss: 0.0159665 valid loss: 0.0246756 P@5: 0.22657 N@5: 0.36648 early stop: 0\n",
      "18 1600 train loss: 0.0166598 valid loss: 0.0246925 P@5: 0.23029 N@5: 0.37974 early stop: 0\n",
      "18 2400 train loss: 0.0174701 valid loss: 0.0244857 P@5: 0.23286 N@5: 0.37932 early stop: 0\n",
      "19 400 train loss: 0.0159989 valid loss: 0.0248281 P@5: 0.22714 N@5: 0.37549 early stop: 0\n",
      "19 1200 train loss: 0.0157733 valid loss: 0.0250093 P@5: 0.22400 N@5: 0.37433 early stop: 0\n",
      "19 2000 train loss: 0.0160443 valid loss: 0.0248676 P@5: 0.23114 N@5: 0.38028 early stop: 0\n",
      "19 2800 train loss: 0.0171808 valid loss: 0.0248973 P@5: 0.22771 N@5: 0.37310 early stop: 0\n",
      "20 800 train loss: 0.0152499 valid loss: 0.0249834 P@5: 0.22286 N@5: 0.37104 early stop: 0\n",
      "20 1600 train loss: 0.0154158 valid loss: 0.0253291 P@5: 0.22257 N@5: 0.36460 early stop: 0\n",
      "20 2400 train loss: 0.0160360 valid loss: 0.0252494 P@5: 0.22714 N@5: 0.37595 early stop: 0\n",
      "21 400 train loss: 0.0153056 valid loss: 0.0251886 P@5: 0.22829 N@5: 0.37819 early stop: 0\n",
      "21 1200 train loss: 0.0149503 valid loss: 0.0254786 P@5: 0.22171 N@5: 0.36672 early stop: 0\n",
      "21 2000 train loss: 0.0157436 valid loss: 0.0250953 P@5: 0.23114 N@5: 0.38866 early stop: 0\n",
      "21 2800 train loss: 0.0158294 valid loss: 0.0252332 P@5: 0.23143 N@5: 0.38083 early stop: 0\n",
      "22 800 train loss: 0.0137954 valid loss: 0.0255958 P@5: 0.23114 N@5: 0.37925 early stop: 0\n",
      "22 1600 train loss: 0.0152199 valid loss: 0.0254569 P@5: 0.22714 N@5: 0.37801 early stop: 0\n",
      "22 2400 train loss: 0.0151192 valid loss: 0.0255465 P@5: 0.22829 N@5: 0.37612 early stop: 0\n",
      "23 400 train loss: 0.0142531 valid loss: 0.0256959 P@5: 0.23029 N@5: 0.38326 early stop: 0\n",
      "23 1200 train loss: 0.0141599 valid loss: 0.0257814 P@5: 0.23286 N@5: 0.38814 early stop: 0\n",
      "23 2000 train loss: 0.0150276 valid loss: 0.0257415 P@5: 0.22886 N@5: 0.37916 early stop: 0\n",
      "23 2800 train loss: 0.0145752 valid loss: 0.0259971 P@5: 0.22971 N@5: 0.38178 early stop: 0\n",
      "24 800 train loss: 0.0128899 valid loss: 0.0262296 P@5: 0.23000 N@5: 0.37689 early stop: 0\n",
      "24 1600 train loss: 0.0143722 valid loss: 0.0261324 P@5: 0.22286 N@5: 0.37370 early stop: 0\n",
      "24 2400 train loss: 0.0140875 valid loss: 0.0260834 P@5: 0.23000 N@5: 0.38247 early stop: 0\n",
      "25 400 train loss: 0.0140195 valid loss: 0.0263175 P@5: 0.23029 N@5: 0.38506 early stop: 0\n",
      "25 1200 train loss: 0.0135753 valid loss: 0.0261574 P@5: 0.23029 N@5: 0.38200 early stop: 0\n",
      "25 2000 train loss: 0.0137196 valid loss: 0.0264088 P@5: 0.22971 N@5: 0.37798 early stop: 0\n",
      "25 2800 train loss: 0.0135677 valid loss: 0.0264109 P@5: 0.22857 N@5: 0.37868 early stop: 0\n",
      "26 800 train loss: 0.0124183 valid loss: 0.0265934 P@5: 0.23143 N@5: 0.37974 early stop: 0\n",
      "26 1600 train loss: 0.0131084 valid loss: 0.0268090 P@5: 0.22314 N@5: 0.36690 early stop: 0\n",
      "26 2400 train loss: 0.0138031 valid loss: 0.0266385 P@5: 0.23000 N@5: 0.38226 early stop: 0\n",
      "27 400 train loss: 0.0125886 valid loss: 0.0267227 P@5: 0.23029 N@5: 0.38435 early stop: 0\n",
      "27 1200 train loss: 0.0118141 valid loss: 0.0267646 P@5: 0.23314 N@5: 0.38610 early stop: 0\n",
      "27 2000 train loss: 0.0131223 valid loss: 0.0270408 P@5: 0.22457 N@5: 0.37230 early stop: 0\n",
      "27 2800 train loss: 0.0140048 valid loss: 0.0270042 P@5: 0.22514 N@5: 0.37500 early stop: 0\n",
      "28 800 train loss: 0.0116144 valid loss: 0.0271998 P@5: 0.22829 N@5: 0.38008 early stop: 0\n",
      "28 1600 train loss: 0.0126174 valid loss: 0.0272860 P@5: 0.22686 N@5: 0.37762 early stop: 0\n",
      "28 2400 train loss: 0.0128741 valid loss: 0.0273202 P@5: 0.23086 N@5: 0.37675 early stop: 0\n",
      "29 400 train loss: 0.0120905 valid loss: 0.0273047 P@5: 0.23029 N@5: 0.38124 early stop: 0\n",
      "29 1200 train loss: 0.0117188 valid loss: 0.0276526 P@5: 0.22257 N@5: 0.37262 early stop: 0\n",
      "29 2000 train loss: 0.0121212 valid loss: 0.0275991 P@5: 0.22829 N@5: 0.37850 early stop: 0\n",
      "29 2800 train loss: 0.0126716 valid loss: 0.0278248 P@5: 0.22829 N@5: 0.37989 early stop: 0\n",
      "30 800 train loss: 0.0104990 valid loss: 0.0282078 P@5: 0.22914 N@5: 0.38094 early stop: 0\n",
      "30 1600 train loss: 0.0118887 valid loss: 0.0279538 P@5: 0.22514 N@5: 0.38135 early stop: 0\n",
      "30 2400 train loss: 0.0121447 valid loss: 0.0280168 P@5: 0.23057 N@5: 0.38125 early stop: 0\n",
      "31 400 train loss: 0.0113220 valid loss: 0.0281523 P@5: 0.23086 N@5: 0.37879 early stop: 0\n",
      "31 1200 train loss: 0.0112978 valid loss: 0.0282743 P@5: 0.22971 N@5: 0.37876 early stop: 0\n",
      "31 2000 train loss: 0.0115276 valid loss: 0.0284248 P@5: 0.22029 N@5: 0.36279 early stop: 0\n",
      "31 2800 train loss: 0.0117003 valid loss: 0.0286019 P@5: 0.22743 N@5: 0.37656 early stop: 0\n",
      "32 800 train loss: 0.0101861 valid loss: 0.0287104 P@5: 0.22686 N@5: 0.37927 early stop: 0\n",
      "32 1600 train loss: 0.0109876 valid loss: 0.0287490 P@5: 0.22543 N@5: 0.37568 early stop: 0\n",
      "32 2400 train loss: 0.0115415 valid loss: 0.0288179 P@5: 0.22743 N@5: 0.36632 early stop: 0\n",
      "33 400 train loss: 0.0112615 valid loss: 0.0287276 P@5: 0.23057 N@5: 0.37436 early stop: 0\n",
      "33 1200 train loss: 0.0108500 valid loss: 0.0288757 P@5: 0.23229 N@5: 0.38579 early stop: 0\n",
      "33 2000 train loss: 0.0108685 valid loss: 0.0288653 P@5: 0.22914 N@5: 0.37601 early stop: 0\n",
      "33 2800 train loss: 0.0110194 valid loss: 0.0288994 P@5: 0.22771 N@5: 0.37613 early stop: 0\n",
      "34 800 train loss: 0.0098867 valid loss: 0.0291233 P@5: 0.22686 N@5: 0.38032 early stop: 0\n",
      "34 1600 train loss: 0.0102553 valid loss: 0.0292690 P@5: 0.23229 N@5: 0.37835 early stop: 0\n",
      "34 2400 train loss: 0.0111919 valid loss: 0.0295695 P@5: 0.22400 N@5: 0.37401 early stop: 0\n",
      "35 400 train loss: 0.0102262 valid loss: 0.0296383 P@5: 0.22514 N@5: 0.36962 early stop: 0\n",
      "35 1200 train loss: 0.0100335 valid loss: 0.0295991 P@5: 0.22200 N@5: 0.37221 early stop: 0\n",
      "35 2000 train loss: 0.0103061 valid loss: 0.0298118 P@5: 0.23029 N@5: 0.38105 early stop: 0\n",
      "35 2800 train loss: 0.0108473 valid loss: 0.0297650 P@5: 0.22286 N@5: 0.36959 early stop: 0\n",
      "36 800 train loss: 0.0094578 valid loss: 0.0298929 P@5: 0.22286 N@5: 0.37040 early stop: 0\n",
      "36 1600 train loss: 0.0098267 valid loss: 0.0301284 P@5: 0.22971 N@5: 0.38090 early stop: 0\n",
      "36 2400 train loss: 0.0101137 valid loss: 0.0303589 P@5: 0.22400 N@5: 0.37408 early stop: 0\n",
      "37 400 train loss: 0.0096174 valid loss: 0.0305403 P@5: 0.22286 N@5: 0.37413 early stop: 0\n",
      "37 1200 train loss: 0.0089840 valid loss: 0.0305546 P@5: 0.22743 N@5: 0.37573 early stop: 0\n",
      "37 2000 train loss: 0.0095720 valid loss: 0.0305311 P@5: 0.22286 N@5: 0.36858 early stop: 0\n",
      "37 2800 train loss: 0.0108381 valid loss: 0.0304832 P@5: 0.22943 N@5: 0.37845 early stop: 0\n",
      "38 800 train loss: 0.0085158 valid loss: 0.0307710 P@5: 0.22314 N@5: 0.37286 early stop: 0\n",
      "38 1600 train loss: 0.0098860 valid loss: 0.0308443 P@5: 0.23086 N@5: 0.37847 early stop: 0\n",
      "38 2400 train loss: 0.0097765 valid loss: 0.0307129 P@5: 0.23257 N@5: 0.37800 early stop: 0\n",
      "39 400 train loss: 0.0092682 valid loss: 0.0309250 P@5: 0.22571 N@5: 0.37061 early stop: 0\n",
      "39 1200 train loss: 0.0082623 valid loss: 0.0312812 P@5: 0.22371 N@5: 0.37083 early stop: 0\n",
      "39 2000 train loss: 0.0098168 valid loss: 0.0315803 P@5: 0.22000 N@5: 0.36702 early stop: 0\n",
      "39 2800 train loss: 0.0099544 valid loss: 0.0312919 P@5: 0.22971 N@5: 0.37386 early stop: 0\n",
      "40 800 train loss: 0.0086509 valid loss: 0.0314138 P@5: 0.22371 N@5: 0.36582 early stop: 0\n",
      "40 1600 train loss: 0.0083566 valid loss: 0.0313432 P@5: 0.22600 N@5: 0.37509 early stop: 0\n",
      "40 2400 train loss: 0.0096721 valid loss: 0.0315987 P@5: 0.22714 N@5: 0.37339 early stop: 0\n",
      "41 400 train loss: 0.0086000 valid loss: 0.0316489 P@5: 0.22543 N@5: 0.37473 early stop: 0\n",
      "41 1200 train loss: 0.0081426 valid loss: 0.0321485 P@5: 0.22886 N@5: 0.37490 early stop: 0\n",
      "41 2000 train loss: 0.0089550 valid loss: 0.0320895 P@5: 0.22543 N@5: 0.36994 early stop: 0\n",
      "41 2800 train loss: 0.0098392 valid loss: 0.0318078 P@5: 0.22886 N@5: 0.37707 early stop: 0\n",
      "42 800 train loss: 0.0079138 valid loss: 0.0321888 P@5: 0.22714 N@5: 0.37651 early stop: 0\n",
      "42 1600 train loss: 0.0085652 valid loss: 0.0324650 P@5: 0.22857 N@5: 0.37610 early stop: 0\n",
      "42 2400 train loss: 0.0091189 valid loss: 0.0326190 P@5: 0.22571 N@5: 0.37036 early stop: 0\n",
      "43 400 train loss: 0.0082171 valid loss: 0.0325196 P@5: 0.22800 N@5: 0.37190 early stop: 0\n",
      "43 1200 train loss: 0.0078652 valid loss: 0.0324673 P@5: 0.22829 N@5: 0.37032 early stop: 0\n",
      "43 2000 train loss: 0.0090912 valid loss: 0.0327554 P@5: 0.21686 N@5: 0.35377 early stop: 0\n",
      "43 2800 train loss: 0.0085455 valid loss: 0.0328049 P@5: 0.22657 N@5: 0.37537 early stop: 0\n",
      "44 800 train loss: 0.0075305 valid loss: 0.0331054 P@5: 0.22800 N@5: 0.37168 early stop: 0\n",
      "44 1600 train loss: 0.0073331 valid loss: 0.0331193 P@5: 0.23000 N@5: 0.37195 early stop: 0\n",
      "44 2400 train loss: 0.0090568 valid loss: 0.0327888 P@5: 0.22771 N@5: 0.37604 early stop: 0\n",
      "45 400 train loss: 0.0083494 valid loss: 0.0328966 P@5: 0.22829 N@5: 0.37068 early stop: 0\n",
      "45 1200 train loss: 0.0079410 valid loss: 0.0334702 P@5: 0.22686 N@5: 0.36897 early stop: 0\n",
      "45 2000 train loss: 0.0081399 valid loss: 0.0330880 P@5: 0.22629 N@5: 0.37069 early stop: 0\n",
      "45 2800 train loss: 0.0084068 valid loss: 0.0334280 P@5: 0.22114 N@5: 0.36581 early stop: 0\n",
      "46 800 train loss: 0.0072948 valid loss: 0.0337796 P@5: 0.22429 N@5: 0.37008 early stop: 0\n",
      "46 1600 train loss: 0.0076270 valid loss: 0.0337103 P@5: 0.22314 N@5: 0.36874 early stop: 0\n",
      "46 2400 train loss: 0.0082694 valid loss: 0.0337600 P@5: 0.23143 N@5: 0.37916 early stop: 0\n",
      "47 400 train loss: 0.0076241 valid loss: 0.0339195 P@5: 0.22971 N@5: 0.37684 early stop: 0\n",
      "47 1200 train loss: 0.0076564 valid loss: 0.0344716 P@5: 0.22829 N@5: 0.37378 early stop: 0\n",
      "47 2000 train loss: 0.0079235 valid loss: 0.0342268 P@5: 0.22886 N@5: 0.37116 early stop: 0\n",
      "47 2800 train loss: 0.0079394 valid loss: 0.0340557 P@5: 0.22543 N@5: 0.37087 early stop: 0\n",
      "48 800 train loss: 0.0066270 valid loss: 0.0346293 P@5: 0.22000 N@5: 0.36506 early stop: 0\n",
      "48 1600 train loss: 0.0077835 valid loss: 0.0342709 P@5: 0.22229 N@5: 0.36877 early stop: 0\n",
      "48 2400 train loss: 0.0078639 valid loss: 0.0342231 P@5: 0.22771 N@5: 0.36873 early stop: 0\n",
      "49 400 train loss: 0.0069007 valid loss: 0.0344604 P@5: 0.22400 N@5: 0.36546 early stop: 0\n",
      "49 1200 train loss: 0.0074603 valid loss: 0.0345745 P@5: 0.21886 N@5: 0.35654 early stop: 0\n",
      "49 2000 train loss: 0.0074570 valid loss: 0.0345851 P@5: 0.22229 N@5: 0.36610 early stop: 0\n",
      "49 2800 train loss: 0.0079397 valid loss: 0.0354329 P@5: 0.21743 N@5: 0.35536 early stop: 0\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, val_loader, opt_params={\"lr\": 1e-4}, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict:   0%|          | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3550228310502283,\n",
       " 0.20821917808219179,\n",
       " 0.138013698630137,\n",
       " 0.3550228310502283,\n",
       " 0.3415911468457437,\n",
       " 0.39143371266886173]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LargeLDA(num_topics=num_labels) + CorNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetLDACorrectionNetLarge\"]+metrics+[\"3min 56s + 4min 56s\"]+[\"1.7 Gb + 103 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add initialization from navek embbandings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "navec = Navec.load(\"../data/navec_hudlit_v1_12B_500K_300d_100q.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 548/548 [03:38<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "topics_embs = lda_embs.get_mean_topic_embandings(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=LDACorrectionNet,\n",
    "              emb_size=300, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1], init_embs=topics_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0948657 valid loss: 0.0332008 P@5: 0.08343 N@5: 0.11876 early stop: 0\n",
      "0 1600 train loss: 0.0315414 valid loss: 0.0307066 P@5: 0.07514 N@5: 0.11036 early stop: 0\n",
      "0 2400 train loss: 0.0308744 valid loss: 0.0303563 P@5: 0.07257 N@5: 0.10521 early stop: 0\n",
      "1 400 train loss: 0.0293636 valid loss: 0.0301267 P@5: 0.07200 N@5: 0.10399 early stop: 0\n",
      "1 1200 train loss: 0.0301600 valid loss: 0.0299820 P@5: 0.07200 N@5: 0.10711 early stop: 0\n",
      "1 2000 train loss: 0.0294593 valid loss: 0.0299008 P@5: 0.08457 N@5: 0.11592 early stop: 0\n",
      "1 2800 train loss: 0.0302023 valid loss: 0.0298478 P@5: 0.07314 N@5: 0.10820 early stop: 0\n",
      "2 800 train loss: 0.0297940 valid loss: 0.0300501 P@5: 0.08229 N@5: 0.11867 early stop: 0\n",
      "2 1600 train loss: 0.0301253 valid loss: 0.0299185 P@5: 0.07571 N@5: 0.10692 early stop: 0\n",
      "2 2400 train loss: 0.0299702 valid loss: 0.0299361 P@5: 0.06200 N@5: 0.08502 early stop: 0\n",
      "3 400 train loss: 0.0293192 valid loss: 0.0299867 P@5: 0.07429 N@5: 0.10828 early stop: 0\n",
      "3 1200 train loss: 0.0297367 valid loss: 0.0300541 P@5: 0.07257 N@5: 0.11358 early stop: 0\n",
      "3 2000 train loss: 0.0304569 valid loss: 0.0299426 P@5: 0.07257 N@5: 0.10629 early stop: 0\n",
      "3 2800 train loss: 0.0298941 valid loss: 0.0298991 P@5: 0.08171 N@5: 0.11864 early stop: 0\n",
      "4 800 train loss: 0.0296912 valid loss: 0.0299918 P@5: 0.07543 N@5: 0.11595 early stop: 0\n",
      "4 1600 train loss: 0.0302957 valid loss: 0.0299386 P@5: 0.07629 N@5: 0.11883 early stop: 0\n",
      "4 2400 train loss: 0.0295792 valid loss: 0.0299281 P@5: 0.07343 N@5: 0.11161 early stop: 0\n",
      "5 400 train loss: 0.0297987 valid loss: 0.0299718 P@5: 0.08029 N@5: 0.11325 early stop: 0\n",
      "5 1200 train loss: 0.0299847 valid loss: 0.0299259 P@5: 0.07686 N@5: 0.11675 early stop: 0\n",
      "5 2000 train loss: 0.0296805 valid loss: 0.0300367 P@5: 0.07286 N@5: 0.10243 early stop: 0\n",
      "5 2800 train loss: 0.0300828 valid loss: 0.0299130 P@5: 0.07857 N@5: 0.11884 early stop: 0\n",
      "6 800 train loss: 0.0296141 valid loss: 0.0300140 P@5: 0.07343 N@5: 0.11385 early stop: 0\n",
      "6 1600 train loss: 0.0296996 valid loss: 0.0300382 P@5: 0.07400 N@5: 0.11284 early stop: 0\n",
      "6 2400 train loss: 0.0301664 valid loss: 0.0298737 P@5: 0.08343 N@5: 0.12893 early stop: 0\n",
      "7 400 train loss: 0.0294912 valid loss: 0.0299594 P@5: 0.09743 N@5: 0.14669 early stop: 0\n",
      "7 1200 train loss: 0.0310179 valid loss: 0.0299475 P@5: 0.07543 N@5: 0.12052 early stop: 0\n",
      "7 2000 train loss: 0.0295797 valid loss: 0.0298683 P@5: 0.07914 N@5: 0.11776 early stop: 0\n",
      "7 2800 train loss: 0.0290317 valid loss: 0.0297597 P@5: 0.07914 N@5: 0.13021 early stop: 0\n",
      "8 800 train loss: 0.0287873 valid loss: 0.0298349 P@5: 0.09000 N@5: 0.14370 early stop: 0\n",
      "8 1600 train loss: 0.0302608 valid loss: 0.0298872 P@5: 0.07971 N@5: 0.12149 early stop: 0\n",
      "8 2400 train loss: 0.0297611 valid loss: 0.0298208 P@5: 0.08629 N@5: 0.12696 early stop: 0\n",
      "9 400 train loss: 0.0296631 valid loss: 0.0298378 P@5: 0.10314 N@5: 0.15050 early stop: 0\n",
      "9 1200 train loss: 0.0293909 valid loss: 0.0298208 P@5: 0.08686 N@5: 0.14634 early stop: 0\n",
      "9 2000 train loss: 0.0295040 valid loss: 0.0298474 P@5: 0.09286 N@5: 0.13338 early stop: 0\n",
      "9 2800 train loss: 0.0301209 valid loss: 0.0296925 P@5: 0.09229 N@5: 0.15172 early stop: 0\n",
      "10 800 train loss: 0.0289894 valid loss: 0.0296243 P@5: 0.10171 N@5: 0.15143 early stop: 0\n",
      "10 1600 train loss: 0.0300308 valid loss: 0.0296798 P@5: 0.09286 N@5: 0.15578 early stop: 0\n",
      "10 2400 train loss: 0.0293789 valid loss: 0.0295125 P@5: 0.09057 N@5: 0.13910 early stop: 0\n",
      "11 400 train loss: 0.0289002 valid loss: 0.0295395 P@5: 0.09629 N@5: 0.15473 early stop: 0\n",
      "11 1200 train loss: 0.0294818 valid loss: 0.0295067 P@5: 0.09657 N@5: 0.15539 early stop: 0\n",
      "11 2000 train loss: 0.0295202 valid loss: 0.0292636 P@5: 0.10229 N@5: 0.15664 early stop: 0\n",
      "11 2800 train loss: 0.0290091 valid loss: 0.0292246 P@5: 0.09086 N@5: 0.14376 early stop: 0\n",
      "12 800 train loss: 0.0291803 valid loss: 0.0293449 P@5: 0.10286 N@5: 0.16863 early stop: 0\n",
      "12 1600 train loss: 0.0286987 valid loss: 0.0292420 P@5: 0.09400 N@5: 0.15340 early stop: 0\n",
      "12 2400 train loss: 0.0289442 valid loss: 0.0290009 P@5: 0.11429 N@5: 0.17206 early stop: 0\n",
      "13 400 train loss: 0.0284352 valid loss: 0.0290917 P@5: 0.09943 N@5: 0.15946 early stop: 0\n",
      "13 1200 train loss: 0.0289883 valid loss: 0.0289270 P@5: 0.11743 N@5: 0.17908 early stop: 0\n",
      "13 2000 train loss: 0.0284594 valid loss: 0.0289609 P@5: 0.10971 N@5: 0.17958 early stop: 0\n",
      "13 2800 train loss: 0.0294133 valid loss: 0.0288338 P@5: 0.11371 N@5: 0.17798 early stop: 0\n",
      "14 800 train loss: 0.0286825 valid loss: 0.0288834 P@5: 0.11657 N@5: 0.17852 early stop: 0\n",
      "14 1600 train loss: 0.0280803 valid loss: 0.0286628 P@5: 0.11629 N@5: 0.18738 early stop: 0\n",
      "14 2400 train loss: 0.0286967 valid loss: 0.0286225 P@5: 0.13143 N@5: 0.20646 early stop: 0\n",
      "15 400 train loss: 0.0279697 valid loss: 0.0287007 P@5: 0.11600 N@5: 0.19349 early stop: 0\n",
      "15 1200 train loss: 0.0278744 valid loss: 0.0285460 P@5: 0.12457 N@5: 0.18957 early stop: 0\n",
      "15 2000 train loss: 0.0288528 valid loss: 0.0286004 P@5: 0.12143 N@5: 0.18896 early stop: 0\n",
      "15 2800 train loss: 0.0278960 valid loss: 0.0284435 P@5: 0.12143 N@5: 0.19622 early stop: 0\n",
      "16 800 train loss: 0.0275324 valid loss: 0.0285307 P@5: 0.12314 N@5: 0.20306 early stop: 0\n",
      "16 1600 train loss: 0.0281213 valid loss: 0.0284758 P@5: 0.12314 N@5: 0.20154 early stop: 0\n",
      "16 2400 train loss: 0.0278201 valid loss: 0.0282090 P@5: 0.13400 N@5: 0.21994 early stop: 0\n",
      "17 400 train loss: 0.0275465 valid loss: 0.0280951 P@5: 0.13686 N@5: 0.22035 early stop: 0\n",
      "17 1200 train loss: 0.0276797 valid loss: 0.0280877 P@5: 0.13771 N@5: 0.21600 early stop: 0\n",
      "17 2000 train loss: 0.0283214 valid loss: 0.0280486 P@5: 0.13971 N@5: 0.22045 early stop: 0\n",
      "17 2800 train loss: 0.0271223 valid loss: 0.0279862 P@5: 0.13886 N@5: 0.22378 early stop: 0\n",
      "18 800 train loss: 0.0265460 valid loss: 0.0279161 P@5: 0.13657 N@5: 0.21314 early stop: 0\n",
      "18 1600 train loss: 0.0276788 valid loss: 0.0279092 P@5: 0.13086 N@5: 0.21352 early stop: 0\n",
      "18 2400 train loss: 0.0278521 valid loss: 0.0278204 P@5: 0.14657 N@5: 0.22886 early stop: 0\n",
      "19 400 train loss: 0.0268891 valid loss: 0.0276357 P@5: 0.14657 N@5: 0.23594 early stop: 0\n",
      "19 1200 train loss: 0.0274507 valid loss: 0.0278164 P@5: 0.14343 N@5: 0.23118 early stop: 0\n",
      "19 2000 train loss: 0.0268322 valid loss: 0.0276727 P@5: 0.13800 N@5: 0.22704 early stop: 0\n",
      "19 2800 train loss: 0.0269461 valid loss: 0.0278282 P@5: 0.13400 N@5: 0.21197 early stop: 0\n",
      "20 800 train loss: 0.0265597 valid loss: 0.0276670 P@5: 0.14457 N@5: 0.23128 early stop: 0\n",
      "20 1600 train loss: 0.0272596 valid loss: 0.0275775 P@5: 0.14629 N@5: 0.23602 early stop: 0\n",
      "20 2400 train loss: 0.0272692 valid loss: 0.0274321 P@5: 0.14771 N@5: 0.24079 early stop: 0\n",
      "21 400 train loss: 0.0256495 valid loss: 0.0275374 P@5: 0.15943 N@5: 0.25644 early stop: 0\n",
      "21 1200 train loss: 0.0263446 valid loss: 0.0274427 P@5: 0.14543 N@5: 0.23955 early stop: 0\n",
      "21 2000 train loss: 0.0270946 valid loss: 0.0273807 P@5: 0.15286 N@5: 0.24218 early stop: 0\n",
      "21 2800 train loss: 0.0265150 valid loss: 0.0273583 P@5: 0.15143 N@5: 0.23652 early stop: 0\n",
      "22 800 train loss: 0.0257123 valid loss: 0.0273341 P@5: 0.15314 N@5: 0.24965 early stop: 0\n",
      "22 1600 train loss: 0.0263883 valid loss: 0.0272382 P@5: 0.14743 N@5: 0.23976 early stop: 0\n",
      "22 2400 train loss: 0.0264420 valid loss: 0.0272001 P@5: 0.15800 N@5: 0.24932 early stop: 0\n",
      "23 400 train loss: 0.0263530 valid loss: 0.0270413 P@5: 0.15486 N@5: 0.25290 early stop: 0\n",
      "23 1200 train loss: 0.0256214 valid loss: 0.0271252 P@5: 0.15486 N@5: 0.24691 early stop: 0\n",
      "23 2000 train loss: 0.0258335 valid loss: 0.0272028 P@5: 0.15543 N@5: 0.24712 early stop: 0\n",
      "23 2800 train loss: 0.0269414 valid loss: 0.0271473 P@5: 0.15886 N@5: 0.24904 early stop: 0\n",
      "24 800 train loss: 0.0257533 valid loss: 0.0270005 P@5: 0.16171 N@5: 0.26016 early stop: 0\n",
      "24 1600 train loss: 0.0260132 valid loss: 0.0270573 P@5: 0.15514 N@5: 0.25014 early stop: 0\n",
      "24 2400 train loss: 0.0259509 valid loss: 0.0269706 P@5: 0.16457 N@5: 0.26389 early stop: 0\n",
      "25 400 train loss: 0.0254925 valid loss: 0.0272706 P@5: 0.15257 N@5: 0.24309 early stop: 0\n",
      "25 1200 train loss: 0.0255783 valid loss: 0.0269930 P@5: 0.16114 N@5: 0.25293 early stop: 0\n",
      "25 2000 train loss: 0.0258636 valid loss: 0.0267558 P@5: 0.16057 N@5: 0.26198 early stop: 0\n",
      "25 2800 train loss: 0.0253777 valid loss: 0.0268049 P@5: 0.16771 N@5: 0.27398 early stop: 0\n",
      "26 800 train loss: 0.0257462 valid loss: 0.0269234 P@5: 0.16000 N@5: 0.25898 early stop: 0\n",
      "26 1600 train loss: 0.0253038 valid loss: 0.0266608 P@5: 0.16400 N@5: 0.26206 early stop: 0\n",
      "26 2400 train loss: 0.0249450 valid loss: 0.0267810 P@5: 0.16171 N@5: 0.26076 early stop: 0\n",
      "27 400 train loss: 0.0255442 valid loss: 0.0266668 P@5: 0.15971 N@5: 0.25890 early stop: 0\n",
      "27 1200 train loss: 0.0249492 valid loss: 0.0267387 P@5: 0.16143 N@5: 0.26501 early stop: 0\n",
      "27 2000 train loss: 0.0250957 valid loss: 0.0265149 P@5: 0.17229 N@5: 0.27847 early stop: 0\n",
      "27 2800 train loss: 0.0253356 valid loss: 0.0266439 P@5: 0.16714 N@5: 0.27538 early stop: 0\n",
      "28 800 train loss: 0.0253187 valid loss: 0.0267543 P@5: 0.16343 N@5: 0.25354 early stop: 0\n",
      "28 1600 train loss: 0.0250350 valid loss: 0.0266698 P@5: 0.16171 N@5: 0.26002 early stop: 0\n",
      "28 2400 train loss: 0.0251074 valid loss: 0.0263018 P@5: 0.17314 N@5: 0.27520 early stop: 0\n",
      "29 400 train loss: 0.0242738 valid loss: 0.0263844 P@5: 0.16714 N@5: 0.27181 early stop: 0\n",
      "29 1200 train loss: 0.0252430 valid loss: 0.0264648 P@5: 0.16429 N@5: 0.27175 early stop: 0\n",
      "29 2000 train loss: 0.0242634 valid loss: 0.0264918 P@5: 0.17000 N@5: 0.27459 early stop: 0\n",
      "29 2800 train loss: 0.0248106 valid loss: 0.0264730 P@5: 0.17371 N@5: 0.27833 early stop: 0\n",
      "30 800 train loss: 0.0241985 valid loss: 0.0263990 P@5: 0.17543 N@5: 0.28795 early stop: 0\n",
      "30 1600 train loss: 0.0242717 valid loss: 0.0265308 P@5: 0.16200 N@5: 0.26432 early stop: 0\n",
      "30 2400 train loss: 0.0248049 valid loss: 0.0263098 P@5: 0.17629 N@5: 0.27213 early stop: 0\n",
      "31 400 train loss: 0.0245943 valid loss: 0.0263696 P@5: 0.16971 N@5: 0.27424 early stop: 0\n",
      "31 1200 train loss: 0.0246804 valid loss: 0.0262774 P@5: 0.17286 N@5: 0.27982 early stop: 0\n",
      "31 2000 train loss: 0.0241191 valid loss: 0.0261602 P@5: 0.17857 N@5: 0.28595 early stop: 0\n",
      "31 2800 train loss: 0.0246794 valid loss: 0.0261925 P@5: 0.17657 N@5: 0.29026 early stop: 0\n",
      "32 800 train loss: 0.0237622 valid loss: 0.0263848 P@5: 0.16657 N@5: 0.27409 early stop: 0\n",
      "32 1600 train loss: 0.0245154 valid loss: 0.0261452 P@5: 0.17514 N@5: 0.28316 early stop: 0\n",
      "32 2400 train loss: 0.0241206 valid loss: 0.0261278 P@5: 0.17771 N@5: 0.28712 early stop: 0\n",
      "33 400 train loss: 0.0236044 valid loss: 0.0262097 P@5: 0.17571 N@5: 0.28768 early stop: 0\n",
      "33 1200 train loss: 0.0241879 valid loss: 0.0260774 P@5: 0.18086 N@5: 0.29533 early stop: 0\n",
      "33 2000 train loss: 0.0243998 valid loss: 0.0259360 P@5: 0.18371 N@5: 0.29149 early stop: 0\n",
      "33 2800 train loss: 0.0241420 valid loss: 0.0260508 P@5: 0.17943 N@5: 0.29389 early stop: 0\n",
      "34 800 train loss: 0.0233500 valid loss: 0.0260815 P@5: 0.17829 N@5: 0.28864 early stop: 0\n",
      "34 1600 train loss: 0.0240461 valid loss: 0.0260040 P@5: 0.18257 N@5: 0.28745 early stop: 0\n",
      "34 2400 train loss: 0.0237442 valid loss: 0.0260572 P@5: 0.18171 N@5: 0.29144 early stop: 0\n",
      "35 400 train loss: 0.0238484 valid loss: 0.0261693 P@5: 0.17914 N@5: 0.29446 early stop: 0\n",
      "35 1200 train loss: 0.0239352 valid loss: 0.0260855 P@5: 0.18057 N@5: 0.29160 early stop: 0\n",
      "35 2000 train loss: 0.0234893 valid loss: 0.0258876 P@5: 0.18143 N@5: 0.29545 early stop: 0\n",
      "35 2800 train loss: 0.0237518 valid loss: 0.0259876 P@5: 0.17514 N@5: 0.28408 early stop: 0\n",
      "36 800 train loss: 0.0233664 valid loss: 0.0259430 P@5: 0.18086 N@5: 0.29747 early stop: 0\n",
      "36 1600 train loss: 0.0232045 valid loss: 0.0259164 P@5: 0.18200 N@5: 0.29264 early stop: 0\n",
      "36 2400 train loss: 0.0239469 valid loss: 0.0258555 P@5: 0.18371 N@5: 0.29725 early stop: 0\n",
      "37 400 train loss: 0.0230711 valid loss: 0.0258763 P@5: 0.18171 N@5: 0.29740 early stop: 0\n",
      "37 1200 train loss: 0.0232711 valid loss: 0.0258105 P@5: 0.18657 N@5: 0.29898 early stop: 0\n",
      "37 2000 train loss: 0.0228450 valid loss: 0.0257497 P@5: 0.18257 N@5: 0.29388 early stop: 0\n",
      "37 2800 train loss: 0.0241091 valid loss: 0.0258442 P@5: 0.17971 N@5: 0.29340 early stop: 0\n",
      "38 800 train loss: 0.0229655 valid loss: 0.0258677 P@5: 0.17486 N@5: 0.29021 early stop: 0\n",
      "38 1600 train loss: 0.0228208 valid loss: 0.0260424 P@5: 0.17286 N@5: 0.27708 early stop: 0\n",
      "38 2400 train loss: 0.0235923 valid loss: 0.0258038 P@5: 0.18429 N@5: 0.30615 early stop: 0\n",
      "39 400 train loss: 0.0231426 valid loss: 0.0257439 P@5: 0.18543 N@5: 0.29656 early stop: 0\n",
      "39 1200 train loss: 0.0226235 valid loss: 0.0257106 P@5: 0.19457 N@5: 0.31283 early stop: 0\n",
      "39 2000 train loss: 0.0230668 valid loss: 0.0257531 P@5: 0.18629 N@5: 0.29838 early stop: 0\n",
      "39 2800 train loss: 0.0231931 valid loss: 0.0256647 P@5: 0.19400 N@5: 0.31348 early stop: 0\n",
      "40 800 train loss: 0.0225458 valid loss: 0.0255567 P@5: 0.19229 N@5: 0.31054 early stop: 0\n",
      "40 1600 train loss: 0.0224942 valid loss: 0.0257891 P@5: 0.18286 N@5: 0.29830 early stop: 0\n",
      "40 2400 train loss: 0.0234733 valid loss: 0.0257410 P@5: 0.18429 N@5: 0.29378 early stop: 0\n",
      "41 400 train loss: 0.0229854 valid loss: 0.0257961 P@5: 0.18543 N@5: 0.29581 early stop: 0\n",
      "41 1200 train loss: 0.0226675 valid loss: 0.0257988 P@5: 0.18914 N@5: 0.31283 early stop: 0\n",
      "41 2000 train loss: 0.0225248 valid loss: 0.0255493 P@5: 0.18971 N@5: 0.31225 early stop: 0\n",
      "41 2800 train loss: 0.0224094 valid loss: 0.0254860 P@5: 0.19171 N@5: 0.30934 early stop: 0\n",
      "42 800 train loss: 0.0231640 valid loss: 0.0256187 P@5: 0.19114 N@5: 0.31285 early stop: 0\n",
      "42 1600 train loss: 0.0220305 valid loss: 0.0256063 P@5: 0.19314 N@5: 0.30997 early stop: 0\n",
      "42 2400 train loss: 0.0224102 valid loss: 0.0256533 P@5: 0.18857 N@5: 0.30394 early stop: 0\n",
      "43 400 train loss: 0.0219905 valid loss: 0.0254745 P@5: 0.19229 N@5: 0.30739 early stop: 0\n",
      "43 1200 train loss: 0.0225879 valid loss: 0.0255290 P@5: 0.18971 N@5: 0.30450 early stop: 0\n",
      "43 2000 train loss: 0.0219434 valid loss: 0.0256565 P@5: 0.18486 N@5: 0.29888 early stop: 0\n",
      "43 2800 train loss: 0.0229748 valid loss: 0.0256323 P@5: 0.18429 N@5: 0.30403 early stop: 0\n",
      "44 800 train loss: 0.0219881 valid loss: 0.0256854 P@5: 0.19571 N@5: 0.31462 early stop: 0\n",
      "44 1600 train loss: 0.0223215 valid loss: 0.0255303 P@5: 0.19229 N@5: 0.31488 early stop: 0\n",
      "44 2400 train loss: 0.0226520 valid loss: 0.0253398 P@5: 0.18629 N@5: 0.30251 early stop: 0\n",
      "45 400 train loss: 0.0216959 valid loss: 0.0254719 P@5: 0.19600 N@5: 0.31985 early stop: 0\n",
      "45 1200 train loss: 0.0219296 valid loss: 0.0255366 P@5: 0.18600 N@5: 0.30180 early stop: 0\n",
      "45 2000 train loss: 0.0220890 valid loss: 0.0254297 P@5: 0.19714 N@5: 0.32024 early stop: 0\n",
      "45 2800 train loss: 0.0224528 valid loss: 0.0253829 P@5: 0.19657 N@5: 0.31718 early stop: 0\n",
      "46 800 train loss: 0.0221732 valid loss: 0.0255079 P@5: 0.19257 N@5: 0.31728 early stop: 0\n",
      "46 1600 train loss: 0.0217359 valid loss: 0.0255290 P@5: 0.19286 N@5: 0.31529 early stop: 0\n",
      "46 2400 train loss: 0.0219420 valid loss: 0.0253125 P@5: 0.19286 N@5: 0.31568 early stop: 0\n",
      "47 400 train loss: 0.0218539 valid loss: 0.0252870 P@5: 0.19343 N@5: 0.31541 early stop: 0\n",
      "47 1200 train loss: 0.0213560 valid loss: 0.0254113 P@5: 0.19571 N@5: 0.31771 early stop: 0\n",
      "47 2000 train loss: 0.0218656 valid loss: 0.0254203 P@5: 0.18914 N@5: 0.30981 early stop: 0\n",
      "47 2800 train loss: 0.0222925 valid loss: 0.0254668 P@5: 0.19514 N@5: 0.31247 early stop: 0\n",
      "48 800 train loss: 0.0217494 valid loss: 0.0253380 P@5: 0.19057 N@5: 0.31195 early stop: 0\n",
      "48 1600 train loss: 0.0221115 valid loss: 0.0251673 P@5: 0.19714 N@5: 0.31946 early stop: 0\n",
      "48 2400 train loss: 0.0208803 valid loss: 0.0253981 P@5: 0.19200 N@5: 0.31366 early stop: 0\n",
      "49 400 train loss: 0.0219544 valid loss: 0.0253581 P@5: 0.19343 N@5: 0.30983 early stop: 0\n",
      "49 1200 train loss: 0.0213249 valid loss: 0.0253596 P@5: 0.19714 N@5: 0.31647 early stop: 0\n",
      "49 2000 train loss: 0.0214347 valid loss: 0.0253720 P@5: 0.19800 N@5: 0.32053 early stop: 0\n",
      "49 2800 train loss: 0.0218564 valid loss: 0.0253689 P@5: 0.19829 N@5: 0.31574 early stop: 0\n",
      "50 800 train loss: 0.0211172 valid loss: 0.0253114 P@5: 0.20000 N@5: 0.32740 early stop: 0\n",
      "50 1600 train loss: 0.0218075 valid loss: 0.0252497 P@5: 0.19486 N@5: 0.31633 early stop: 0\n",
      "50 2400 train loss: 0.0213941 valid loss: 0.0253604 P@5: 0.19543 N@5: 0.31321 early stop: 0\n",
      "51 400 train loss: 0.0210468 valid loss: 0.0252966 P@5: 0.20029 N@5: 0.32034 early stop: 0\n",
      "51 1200 train loss: 0.0211592 valid loss: 0.0250956 P@5: 0.19743 N@5: 0.32621 early stop: 0\n",
      "51 2000 train loss: 0.0216638 valid loss: 0.0253284 P@5: 0.19657 N@5: 0.32369 early stop: 0\n",
      "51 2800 train loss: 0.0213164 valid loss: 0.0252788 P@5: 0.18914 N@5: 0.31218 early stop: 0\n",
      "52 800 train loss: 0.0212089 valid loss: 0.0252244 P@5: 0.19743 N@5: 0.32089 early stop: 0\n",
      "52 1600 train loss: 0.0214380 valid loss: 0.0253030 P@5: 0.20229 N@5: 0.32259 early stop: 0\n",
      "52 2400 train loss: 0.0211694 valid loss: 0.0252697 P@5: 0.19543 N@5: 0.31921 early stop: 0\n",
      "53 400 train loss: 0.0207858 valid loss: 0.0252402 P@5: 0.19771 N@5: 0.32496 early stop: 0\n",
      "53 1200 train loss: 0.0213288 valid loss: 0.0252171 P@5: 0.19629 N@5: 0.31492 early stop: 0\n",
      "53 2000 train loss: 0.0207877 valid loss: 0.0251533 P@5: 0.19457 N@5: 0.31483 early stop: 0\n",
      "53 2800 train loss: 0.0212267 valid loss: 0.0252325 P@5: 0.19714 N@5: 0.31757 early stop: 0\n",
      "54 800 train loss: 0.0206456 valid loss: 0.0252542 P@5: 0.19486 N@5: 0.31186 early stop: 0\n",
      "54 1600 train loss: 0.0211956 valid loss: 0.0251914 P@5: 0.19943 N@5: 0.32126 early stop: 0\n",
      "54 2400 train loss: 0.0207437 valid loss: 0.0251687 P@5: 0.19971 N@5: 0.32446 early stop: 0\n",
      "55 400 train loss: 0.0210737 valid loss: 0.0252450 P@5: 0.19514 N@5: 0.32589 early stop: 0\n",
      "55 1200 train loss: 0.0205389 valid loss: 0.0250857 P@5: 0.19886 N@5: 0.32410 early stop: 0\n",
      "55 2000 train loss: 0.0212684 valid loss: 0.0251590 P@5: 0.19714 N@5: 0.32043 early stop: 0\n",
      "55 2800 train loss: 0.0206294 valid loss: 0.0251039 P@5: 0.19343 N@5: 0.31541 early stop: 0\n",
      "56 800 train loss: 0.0206446 valid loss: 0.0251930 P@5: 0.19943 N@5: 0.33178 early stop: 0\n",
      "56 1600 train loss: 0.0201350 valid loss: 0.0251516 P@5: 0.20000 N@5: 0.32583 early stop: 0\n",
      "56 2400 train loss: 0.0212786 valid loss: 0.0249924 P@5: 0.19886 N@5: 0.32314 early stop: 0\n",
      "57 400 train loss: 0.0206930 valid loss: 0.0251048 P@5: 0.19800 N@5: 0.32753 early stop: 0\n",
      "57 1200 train loss: 0.0210710 valid loss: 0.0251216 P@5: 0.19857 N@5: 0.32212 early stop: 0\n",
      "57 2000 train loss: 0.0203063 valid loss: 0.0251098 P@5: 0.19543 N@5: 0.31652 early stop: 0\n",
      "57 2800 train loss: 0.0203356 valid loss: 0.0250770 P@5: 0.20743 N@5: 0.33688 early stop: 0\n",
      "58 800 train loss: 0.0205100 valid loss: 0.0252927 P@5: 0.19600 N@5: 0.31599 early stop: 0\n",
      "58 1600 train loss: 0.0209476 valid loss: 0.0250411 P@5: 0.20057 N@5: 0.32553 early stop: 0\n",
      "58 2400 train loss: 0.0201797 valid loss: 0.0249718 P@5: 0.20286 N@5: 0.33021 early stop: 0\n",
      "59 400 train loss: 0.0204207 valid loss: 0.0251402 P@5: 0.19486 N@5: 0.32327 early stop: 0\n",
      "59 1200 train loss: 0.0205819 valid loss: 0.0250741 P@5: 0.20429 N@5: 0.32792 early stop: 0\n",
      "59 2000 train loss: 0.0201697 valid loss: 0.0249937 P@5: 0.20686 N@5: 0.33794 early stop: 0\n",
      "59 2800 train loss: 0.0202121 valid loss: 0.0250754 P@5: 0.20657 N@5: 0.34425 early stop: 0\n",
      "60 800 train loss: 0.0202491 valid loss: 0.0250558 P@5: 0.20829 N@5: 0.33677 early stop: 0\n",
      "60 1600 train loss: 0.0195813 valid loss: 0.0249084 P@5: 0.20314 N@5: 0.33454 early stop: 0\n",
      "60 2400 train loss: 0.0205367 valid loss: 0.0250421 P@5: 0.20200 N@5: 0.33098 early stop: 0\n",
      "61 400 train loss: 0.0204496 valid loss: 0.0250117 P@5: 0.20886 N@5: 0.33869 early stop: 0\n",
      "61 1200 train loss: 0.0204347 valid loss: 0.0249138 P@5: 0.20343 N@5: 0.32389 early stop: 0\n",
      "61 2000 train loss: 0.0207636 valid loss: 0.0250126 P@5: 0.20543 N@5: 0.33859 early stop: 0\n",
      "61 2800 train loss: 0.0196043 valid loss: 0.0250302 P@5: 0.20629 N@5: 0.33568 early stop: 0\n",
      "62 800 train loss: 0.0199084 valid loss: 0.0250132 P@5: 0.20086 N@5: 0.32578 early stop: 0\n",
      "62 1600 train loss: 0.0195613 valid loss: 0.0250711 P@5: 0.20143 N@5: 0.32424 early stop: 0\n",
      "62 2400 train loss: 0.0207149 valid loss: 0.0249926 P@5: 0.20543 N@5: 0.33604 early stop: 0\n",
      "63 400 train loss: 0.0200338 valid loss: 0.0250682 P@5: 0.20314 N@5: 0.32727 early stop: 0\n",
      "63 1200 train loss: 0.0199448 valid loss: 0.0250111 P@5: 0.20486 N@5: 0.33254 early stop: 0\n",
      "63 2000 train loss: 0.0202076 valid loss: 0.0248404 P@5: 0.20886 N@5: 0.33822 early stop: 0\n",
      "63 2800 train loss: 0.0197497 valid loss: 0.0249454 P@5: 0.20629 N@5: 0.33421 early stop: 0\n",
      "64 800 train loss: 0.0197217 valid loss: 0.0249425 P@5: 0.20514 N@5: 0.33463 early stop: 0\n",
      "64 1600 train loss: 0.0196356 valid loss: 0.0250522 P@5: 0.20600 N@5: 0.33668 early stop: 0\n",
      "64 2400 train loss: 0.0207116 valid loss: 0.0249555 P@5: 0.20886 N@5: 0.34215 early stop: 0\n",
      "65 400 train loss: 0.0194984 valid loss: 0.0249584 P@5: 0.19971 N@5: 0.32643 early stop: 0\n",
      "65 1200 train loss: 0.0201317 valid loss: 0.0248298 P@5: 0.21200 N@5: 0.34058 early stop: 0\n",
      "65 2000 train loss: 0.0195012 valid loss: 0.0250228 P@5: 0.20800 N@5: 0.33839 early stop: 0\n",
      "65 2800 train loss: 0.0195900 valid loss: 0.0249965 P@5: 0.19743 N@5: 0.31885 early stop: 0\n",
      "66 800 train loss: 0.0195242 valid loss: 0.0250141 P@5: 0.20943 N@5: 0.33681 early stop: 0\n",
      "66 1600 train loss: 0.0200315 valid loss: 0.0249435 P@5: 0.20829 N@5: 0.34340 early stop: 0\n",
      "66 2400 train loss: 0.0197563 valid loss: 0.0248036 P@5: 0.21257 N@5: 0.34692 early stop: 0\n",
      "67 400 train loss: 0.0197634 valid loss: 0.0247529 P@5: 0.21514 N@5: 0.35009 early stop: 0\n",
      "67 1200 train loss: 0.0191299 valid loss: 0.0248837 P@5: 0.20200 N@5: 0.32835 early stop: 0\n",
      "67 2000 train loss: 0.0191300 valid loss: 0.0249182 P@5: 0.20600 N@5: 0.33893 early stop: 0\n",
      "67 2800 train loss: 0.0202398 valid loss: 0.0248644 P@5: 0.20971 N@5: 0.33835 early stop: 0\n",
      "68 800 train loss: 0.0191351 valid loss: 0.0248808 P@5: 0.21143 N@5: 0.34759 early stop: 0\n",
      "68 1600 train loss: 0.0196115 valid loss: 0.0248651 P@5: 0.20686 N@5: 0.34384 early stop: 0\n",
      "68 2400 train loss: 0.0199455 valid loss: 0.0248619 P@5: 0.20829 N@5: 0.33469 early stop: 0\n",
      "69 400 train loss: 0.0196485 valid loss: 0.0249870 P@5: 0.20143 N@5: 0.32931 early stop: 0\n",
      "69 1200 train loss: 0.0189468 valid loss: 0.0249312 P@5: 0.20657 N@5: 0.33226 early stop: 0\n",
      "69 2000 train loss: 0.0195573 valid loss: 0.0248576 P@5: 0.20343 N@5: 0.33824 early stop: 0\n",
      "69 2800 train loss: 0.0192486 valid loss: 0.0247862 P@5: 0.21029 N@5: 0.34059 early stop: 0\n",
      "70 800 train loss: 0.0189273 valid loss: 0.0248642 P@5: 0.21543 N@5: 0.35150 early stop: 0\n",
      "70 1600 train loss: 0.0193001 valid loss: 0.0247969 P@5: 0.21057 N@5: 0.34031 early stop: 0\n",
      "70 2400 train loss: 0.0191555 valid loss: 0.0247954 P@5: 0.20029 N@5: 0.33433 early stop: 0\n",
      "71 400 train loss: 0.0194630 valid loss: 0.0246521 P@5: 0.21714 N@5: 0.34878 early stop: 0\n",
      "71 1200 train loss: 0.0190863 valid loss: 0.0248802 P@5: 0.20914 N@5: 0.33991 early stop: 0\n",
      "71 2000 train loss: 0.0190078 valid loss: 0.0247560 P@5: 0.21086 N@5: 0.34185 early stop: 0\n",
      "71 2800 train loss: 0.0198372 valid loss: 0.0248048 P@5: 0.20743 N@5: 0.33771 early stop: 0\n",
      "72 800 train loss: 0.0192170 valid loss: 0.0248201 P@5: 0.20971 N@5: 0.33980 early stop: 0\n",
      "72 1600 train loss: 0.0192842 valid loss: 0.0247281 P@5: 0.21314 N@5: 0.34788 early stop: 0\n",
      "72 2400 train loss: 0.0192412 valid loss: 0.0247740 P@5: 0.21571 N@5: 0.34597 early stop: 0\n",
      "73 400 train loss: 0.0186814 valid loss: 0.0247758 P@5: 0.21257 N@5: 0.35010 early stop: 0\n",
      "73 1200 train loss: 0.0183810 valid loss: 0.0247268 P@5: 0.20857 N@5: 0.34012 early stop: 0\n",
      "73 2000 train loss: 0.0194692 valid loss: 0.0247742 P@5: 0.20657 N@5: 0.33329 early stop: 0\n",
      "73 2800 train loss: 0.0192552 valid loss: 0.0247154 P@5: 0.21371 N@5: 0.34698 early stop: 0\n",
      "74 800 train loss: 0.0182865 valid loss: 0.0248215 P@5: 0.21457 N@5: 0.34440 early stop: 0\n",
      "74 1600 train loss: 0.0191475 valid loss: 0.0247339 P@5: 0.21686 N@5: 0.34682 early stop: 0\n",
      "74 2400 train loss: 0.0192821 valid loss: 0.0246539 P@5: 0.21457 N@5: 0.34968 early stop: 0\n",
      "75 400 train loss: 0.0188049 valid loss: 0.0248273 P@5: 0.21286 N@5: 0.34834 early stop: 0\n",
      "75 1200 train loss: 0.0184496 valid loss: 0.0248690 P@5: 0.20686 N@5: 0.34047 early stop: 0\n",
      "75 2000 train loss: 0.0194563 valid loss: 0.0248383 P@5: 0.20829 N@5: 0.34454 early stop: 0\n",
      "75 2800 train loss: 0.0190264 valid loss: 0.0247295 P@5: 0.21457 N@5: 0.34883 early stop: 0\n",
      "76 800 train loss: 0.0185978 valid loss: 0.0247147 P@5: 0.21000 N@5: 0.34536 early stop: 0\n",
      "76 1600 train loss: 0.0186656 valid loss: 0.0246738 P@5: 0.21657 N@5: 0.35403 early stop: 0\n",
      "76 2400 train loss: 0.0194923 valid loss: 0.0247179 P@5: 0.21057 N@5: 0.34200 early stop: 0\n",
      "77 400 train loss: 0.0183330 valid loss: 0.0246866 P@5: 0.21200 N@5: 0.34570 early stop: 0\n",
      "77 1200 train loss: 0.0189546 valid loss: 0.0246900 P@5: 0.21000 N@5: 0.34100 early stop: 0\n",
      "77 2000 train loss: 0.0183248 valid loss: 0.0246748 P@5: 0.21114 N@5: 0.34168 early stop: 0\n",
      "77 2800 train loss: 0.0186701 valid loss: 0.0247535 P@5: 0.21000 N@5: 0.34419 early stop: 0\n",
      "78 800 train loss: 0.0184048 valid loss: 0.0247174 P@5: 0.21086 N@5: 0.34345 early stop: 0\n",
      "78 1600 train loss: 0.0187082 valid loss: 0.0247100 P@5: 0.20886 N@5: 0.33425 early stop: 0\n",
      "78 2400 train loss: 0.0181539 valid loss: 0.0247645 P@5: 0.21229 N@5: 0.34318 early stop: 0\n",
      "79 400 train loss: 0.0190332 valid loss: 0.0246625 P@5: 0.21429 N@5: 0.35450 early stop: 0\n",
      "79 1200 train loss: 0.0183048 valid loss: 0.0245563 P@5: 0.22000 N@5: 0.35884 early stop: 0\n",
      "79 2000 train loss: 0.0186001 valid loss: 0.0246168 P@5: 0.21314 N@5: 0.35084 early stop: 0\n",
      "79 2800 train loss: 0.0187315 valid loss: 0.0247184 P@5: 0.20714 N@5: 0.33555 early stop: 0\n",
      "80 800 train loss: 0.0183835 valid loss: 0.0246906 P@5: 0.21743 N@5: 0.35427 early stop: 0\n",
      "80 1600 train loss: 0.0184770 valid loss: 0.0246772 P@5: 0.21200 N@5: 0.35114 early stop: 0\n",
      "80 2400 train loss: 0.0181654 valid loss: 0.0246613 P@5: 0.21429 N@5: 0.34828 early stop: 0\n",
      "81 400 train loss: 0.0184190 valid loss: 0.0248352 P@5: 0.20457 N@5: 0.33531 early stop: 0\n",
      "81 1200 train loss: 0.0185161 valid loss: 0.0247341 P@5: 0.21200 N@5: 0.34759 early stop: 0\n",
      "81 2000 train loss: 0.0178858 valid loss: 0.0246446 P@5: 0.21600 N@5: 0.35266 early stop: 0\n",
      "81 2800 train loss: 0.0188965 valid loss: 0.0246306 P@5: 0.21714 N@5: 0.35417 early stop: 0\n",
      "82 800 train loss: 0.0181705 valid loss: 0.0247287 P@5: 0.21314 N@5: 0.35074 early stop: 0\n",
      "82 1600 train loss: 0.0183784 valid loss: 0.0246873 P@5: 0.21743 N@5: 0.34826 early stop: 0\n",
      "82 2400 train loss: 0.0184245 valid loss: 0.0246911 P@5: 0.21000 N@5: 0.34391 early stop: 0\n",
      "83 400 train loss: 0.0182702 valid loss: 0.0246771 P@5: 0.21371 N@5: 0.34899 early stop: 0\n",
      "83 1200 train loss: 0.0170155 valid loss: 0.0245974 P@5: 0.21743 N@5: 0.35956 early stop: 0\n",
      "83 2000 train loss: 0.0187766 valid loss: 0.0246461 P@5: 0.21286 N@5: 0.34097 early stop: 0\n",
      "83 2800 train loss: 0.0184369 valid loss: 0.0246508 P@5: 0.22086 N@5: 0.35825 early stop: 0\n",
      "84 800 train loss: 0.0178329 valid loss: 0.0245876 P@5: 0.22000 N@5: 0.35441 early stop: 0\n",
      "84 1600 train loss: 0.0180776 valid loss: 0.0245646 P@5: 0.21657 N@5: 0.34703 early stop: 0\n",
      "84 2400 train loss: 0.0185239 valid loss: 0.0245898 P@5: 0.21457 N@5: 0.35203 early stop: 0\n",
      "85 400 train loss: 0.0180027 valid loss: 0.0245592 P@5: 0.21543 N@5: 0.34903 early stop: 0\n",
      "85 1200 train loss: 0.0179998 valid loss: 0.0247126 P@5: 0.21486 N@5: 0.35093 early stop: 0\n",
      "85 2000 train loss: 0.0181107 valid loss: 0.0246836 P@5: 0.21314 N@5: 0.35366 early stop: 0\n",
      "85 2800 train loss: 0.0179716 valid loss: 0.0245239 P@5: 0.22029 N@5: 0.36012 early stop: 0\n",
      "86 800 train loss: 0.0171406 valid loss: 0.0246968 P@5: 0.21829 N@5: 0.35183 early stop: 0\n",
      "86 1600 train loss: 0.0180210 valid loss: 0.0246414 P@5: 0.22057 N@5: 0.35780 early stop: 0\n",
      "86 2400 train loss: 0.0183674 valid loss: 0.0245001 P@5: 0.21914 N@5: 0.36044 early stop: 0\n",
      "87 400 train loss: 0.0182018 valid loss: 0.0244508 P@5: 0.22000 N@5: 0.36329 early stop: 0\n",
      "87 1200 train loss: 0.0179900 valid loss: 0.0245261 P@5: 0.21743 N@5: 0.34991 early stop: 0\n",
      "87 2000 train loss: 0.0174076 valid loss: 0.0245625 P@5: 0.21657 N@5: 0.35634 early stop: 0\n",
      "87 2800 train loss: 0.0181614 valid loss: 0.0244400 P@5: 0.21971 N@5: 0.36053 early stop: 0\n",
      "88 800 train loss: 0.0172326 valid loss: 0.0245905 P@5: 0.22257 N@5: 0.36394 early stop: 0\n",
      "88 1600 train loss: 0.0177206 valid loss: 0.0245572 P@5: 0.22000 N@5: 0.35947 early stop: 0\n",
      "88 2400 train loss: 0.0181490 valid loss: 0.0246843 P@5: 0.21057 N@5: 0.35074 early stop: 0\n",
      "89 400 train loss: 0.0181085 valid loss: 0.0245209 P@5: 0.21914 N@5: 0.36204 early stop: 0\n",
      "89 1200 train loss: 0.0177400 valid loss: 0.0245889 P@5: 0.21600 N@5: 0.35421 early stop: 0\n",
      "89 2000 train loss: 0.0173447 valid loss: 0.0246252 P@5: 0.22114 N@5: 0.36033 early stop: 0\n",
      "89 2800 train loss: 0.0178735 valid loss: 0.0245103 P@5: 0.22571 N@5: 0.36783 early stop: 0\n",
      "90 800 train loss: 0.0170750 valid loss: 0.0247288 P@5: 0.21543 N@5: 0.35495 early stop: 0\n",
      "90 1600 train loss: 0.0181601 valid loss: 0.0244109 P@5: 0.22229 N@5: 0.36393 early stop: 0\n",
      "90 2400 train loss: 0.0174642 valid loss: 0.0245461 P@5: 0.22114 N@5: 0.36167 early stop: 0\n",
      "91 400 train loss: 0.0171268 valid loss: 0.0247817 P@5: 0.21057 N@5: 0.34557 early stop: 0\n",
      "91 1200 train loss: 0.0174606 valid loss: 0.0244957 P@5: 0.22400 N@5: 0.36333 early stop: 0\n",
      "91 2000 train loss: 0.0175211 valid loss: 0.0244490 P@5: 0.22257 N@5: 0.36431 early stop: 0\n",
      "91 2800 train loss: 0.0181467 valid loss: 0.0243925 P@5: 0.22000 N@5: 0.36528 early stop: 0\n",
      "92 800 train loss: 0.0172856 valid loss: 0.0245636 P@5: 0.22143 N@5: 0.36228 early stop: 0\n",
      "92 1600 train loss: 0.0175208 valid loss: 0.0245846 P@5: 0.21629 N@5: 0.35843 early stop: 0\n",
      "92 2400 train loss: 0.0174009 valid loss: 0.0244234 P@5: 0.22143 N@5: 0.36697 early stop: 0\n",
      "93 400 train loss: 0.0172281 valid loss: 0.0244644 P@5: 0.22171 N@5: 0.36228 early stop: 0\n",
      "93 1200 train loss: 0.0173392 valid loss: 0.0245258 P@5: 0.21886 N@5: 0.36086 early stop: 0\n",
      "93 2000 train loss: 0.0179784 valid loss: 0.0243781 P@5: 0.22429 N@5: 0.36457 early stop: 0\n",
      "93 2800 train loss: 0.0171717 valid loss: 0.0244330 P@5: 0.22371 N@5: 0.36286 early stop: 0\n",
      "94 800 train loss: 0.0168641 valid loss: 0.0247624 P@5: 0.22029 N@5: 0.35989 early stop: 0\n",
      "94 1600 train loss: 0.0171099 valid loss: 0.0245747 P@5: 0.22000 N@5: 0.35887 early stop: 0\n",
      "94 2400 train loss: 0.0175395 valid loss: 0.0244476 P@5: 0.22086 N@5: 0.35874 early stop: 0\n",
      "95 400 train loss: 0.0172578 valid loss: 0.0244642 P@5: 0.22171 N@5: 0.36727 early stop: 0\n",
      "95 1200 train loss: 0.0179536 valid loss: 0.0244331 P@5: 0.22171 N@5: 0.36239 early stop: 0\n",
      "95 2000 train loss: 0.0172234 valid loss: 0.0245385 P@5: 0.21800 N@5: 0.36326 early stop: 0\n",
      "95 2800 train loss: 0.0168394 valid loss: 0.0244475 P@5: 0.22257 N@5: 0.36324 early stop: 0\n",
      "96 800 train loss: 0.0173339 valid loss: 0.0244088 P@5: 0.22314 N@5: 0.36310 early stop: 0\n",
      "96 1600 train loss: 0.0167762 valid loss: 0.0244829 P@5: 0.22057 N@5: 0.36702 early stop: 0\n",
      "96 2400 train loss: 0.0172668 valid loss: 0.0245347 P@5: 0.21857 N@5: 0.36374 early stop: 0\n",
      "97 400 train loss: 0.0172943 valid loss: 0.0245648 P@5: 0.22057 N@5: 0.36674 early stop: 0\n",
      "97 1200 train loss: 0.0170993 valid loss: 0.0244973 P@5: 0.22343 N@5: 0.36653 early stop: 0\n",
      "97 2000 train loss: 0.0167949 valid loss: 0.0245065 P@5: 0.21429 N@5: 0.34952 early stop: 0\n",
      "97 2800 train loss: 0.0172307 valid loss: 0.0243984 P@5: 0.22371 N@5: 0.36918 early stop: 0\n",
      "98 800 train loss: 0.0167104 valid loss: 0.0244672 P@5: 0.22286 N@5: 0.36601 early stop: 0\n",
      "98 1600 train loss: 0.0170708 valid loss: 0.0245125 P@5: 0.22543 N@5: 0.37227 early stop: 0\n",
      "98 2400 train loss: 0.0171857 valid loss: 0.0244787 P@5: 0.22143 N@5: 0.36153 early stop: 0\n",
      "99 400 train loss: 0.0170752 valid loss: 0.0245615 P@5: 0.21057 N@5: 0.34335 early stop: 0\n",
      "99 1200 train loss: 0.0165291 valid loss: 0.0245054 P@5: 0.22229 N@5: 0.36217 early stop: 0\n",
      "99 2000 train loss: 0.0168672 valid loss: 0.0245767 P@5: 0.21857 N@5: 0.35741 early stop: 0\n",
      "99 2800 train loss: 0.0173072 valid loss: 0.0243336 P@5: 0.22371 N@5: 0.36978 early stop: 0\n",
      "CPU times: user 1min 29s, sys: 940 ms, total: 1min 30s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, opt_params={\"lr\": 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3641552511415525,\n",
       " 0.20684931506849316,\n",
       " 0.14132420091324202,\n",
       " 0.3641552511415525,\n",
       " 0.3372622402061743,\n",
       " 0.39297114342816125]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LDA(num_topics=num_labels) + navek_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"LDACorrectionNet with init\"]+metrics+[\"3min 56s + 3min 39s + 1min 30s\"]+[\"1.7 Gb + 8 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=LDACorrectionNetLarge,\n",
    "              emb_size=300, num_labels=train_labels.shape[1], num_topics=train_labels.shape[1], hidden_states=[900, 1500, 900],\n",
    "              init_embs=topics_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.1099526 valid loss: 0.0304924 P@5: 0.07000 N@5: 0.10580 early stop: 0\n",
      "0 1600 train loss: 0.0307205 valid loss: 0.0305396 P@5: 0.07429 N@5: 0.11198 early stop: 0\n",
      "0 2400 train loss: 0.0305679 valid loss: 0.0305632 P@5: 0.06714 N@5: 0.07909 early stop: 0\n",
      "1 400 train loss: 0.0308407 valid loss: 0.0301646 P@5: 0.07600 N@5: 0.10585 early stop: 0\n",
      "1 1200 train loss: 0.0303319 valid loss: 0.0304293 P@5: 0.07143 N@5: 0.10702 early stop: 0\n",
      "1 2000 train loss: 0.0303538 valid loss: 0.0301880 P@5: 0.06486 N@5: 0.09751 early stop: 0\n",
      "1 2800 train loss: 0.0307772 valid loss: 0.0304892 P@5: 0.06600 N@5: 0.09786 early stop: 0\n",
      "2 800 train loss: 0.0308837 valid loss: 0.0302520 P@5: 0.08114 N@5: 0.10625 early stop: 0\n",
      "2 1600 train loss: 0.0296426 valid loss: 0.0302876 P@5: 0.07857 N@5: 0.11881 early stop: 0\n",
      "2 2400 train loss: 0.0305746 valid loss: 0.0302469 P@5: 0.05886 N@5: 0.09439 early stop: 0\n",
      "3 400 train loss: 0.0299799 valid loss: 0.0301903 P@5: 0.06114 N@5: 0.08823 early stop: 0\n",
      "3 1200 train loss: 0.0304581 valid loss: 0.0299941 P@5: 0.07571 N@5: 0.11434 early stop: 0\n",
      "3 2000 train loss: 0.0296410 valid loss: 0.0300853 P@5: 0.07686 N@5: 0.10779 early stop: 0\n",
      "3 2800 train loss: 0.0310966 valid loss: 0.0302836 P@5: 0.07743 N@5: 0.11299 early stop: 0\n",
      "4 800 train loss: 0.0302066 valid loss: 0.0301968 P@5: 0.07200 N@5: 0.09678 early stop: 0\n",
      "4 1600 train loss: 0.0298204 valid loss: 0.0300719 P@5: 0.07914 N@5: 0.11616 early stop: 0\n",
      "4 2400 train loss: 0.0303321 valid loss: 0.0300932 P@5: 0.06971 N@5: 0.09653 early stop: 0\n",
      "5 400 train loss: 0.0305209 valid loss: 0.0300560 P@5: 0.07429 N@5: 0.11160 early stop: 0\n",
      "5 1200 train loss: 0.0299522 valid loss: 0.0299318 P@5: 0.07600 N@5: 0.11550 early stop: 0\n",
      "5 2000 train loss: 0.0297957 valid loss: 0.0299346 P@5: 0.07286 N@5: 0.11045 early stop: 0\n",
      "5 2800 train loss: 0.0305005 valid loss: 0.0298010 P@5: 0.08371 N@5: 0.12460 early stop: 0\n",
      "6 800 train loss: 0.0296035 valid loss: 0.0301338 P@5: 0.07571 N@5: 0.10318 early stop: 0\n",
      "6 1600 train loss: 0.0302396 valid loss: 0.0300571 P@5: 0.07286 N@5: 0.10821 early stop: 0\n",
      "6 2400 train loss: 0.0302174 valid loss: 0.0297877 P@5: 0.08286 N@5: 0.11845 early stop: 0\n",
      "7 400 train loss: 0.0297563 valid loss: 0.0299194 P@5: 0.08657 N@5: 0.11923 early stop: 0\n",
      "7 1200 train loss: 0.0304363 valid loss: 0.0299345 P@5: 0.07371 N@5: 0.09635 early stop: 0\n",
      "7 2000 train loss: 0.0297016 valid loss: 0.0298758 P@5: 0.06914 N@5: 0.10489 early stop: 0\n",
      "7 2800 train loss: 0.0300076 valid loss: 0.0298929 P@5: 0.07686 N@5: 0.12434 early stop: 0\n",
      "8 800 train loss: 0.0295614 valid loss: 0.0297705 P@5: 0.08886 N@5: 0.13398 early stop: 0\n",
      "8 1600 train loss: 0.0298645 valid loss: 0.0296286 P@5: 0.08914 N@5: 0.13325 early stop: 0\n",
      "8 2400 train loss: 0.0296588 valid loss: 0.0295036 P@5: 0.08771 N@5: 0.14090 early stop: 0\n",
      "9 400 train loss: 0.0288492 valid loss: 0.0293218 P@5: 0.10057 N@5: 0.15640 early stop: 0\n",
      "9 1200 train loss: 0.0290301 valid loss: 0.0290720 P@5: 0.10086 N@5: 0.16014 early stop: 0\n",
      "9 2000 train loss: 0.0297503 valid loss: 0.0291016 P@5: 0.10314 N@5: 0.15212 early stop: 0\n",
      "9 2800 train loss: 0.0294157 valid loss: 0.0286947 P@5: 0.10800 N@5: 0.16567 early stop: 0\n",
      "10 800 train loss: 0.0281406 valid loss: 0.0285849 P@5: 0.11143 N@5: 0.16884 early stop: 0\n",
      "10 1600 train loss: 0.0291527 valid loss: 0.0283857 P@5: 0.11200 N@5: 0.17776 early stop: 0\n",
      "10 2400 train loss: 0.0285372 valid loss: 0.0284548 P@5: 0.10657 N@5: 0.16426 early stop: 0\n",
      "11 400 train loss: 0.0281041 valid loss: 0.0282079 P@5: 0.12029 N@5: 0.18439 early stop: 0\n",
      "11 1200 train loss: 0.0283189 valid loss: 0.0281886 P@5: 0.12171 N@5: 0.18173 early stop: 0\n",
      "11 2000 train loss: 0.0281524 valid loss: 0.0280454 P@5: 0.11171 N@5: 0.17037 early stop: 0\n",
      "11 2800 train loss: 0.0278235 valid loss: 0.0279307 P@5: 0.12029 N@5: 0.18259 early stop: 0\n",
      "12 800 train loss: 0.0279449 valid loss: 0.0279207 P@5: 0.12371 N@5: 0.19598 early stop: 0\n",
      "12 1600 train loss: 0.0268892 valid loss: 0.0277015 P@5: 0.12714 N@5: 0.20765 early stop: 0\n",
      "12 2400 train loss: 0.0276588 valid loss: 0.0276727 P@5: 0.13400 N@5: 0.21556 early stop: 0\n",
      "13 400 train loss: 0.0280079 valid loss: 0.0276913 P@5: 0.12829 N@5: 0.20409 early stop: 0\n",
      "13 1200 train loss: 0.0273592 valid loss: 0.0274877 P@5: 0.13686 N@5: 0.21180 early stop: 0\n",
      "13 2000 train loss: 0.0268399 valid loss: 0.0272255 P@5: 0.14143 N@5: 0.21495 early stop: 0\n",
      "13 2800 train loss: 0.0272323 valid loss: 0.0275843 P@5: 0.12486 N@5: 0.18453 early stop: 0\n",
      "14 800 train loss: 0.0265912 valid loss: 0.0277164 P@5: 0.13143 N@5: 0.20745 early stop: 0\n",
      "14 1600 train loss: 0.0270246 valid loss: 0.0270441 P@5: 0.14086 N@5: 0.22231 early stop: 0\n",
      "14 2400 train loss: 0.0270233 valid loss: 0.0271113 P@5: 0.13486 N@5: 0.21794 early stop: 0\n",
      "15 400 train loss: 0.0264255 valid loss: 0.0270088 P@5: 0.14800 N@5: 0.23106 early stop: 0\n",
      "15 1200 train loss: 0.0264794 valid loss: 0.0270210 P@5: 0.14800 N@5: 0.23566 early stop: 0\n",
      "15 2000 train loss: 0.0266179 valid loss: 0.0270033 P@5: 0.14457 N@5: 0.22712 early stop: 0\n",
      "15 2800 train loss: 0.0272432 valid loss: 0.0268675 P@5: 0.14486 N@5: 0.23232 early stop: 0\n",
      "16 800 train loss: 0.0254433 valid loss: 0.0269135 P@5: 0.14771 N@5: 0.23352 early stop: 0\n",
      "16 1600 train loss: 0.0265083 valid loss: 0.0268273 P@5: 0.15143 N@5: 0.24103 early stop: 0\n",
      "16 2400 train loss: 0.0271992 valid loss: 0.0267538 P@5: 0.14886 N@5: 0.22395 early stop: 0\n",
      "17 400 train loss: 0.0260080 valid loss: 0.0267866 P@5: 0.14143 N@5: 0.22917 early stop: 0\n",
      "17 1200 train loss: 0.0259814 valid loss: 0.0266678 P@5: 0.15229 N@5: 0.24088 early stop: 0\n",
      "17 2000 train loss: 0.0266692 valid loss: 0.0266234 P@5: 0.15400 N@5: 0.24586 early stop: 0\n",
      "17 2800 train loss: 0.0258119 valid loss: 0.0267524 P@5: 0.15429 N@5: 0.24520 early stop: 0\n",
      "18 800 train loss: 0.0257222 valid loss: 0.0264347 P@5: 0.15886 N@5: 0.24856 early stop: 0\n",
      "18 1600 train loss: 0.0259282 valid loss: 0.0267491 P@5: 0.15000 N@5: 0.24796 early stop: 0\n",
      "18 2400 train loss: 0.0254332 valid loss: 0.0263331 P@5: 0.15486 N@5: 0.24913 early stop: 0\n",
      "19 400 train loss: 0.0253045 valid loss: 0.0264675 P@5: 0.15657 N@5: 0.24876 early stop: 0\n",
      "19 1200 train loss: 0.0254795 valid loss: 0.0263819 P@5: 0.15257 N@5: 0.25100 early stop: 0\n",
      "19 2000 train loss: 0.0254788 valid loss: 0.0262408 P@5: 0.16143 N@5: 0.25659 early stop: 0\n",
      "19 2800 train loss: 0.0259830 valid loss: 0.0262529 P@5: 0.15914 N@5: 0.26073 early stop: 0\n",
      "20 800 train loss: 0.0248327 valid loss: 0.0260094 P@5: 0.16857 N@5: 0.27028 early stop: 0\n",
      "20 1600 train loss: 0.0251255 valid loss: 0.0261693 P@5: 0.16314 N@5: 0.26121 early stop: 0\n",
      "20 2400 train loss: 0.0255935 valid loss: 0.0260368 P@5: 0.16257 N@5: 0.26019 early stop: 0\n",
      "21 400 train loss: 0.0251086 valid loss: 0.0259577 P@5: 0.16714 N@5: 0.26824 early stop: 0\n",
      "21 1200 train loss: 0.0244773 valid loss: 0.0258783 P@5: 0.17171 N@5: 0.27977 early stop: 0\n",
      "21 2000 train loss: 0.0246965 valid loss: 0.0259791 P@5: 0.16314 N@5: 0.25842 early stop: 0\n",
      "21 2800 train loss: 0.0250577 valid loss: 0.0257132 P@5: 0.17286 N@5: 0.27221 early stop: 0\n",
      "22 800 train loss: 0.0250923 valid loss: 0.0256112 P@5: 0.18029 N@5: 0.28675 early stop: 0\n",
      "22 1600 train loss: 0.0240710 valid loss: 0.0256338 P@5: 0.17543 N@5: 0.27699 early stop: 0\n",
      "22 2400 train loss: 0.0245820 valid loss: 0.0257509 P@5: 0.17086 N@5: 0.27833 early stop: 0\n",
      "23 400 train loss: 0.0239388 valid loss: 0.0255959 P@5: 0.17943 N@5: 0.27496 early stop: 0\n",
      "23 1200 train loss: 0.0242881 valid loss: 0.0256536 P@5: 0.17657 N@5: 0.28093 early stop: 0\n",
      "23 2000 train loss: 0.0244876 valid loss: 0.0256594 P@5: 0.17200 N@5: 0.27657 early stop: 0\n",
      "23 2800 train loss: 0.0241158 valid loss: 0.0257677 P@5: 0.17114 N@5: 0.26972 early stop: 0\n",
      "24 800 train loss: 0.0239794 valid loss: 0.0255840 P@5: 0.18314 N@5: 0.29097 early stop: 0\n",
      "24 1600 train loss: 0.0236143 valid loss: 0.0254189 P@5: 0.17600 N@5: 0.28898 early stop: 0\n",
      "24 2400 train loss: 0.0240853 valid loss: 0.0253498 P@5: 0.18400 N@5: 0.29123 early stop: 0\n",
      "25 400 train loss: 0.0231230 valid loss: 0.0255136 P@5: 0.18057 N@5: 0.28983 early stop: 0\n",
      "25 1200 train loss: 0.0235565 valid loss: 0.0256080 P@5: 0.18286 N@5: 0.29364 early stop: 0\n",
      "25 2000 train loss: 0.0240507 valid loss: 0.0254670 P@5: 0.17629 N@5: 0.28394 early stop: 0\n",
      "25 2800 train loss: 0.0240283 valid loss: 0.0253290 P@5: 0.17600 N@5: 0.28226 early stop: 0\n",
      "26 800 train loss: 0.0226937 valid loss: 0.0254896 P@5: 0.17543 N@5: 0.28255 early stop: 0\n",
      "26 1600 train loss: 0.0239603 valid loss: 0.0254542 P@5: 0.18400 N@5: 0.29578 early stop: 0\n",
      "26 2400 train loss: 0.0231116 valid loss: 0.0257495 P@5: 0.17800 N@5: 0.28546 early stop: 0\n",
      "27 400 train loss: 0.0228814 valid loss: 0.0252727 P@5: 0.18171 N@5: 0.29618 early stop: 0\n",
      "27 1200 train loss: 0.0231775 valid loss: 0.0252663 P@5: 0.18971 N@5: 0.29813 early stop: 0\n",
      "27 2000 train loss: 0.0234290 valid loss: 0.0253580 P@5: 0.18343 N@5: 0.29925 early stop: 0\n",
      "27 2800 train loss: 0.0237135 valid loss: 0.0253360 P@5: 0.18200 N@5: 0.28790 early stop: 0\n",
      "28 800 train loss: 0.0224511 valid loss: 0.0252001 P@5: 0.19143 N@5: 0.30331 early stop: 0\n",
      "28 1600 train loss: 0.0224255 valid loss: 0.0251767 P@5: 0.18657 N@5: 0.30402 early stop: 0\n",
      "28 2400 train loss: 0.0236651 valid loss: 0.0253674 P@5: 0.18086 N@5: 0.28998 early stop: 0\n",
      "29 400 train loss: 0.0226382 valid loss: 0.0254920 P@5: 0.18057 N@5: 0.29069 early stop: 0\n",
      "29 1200 train loss: 0.0225748 valid loss: 0.0257289 P@5: 0.18343 N@5: 0.28928 early stop: 0\n",
      "29 2000 train loss: 0.0232281 valid loss: 0.0252611 P@5: 0.18886 N@5: 0.30280 early stop: 0\n",
      "29 2800 train loss: 0.0228869 valid loss: 0.0254111 P@5: 0.18429 N@5: 0.30174 early stop: 0\n",
      "30 800 train loss: 0.0222093 valid loss: 0.0253366 P@5: 0.19000 N@5: 0.30527 early stop: 0\n",
      "30 1600 train loss: 0.0220449 valid loss: 0.0253288 P@5: 0.19286 N@5: 0.31129 early stop: 0\n",
      "30 2400 train loss: 0.0229855 valid loss: 0.0253846 P@5: 0.18829 N@5: 0.29870 early stop: 0\n",
      "31 400 train loss: 0.0224734 valid loss: 0.0252976 P@5: 0.19000 N@5: 0.30390 early stop: 0\n",
      "31 1200 train loss: 0.0220421 valid loss: 0.0253371 P@5: 0.19029 N@5: 0.29785 early stop: 0\n",
      "31 2000 train loss: 0.0220084 valid loss: 0.0253662 P@5: 0.18857 N@5: 0.30678 early stop: 0\n",
      "31 2800 train loss: 0.0228616 valid loss: 0.0253043 P@5: 0.19486 N@5: 0.30748 early stop: 0\n",
      "32 800 train loss: 0.0215202 valid loss: 0.0253154 P@5: 0.19229 N@5: 0.30701 early stop: 0\n",
      "32 1600 train loss: 0.0223135 valid loss: 0.0252738 P@5: 0.19457 N@5: 0.30788 early stop: 0\n",
      "32 2400 train loss: 0.0223529 valid loss: 0.0253606 P@5: 0.18686 N@5: 0.29613 early stop: 0\n",
      "33 400 train loss: 0.0223498 valid loss: 0.0253220 P@5: 0.18743 N@5: 0.30472 early stop: 0\n",
      "33 1200 train loss: 0.0219471 valid loss: 0.0254738 P@5: 0.19057 N@5: 0.30510 early stop: 0\n",
      "33 2000 train loss: 0.0219355 valid loss: 0.0253439 P@5: 0.19229 N@5: 0.30470 early stop: 0\n",
      "33 2800 train loss: 0.0218223 valid loss: 0.0251392 P@5: 0.19657 N@5: 0.31534 early stop: 0\n",
      "34 800 train loss: 0.0213832 valid loss: 0.0252388 P@5: 0.19314 N@5: 0.31084 early stop: 0\n",
      "34 1600 train loss: 0.0216885 valid loss: 0.0253787 P@5: 0.19114 N@5: 0.30641 early stop: 0\n",
      "34 2400 train loss: 0.0219381 valid loss: 0.0254270 P@5: 0.19171 N@5: 0.30119 early stop: 0\n",
      "35 400 train loss: 0.0217908 valid loss: 0.0255113 P@5: 0.19314 N@5: 0.30324 early stop: 0\n",
      "35 1200 train loss: 0.0211993 valid loss: 0.0253178 P@5: 0.19771 N@5: 0.31901 early stop: 0\n",
      "35 2000 train loss: 0.0216764 valid loss: 0.0254046 P@5: 0.19629 N@5: 0.31307 early stop: 0\n",
      "35 2800 train loss: 0.0213588 valid loss: 0.0254020 P@5: 0.19229 N@5: 0.30520 early stop: 0\n",
      "36 800 train loss: 0.0207153 valid loss: 0.0254919 P@5: 0.19257 N@5: 0.30215 early stop: 0\n",
      "36 1600 train loss: 0.0213781 valid loss: 0.0253832 P@5: 0.19571 N@5: 0.31605 early stop: 0\n",
      "36 2400 train loss: 0.0217370 valid loss: 0.0253680 P@5: 0.19400 N@5: 0.31356 early stop: 0\n",
      "37 400 train loss: 0.0206246 valid loss: 0.0254861 P@5: 0.19686 N@5: 0.31909 early stop: 0\n",
      "37 1200 train loss: 0.0214058 valid loss: 0.0256367 P@5: 0.19714 N@5: 0.31352 early stop: 0\n",
      "37 2000 train loss: 0.0214487 valid loss: 0.0255223 P@5: 0.19086 N@5: 0.30766 early stop: 0\n",
      "37 2800 train loss: 0.0209814 valid loss: 0.0255818 P@5: 0.19143 N@5: 0.30279 early stop: 0\n",
      "38 800 train loss: 0.0200375 valid loss: 0.0256545 P@5: 0.19143 N@5: 0.30234 early stop: 0\n",
      "38 1600 train loss: 0.0205881 valid loss: 0.0253445 P@5: 0.19800 N@5: 0.31804 early stop: 0\n",
      "38 2400 train loss: 0.0214476 valid loss: 0.0255291 P@5: 0.19686 N@5: 0.31143 early stop: 0\n",
      "39 400 train loss: 0.0211413 valid loss: 0.0254712 P@5: 0.19171 N@5: 0.31062 early stop: 0\n",
      "39 1200 train loss: 0.0212119 valid loss: 0.0256112 P@5: 0.19771 N@5: 0.31957 early stop: 0\n",
      "39 2000 train loss: 0.0204952 valid loss: 0.0256354 P@5: 0.19457 N@5: 0.31074 early stop: 0\n",
      "39 2800 train loss: 0.0207234 valid loss: 0.0256938 P@5: 0.19571 N@5: 0.30379 early stop: 0\n",
      "40 800 train loss: 0.0199100 valid loss: 0.0258220 P@5: 0.19686 N@5: 0.31235 early stop: 0\n",
      "40 1600 train loss: 0.0206709 valid loss: 0.0254940 P@5: 0.20257 N@5: 0.32309 early stop: 0\n",
      "40 2400 train loss: 0.0207183 valid loss: 0.0254673 P@5: 0.19657 N@5: 0.32250 early stop: 0\n",
      "41 400 train loss: 0.0200459 valid loss: 0.0256875 P@5: 0.19171 N@5: 0.31146 early stop: 0\n",
      "41 1200 train loss: 0.0203020 valid loss: 0.0256742 P@5: 0.19657 N@5: 0.31267 early stop: 0\n",
      "41 2000 train loss: 0.0202050 valid loss: 0.0255955 P@5: 0.19571 N@5: 0.32394 early stop: 0\n",
      "41 2800 train loss: 0.0209465 valid loss: 0.0256084 P@5: 0.20114 N@5: 0.32123 early stop: 0\n",
      "42 800 train loss: 0.0194771 valid loss: 0.0257219 P@5: 0.19714 N@5: 0.31622 early stop: 0\n",
      "42 1600 train loss: 0.0193596 valid loss: 0.0262570 P@5: 0.18629 N@5: 0.29737 early stop: 0\n",
      "42 2400 train loss: 0.0210504 valid loss: 0.0255195 P@5: 0.20057 N@5: 0.32055 early stop: 0\n",
      "43 400 train loss: 0.0202925 valid loss: 0.0256093 P@5: 0.19771 N@5: 0.32019 early stop: 0\n",
      "43 1200 train loss: 0.0200727 valid loss: 0.0257289 P@5: 0.20314 N@5: 0.32842 early stop: 0\n",
      "43 2000 train loss: 0.0197907 valid loss: 0.0260048 P@5: 0.18886 N@5: 0.30662 early stop: 0\n",
      "43 2800 train loss: 0.0200071 valid loss: 0.0257149 P@5: 0.19629 N@5: 0.31266 early stop: 0\n",
      "44 800 train loss: 0.0189080 valid loss: 0.0259210 P@5: 0.19343 N@5: 0.30501 early stop: 0\n",
      "44 1600 train loss: 0.0198633 valid loss: 0.0257316 P@5: 0.20486 N@5: 0.32685 early stop: 0\n",
      "44 2400 train loss: 0.0205886 valid loss: 0.0257854 P@5: 0.19657 N@5: 0.31959 early stop: 0\n",
      "45 400 train loss: 0.0187694 valid loss: 0.0258488 P@5: 0.19229 N@5: 0.31867 early stop: 0\n",
      "45 1200 train loss: 0.0194414 valid loss: 0.0258522 P@5: 0.19714 N@5: 0.31538 early stop: 0\n",
      "45 2000 train loss: 0.0197075 valid loss: 0.0261269 P@5: 0.19314 N@5: 0.31237 early stop: 0\n",
      "45 2800 train loss: 0.0202938 valid loss: 0.0259651 P@5: 0.19543 N@5: 0.31424 early stop: 0\n",
      "46 800 train loss: 0.0188968 valid loss: 0.0260467 P@5: 0.19657 N@5: 0.32084 early stop: 0\n",
      "46 1600 train loss: 0.0191237 valid loss: 0.0260454 P@5: 0.19571 N@5: 0.31490 early stop: 0\n",
      "46 2400 train loss: 0.0196747 valid loss: 0.0260335 P@5: 0.20029 N@5: 0.31809 early stop: 0\n",
      "47 400 train loss: 0.0192720 valid loss: 0.0259302 P@5: 0.20000 N@5: 0.31895 early stop: 0\n",
      "47 1200 train loss: 0.0196830 valid loss: 0.0258709 P@5: 0.20200 N@5: 0.32612 early stop: 0\n",
      "47 2000 train loss: 0.0192812 valid loss: 0.0258436 P@5: 0.20514 N@5: 0.32680 early stop: 0\n",
      "47 2800 train loss: 0.0191417 valid loss: 0.0259873 P@5: 0.20400 N@5: 0.32879 early stop: 0\n",
      "48 800 train loss: 0.0182263 valid loss: 0.0262210 P@5: 0.19257 N@5: 0.31661 early stop: 0\n",
      "48 1600 train loss: 0.0189903 valid loss: 0.0262393 P@5: 0.19686 N@5: 0.31669 early stop: 0\n",
      "48 2400 train loss: 0.0194168 valid loss: 0.0259336 P@5: 0.20143 N@5: 0.32671 early stop: 0\n",
      "49 400 train loss: 0.0187637 valid loss: 0.0262682 P@5: 0.19543 N@5: 0.31195 early stop: 0\n",
      "49 1200 train loss: 0.0185735 valid loss: 0.0263251 P@5: 0.20457 N@5: 0.32416 early stop: 0\n",
      "49 2000 train loss: 0.0191828 valid loss: 0.0264386 P@5: 0.19657 N@5: 0.31377 early stop: 0\n",
      "49 2800 train loss: 0.0193104 valid loss: 0.0261262 P@5: 0.20343 N@5: 0.32577 early stop: 0\n",
      "CPU times: user 4min 21s, sys: 789 ms, total: 4min 22s\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, opt_params={\"lr\": 1e-4}, nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3162100456621005,\n",
       " 0.18812785388127853,\n",
       " 0.12990867579908677,\n",
       " 0.3162100456621005,\n",
       " 0.3005792302593037,\n",
       " 0.3526834525810236]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(lda_embs, model, \"LargeLDA(num_topics=num_labels) + navek_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"LDACorrectionNetLarge with init\"]+metrics+[\"3min 56s + 3min 39s + 4min 23s\"]+[\"1.7 Gb + 97 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDACorrectionNet</td>\n",
       "      <td>0.344749</td>\n",
       "      <td>0.207534</td>\n",
       "      <td>0.138584</td>\n",
       "      <td>0.344749</td>\n",
       "      <td>0.333974</td>\n",
       "      <td>0.384888</td>\n",
       "      <td>3min 56s + 2min 57s</td>\n",
       "      <td>1.7 Gb + 7.8 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CorNetLDACorrectionNet</td>\n",
       "      <td>0.288813</td>\n",
       "      <td>0.179909</td>\n",
       "      <td>0.127740</td>\n",
       "      <td>0.288813</td>\n",
       "      <td>0.286869</td>\n",
       "      <td>0.342457</td>\n",
       "      <td>3min 17s + 2min 11s</td>\n",
       "      <td>1.7 Gb + 10 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDACorrectionNet(num_topics=300)</td>\n",
       "      <td>0.313927</td>\n",
       "      <td>0.188813</td>\n",
       "      <td>0.129909</td>\n",
       "      <td>0.313927</td>\n",
       "      <td>0.300799</td>\n",
       "      <td>0.353295</td>\n",
       "      <td>1min 17s + 1min 35s</td>\n",
       "      <td>836 Mb + 8 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDACorrectionNetLarge</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.214840</td>\n",
       "      <td>0.139269</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.351067</td>\n",
       "      <td>0.398465</td>\n",
       "      <td>3min 56s + 4min 48s</td>\n",
       "      <td>1.7 Gb + 97 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CorNetLDACorrectionNetLarge</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>0.208219</td>\n",
       "      <td>0.138014</td>\n",
       "      <td>0.355023</td>\n",
       "      <td>0.341591</td>\n",
       "      <td>0.391434</td>\n",
       "      <td>3min 56s + 4min 56s</td>\n",
       "      <td>1.7 Gb + 103 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDACorrectionNet with init</td>\n",
       "      <td>0.364155</td>\n",
       "      <td>0.206849</td>\n",
       "      <td>0.141324</td>\n",
       "      <td>0.364155</td>\n",
       "      <td>0.337262</td>\n",
       "      <td>0.392971</td>\n",
       "      <td>3min 56s + 3min 39s + 1min 30s</td>\n",
       "      <td>1.7 Gb + 8 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDACorrectionNetLarge with init</td>\n",
       "      <td>0.316210</td>\n",
       "      <td>0.188128</td>\n",
       "      <td>0.129909</td>\n",
       "      <td>0.316210</td>\n",
       "      <td>0.300579</td>\n",
       "      <td>0.352683</td>\n",
       "      <td>3min 56s + 3min 39s + 4min 23s</td>\n",
       "      <td>1.7 Gb + 97 Mb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model_name       P@1       P@5      P@10       N@1  \\\n",
       "0                  LDACorrectionNet  0.344749  0.207534  0.138584  0.344749   \n",
       "0            CorNetLDACorrectionNet  0.288813  0.179909  0.127740  0.288813   \n",
       "0  LDACorrectionNet(num_topics=300)  0.313927  0.188813  0.129909  0.313927   \n",
       "0             LDACorrectionNetLarge  0.369863  0.214840  0.139269  0.369863   \n",
       "0       CorNetLDACorrectionNetLarge  0.355023  0.208219  0.138014  0.355023   \n",
       "0        LDACorrectionNet with init  0.364155  0.206849  0.141324  0.364155   \n",
       "0   LDACorrectionNetLarge with init  0.316210  0.188128  0.129909  0.316210   \n",
       "\n",
       "        N@5      N@10                            time             size  \n",
       "0  0.333974  0.384888             3min 56s + 2min 57s  1.7 Gb + 7.8 Mb  \n",
       "0  0.286869  0.342457             3min 17s + 2min 11s   1.7 Gb + 10 Mb  \n",
       "0  0.300799  0.353295             1min 17s + 1min 35s    836 Mb + 8 Mb  \n",
       "0  0.351067  0.398465             3min 56s + 4min 48s   1.7 Gb + 97 Mb  \n",
       "0  0.341591  0.391434             3min 56s + 4min 56s  1.7 Gb + 103 Mb  \n",
       "0  0.337262  0.392971  3min 56s + 3min 39s + 1min 30s    1.7 Gb + 8 Mb  \n",
       "0  0.300579  0.352683  3min 56s + 3min 39s + 4min 23s   1.7 Gb + 97 Mb  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
