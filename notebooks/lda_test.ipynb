{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from deepxml.dataset import MultiLabelDataset\n",
    "from deepxml.models import Model\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from deepxml.evaluation import get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10\n",
    "\n",
    "from deepxml.cornet import CorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/habr_posts_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807711</td>\n",
       "      <td>Kaspersky_Lab</td>\n",
       "      <td>Security Week 2416: уязвимость в серверных мат...</td>\n",
       "      <td>[Блог компании «Лаборатория Касперского», Инфо...</td>\n",
       "      <td>На прошлой неделе исследователи компании Binar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>807709</td>\n",
       "      <td>markshevchenko</td>\n",
       "      <td>Вычислительные выражения: Подробнее про типы-о...</td>\n",
       "      <td>[.NET, Функциональное программирование, F#]</td>\n",
       "      <td>В предыдущем посте мы познакомились с концепци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>807707</td>\n",
       "      <td>ru_vds</td>\n",
       "      <td>Угадай местоположение льдины с арктическим ЦОД...</td>\n",
       "      <td>[Блог компании RUVDS.com, Хостинг, Системное а...</td>\n",
       "      <td>Как вы наверняка знаете, 12 апреля RUVDS успеш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>807705</td>\n",
       "      <td>shaddyk</td>\n",
       "      <td>Запустили проект с НСИС по повышению качества ...</td>\n",
       "      <td>[Блог компании HFLabs, Открытые данные, IT-ком...</td>\n",
       "      <td>НСИС — оператор единой автоматизированной инфо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>807703</td>\n",
       "      <td>VokaMut</td>\n",
       "      <td>Тестируем AI на создании прикладного приложения</td>\n",
       "      <td>[Веб-разработка, Искусственный интеллект, Natu...</td>\n",
       "      <td>Всем привет, я Григорий Тумаков, CTO в Моризо ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>797723</td>\n",
       "      <td>Squirrelfm</td>\n",
       "      <td>Анатомия эффективного собеседования. Что делат...</td>\n",
       "      <td>[Блог компании Raft, Управление персоналом, Ка...</td>\n",
       "      <td>Я провел много собеседований за свою карьеру, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>797721</td>\n",
       "      <td>maybe_elf</td>\n",
       "      <td>None</td>\n",
       "      <td>[Законодательство в IT, Искусственный интеллек...</td>\n",
       "      <td>В OpenAI рассказали о мотивах Илона Маска при ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>797719</td>\n",
       "      <td>ar4w</td>\n",
       "      <td>SD-Access без DNAC и ISE</td>\n",
       "      <td>[Информационная безопасность, IT-инфраструктур...</td>\n",
       "      <td>В 2019 мы закупили комплект оборудования и лиц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>797715</td>\n",
       "      <td>maybe_elf</td>\n",
       "      <td>Meta* удалит все учётные записи Oculus в конце...</td>\n",
       "      <td>[Управление сообществом, Разработка под AR и V...</td>\n",
       "      <td>Meta* в электронной рассылке сообщила пользова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>797713</td>\n",
       "      <td>maybe_elf</td>\n",
       "      <td>Microsoft готовит Copilot для OneDrive</td>\n",
       "      <td>[Облачные сервисы, Искусственный интеллект]</td>\n",
       "      <td>Microsoft объявила о подготовке Copilot для On...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4376 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id          author  \\\n",
       "0      807711   Kaspersky_Lab   \n",
       "1      807709  markshevchenko   \n",
       "2      807707          ru_vds   \n",
       "3      807705         shaddyk   \n",
       "4      807703         VokaMut   \n",
       "...       ...             ...   \n",
       "4371   797723      Squirrelfm   \n",
       "4372   797721       maybe_elf   \n",
       "4373   797719            ar4w   \n",
       "4374   797715       maybe_elf   \n",
       "4375   797713       maybe_elf   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Security Week 2416: уязвимость в серверных мат...   \n",
       "1     Вычислительные выражения: Подробнее про типы-о...   \n",
       "2     Угадай местоположение льдины с арктическим ЦОД...   \n",
       "3     Запустили проект с НСИС по повышению качества ...   \n",
       "4       Тестируем AI на создании прикладного приложения   \n",
       "...                                                 ...   \n",
       "4371  Анатомия эффективного собеседования. Что делат...   \n",
       "4372                                               None   \n",
       "4373                           SD-Access без DNAC и ISE   \n",
       "4374  Meta* удалит все учётные записи Oculus в конце...   \n",
       "4375             Microsoft готовит Copilot для OneDrive   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     [Блог компании «Лаборатория Касперского», Инфо...   \n",
       "1           [.NET, Функциональное программирование, F#]   \n",
       "2     [Блог компании RUVDS.com, Хостинг, Системное а...   \n",
       "3     [Блог компании HFLabs, Открытые данные, IT-ком...   \n",
       "4     [Веб-разработка, Искусственный интеллект, Natu...   \n",
       "...                                                 ...   \n",
       "4371  [Блог компании Raft, Управление персоналом, Ка...   \n",
       "4372  [Законодательство в IT, Искусственный интеллек...   \n",
       "4373  [Информационная безопасность, IT-инфраструктур...   \n",
       "4374  [Управление сообществом, Разработка под AR и V...   \n",
       "4375        [Облачные сервисы, Искусственный интеллект]   \n",
       "\n",
       "                                                   text  \n",
       "0     На прошлой неделе исследователи компании Binar...  \n",
       "1     В предыдущем посте мы познакомились с концепци...  \n",
       "2     Как вы наверняка знаете, 12 апреля RUVDS успеш...  \n",
       "3     НСИС — оператор единой автоматизированной инфо...  \n",
       "4     Всем привет, я Григорий Тумаков, CTO в Моризо ...  \n",
       "...                                                 ...  \n",
       "4371  Я провел много собеседований за свою карьеру, ...  \n",
       "4372  В OpenAI рассказали о мотивах Илона Маска при ...  \n",
       "4373  В 2019 мы закупили комплект оборудования и лиц...  \n",
       "4374  Meta* в электронной рассылке сообщила пользова...  \n",
       "4375  Microsoft объявила о подготовке Copilot для On...  \n",
       "\n",
       "[4376 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str, sep='/SEP/'):\n",
    "    # We added a /SEP/ symbol between titles and descriptions such as Amazon datasets.\n",
    "    return [token.lower() if token != sep else token for token in word_tokenize(sentence)\n",
    "            if len(re.sub(r'[^\\w]', '', token)) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4376 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4376/4376 [00:24<00:00, 178.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized = []\n",
    "for post in tqdm(df[\"text\"]):\n",
    "    tokenized.append(tokenize(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [[i for i in doc if i not in stop_words] for doc in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [[i for i in doc if not i.isdigit()] for doc in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mlb.fit_transform(df[\"tags\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_text, train_labels, val_labels = train_test_split(tokenized, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dictionary(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d.doc2bow(text) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 20)\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [d.get(int(i[0])) for i in words]\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>это</td>\n",
       "      <td>медведи</td>\n",
       "      <td>quest</td>\n",
       "      <td>это</td>\n",
       "      <td>это</td>\n",
       "      <td>это</td>\n",
       "      <td>это</td>\n",
       "      <td>стекла</td>\n",
       "      <td>implicit</td>\n",
       "      <td>это</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>компилятора</td>\n",
       "      <td>column</td>\n",
       "      <td>meta</td>\n",
       "      <td>weights</td>\n",
       "      <td>словари</td>\n",
       "      <td>gpt</td>\n",
       "      <td>image</td>\n",
       "      <td>солнца</td>\n",
       "      <td>typeclass</td>\n",
       "      <td>красном</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>альфа-версии</td>\n",
       "      <td>льда</td>\n",
       "      <td>цветные</td>\n",
       "      <td>контрольных</td>\n",
       "      <td>данных</td>\n",
       "      <td>держит</td>\n",
       "      <td>которые</td>\n",
       "      <td>это</td>\n",
       "      <td>квест</td>\n",
       "      <td>солнце</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>которые</td>\n",
       "      <td>гренландии</td>\n",
       "      <td>устройство</td>\n",
       "      <td>os</td>\n",
       "      <td>например</td>\n",
       "      <td>писатель</td>\n",
       "      <td>сайта</td>\n",
       "      <td>температуре</td>\n",
       "      <td>это</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>также</td>\n",
       "      <td>composable</td>\n",
       "      <td>вадим</td>\n",
       "      <td>usememo</td>\n",
       "      <td>время</td>\n",
       "      <td>которые</td>\n",
       "      <td>нужно</td>\n",
       "      <td>охлаждения</td>\n",
       "      <td>букв</td>\n",
       "      <td>складов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>время</td>\n",
       "      <td>это</td>\n",
       "      <td>это</td>\n",
       "      <td>al</td>\n",
       "      <td>которые</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>например</td>\n",
       "      <td>галактик</td>\n",
       "      <td>selectel</td>\n",
       "      <td>которые</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>например</td>\n",
       "      <td>лёд</td>\n",
       "      <td>копий</td>\n",
       "      <td>данных</td>\n",
       "      <td>который</td>\n",
       "      <td>компании</td>\n",
       "      <td>docker</td>\n",
       "      <td>звёзд</td>\n",
       "      <td>scala</td>\n",
       "      <td>время</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>системы</td>\n",
       "      <td>которые</td>\n",
       "      <td>также</td>\n",
       "      <td>usecallback</td>\n",
       "      <td>volume</td>\n",
       "      <td>рассуждать</td>\n",
       "      <td>run</td>\n",
       "      <td>млечный</td>\n",
       "      <td>мерч</td>\n",
       "      <td>секретами</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>данных</td>\n",
       "      <td>бетона</td>\n",
       "      <td>р</td>\n",
       "      <td>которые</td>\n",
       "      <td>просто</td>\n",
       "      <td>очень</td>\n",
       "      <td>несколько</td>\n",
       "      <td>телескопа</td>\n",
       "      <td>также</td>\n",
       "      <td>раствор</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>который</td>\n",
       "      <td>айсберг</td>\n",
       "      <td>весе</td>\n",
       "      <td>co_await</td>\n",
       "      <td>данные</td>\n",
       "      <td>могут</td>\n",
       "      <td>поэтому</td>\n",
       "      <td>массивных</td>\n",
       "      <td>спрятанные</td>\n",
       "      <td>который</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>несколько</td>\n",
       "      <td>and</td>\n",
       "      <td>печати</td>\n",
       "      <td>модели</td>\n",
       "      <td>компании</td>\n",
       "      <td>который</td>\n",
       "      <td>который</td>\n",
       "      <td>co2</td>\n",
       "      <td>странице</td>\n",
       "      <td>белых</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>создаст</td>\n",
       "      <td>плотного</td>\n",
       "      <td>которые</td>\n",
       "      <td>гиперпараметры</td>\n",
       "      <td>также</td>\n",
       "      <td>например</td>\n",
       "      <td>могут</td>\n",
       "      <td>телескопов</td>\n",
       "      <td>использовать</td>\n",
       "      <td>линзы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>помощью</td>\n",
       "      <td>снег</td>\n",
       "      <td>facebook</td>\n",
       "      <td>эпохе</td>\n",
       "      <td>работает</td>\n",
       "      <td>вопросы</td>\n",
       "      <td>позволяет</td>\n",
       "      <td>гелия</td>\n",
       "      <td>бонусов</td>\n",
       "      <td>данных</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>альфа-версию</td>\n",
       "      <td>могут</td>\n",
       "      <td>устройства</td>\n",
       "      <td>использовать</td>\n",
       "      <td>нам</td>\n",
       "      <td>сделать</td>\n",
       "      <td>dockerfile</td>\n",
       "      <td>вселенной</td>\n",
       "      <td>сервер</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>всё</td>\n",
       "      <td>просто</td>\n",
       "      <td>мфу</td>\n",
       "      <td>поэтому</td>\n",
       "      <td>достаточно</td>\n",
       "      <td>всё</td>\n",
       "      <td>использовать</td>\n",
       "      <td>наблюдения</td>\n",
       "      <td>которые</td>\n",
       "      <td>очень</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>позволяет</td>\n",
       "      <td>нужно</td>\n",
       "      <td>разрешение</td>\n",
       "      <td>всё</td>\n",
       "      <td>нужно</td>\n",
       "      <td>решения</td>\n",
       "      <td>случае</td>\n",
       "      <td>экзопланет</td>\n",
       "      <td>страницах</td>\n",
       "      <td>игры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>работы</td>\n",
       "      <td>the</td>\n",
       "      <td>гарнитура</td>\n",
       "      <td>await_suspend</td>\n",
       "      <td>поэтому</td>\n",
       "      <td>openai</td>\n",
       "      <td>приложение</td>\n",
       "      <td>наблюдений</td>\n",
       "      <td>промокоды</td>\n",
       "      <td>разработчикам</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>тестирования</td>\n",
       "      <td>еды</td>\n",
       "      <td>лазерное</td>\n",
       "      <td>компонент</td>\n",
       "      <td>всё</td>\n",
       "      <td>работы</td>\n",
       "      <td>помощью</td>\n",
       "      <td>которые</td>\n",
       "      <td>который</td>\n",
       "      <td>датасете</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>года</td>\n",
       "      <td>часть</td>\n",
       "      <td>печатать</td>\n",
       "      <td>et</td>\n",
       "      <td>использовать</td>\n",
       "      <td>также</td>\n",
       "      <td>cms</td>\n",
       "      <td>солнце</td>\n",
       "      <td>квесте</td>\n",
       "      <td>также</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>нужно</td>\n",
       "      <td>данных</td>\n",
       "      <td>криптовалют</td>\n",
       "      <td>также</td>\n",
       "      <td>модели</td>\n",
       "      <td>цифровых</td>\n",
       "      <td>инструкции</td>\n",
       "      <td>планеты</td>\n",
       "      <td>скидку</td>\n",
       "      <td>лет</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic # 01  Topic # 02   Topic # 03      Topic # 04    Topic # 05  \\\n",
       "0            это     медведи        quest             это           это   \n",
       "1    компилятора      column         meta         weights       словари   \n",
       "2   альфа-версии        льда      цветные     контрольных        данных   \n",
       "3        которые  гренландии   устройство              os      например   \n",
       "4          также  composable        вадим         usememo         время   \n",
       "5          время         это          это              al       которые   \n",
       "6       например         лёд        копий          данных       который   \n",
       "7        системы     которые        также     usecallback        volume   \n",
       "8         данных      бетона            р         которые        просто   \n",
       "9        который     айсберг         весе        co_await        данные   \n",
       "10     несколько         and       печати          модели      компании   \n",
       "11       создаст    плотного      которые  гиперпараметры         также   \n",
       "12       помощью        снег     facebook           эпохе      работает   \n",
       "13  альфа-версию       могут   устройства    использовать           нам   \n",
       "14           всё      просто          мфу         поэтому    достаточно   \n",
       "15     позволяет       нужно   разрешение             всё         нужно   \n",
       "16        работы         the    гарнитура   await_suspend       поэтому   \n",
       "17  тестирования         еды     лазерное       компонент           всё   \n",
       "18          года       часть     печатать              et  использовать   \n",
       "19         нужно      данных  криптовалют           также        модели   \n",
       "\n",
       "    Topic # 06    Topic # 07   Topic # 08    Topic # 09     Topic # 10  \n",
       "0          это           это       стекла      implicit            это  \n",
       "1          gpt         image       солнца     typeclass        красном  \n",
       "2       держит       которые          это         квест         солнце  \n",
       "3     писатель         сайта  температуре           это          death  \n",
       "4      которые         нужно   охлаждения          букв        складов  \n",
       "5      chatgpt      например     галактик      selectel        которые  \n",
       "6     компании        docker        звёзд         scala          время  \n",
       "7   рассуждать           run      млечный          мерч      секретами  \n",
       "8        очень     несколько    телескопа         также        раствор  \n",
       "9        могут       поэтому    массивных    спрятанные        который  \n",
       "10     который       который          co2      странице          белых  \n",
       "11    например         могут   телескопов  использовать          линзы  \n",
       "12     вопросы     позволяет        гелия       бонусов         данных  \n",
       "13     сделать    dockerfile    вселенной        сервер            the  \n",
       "14         всё  использовать   наблюдения       которые          очень  \n",
       "15     решения        случае   экзопланет     страницах           игры  \n",
       "16      openai    приложение   наблюдений     промокоды  разработчикам  \n",
       "17      работы       помощью      которые       который       датасете  \n",
       "18       также           cms       солнце        квесте          также  \n",
       "19    цифровых    инструкции      планеты        скидку            лет  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(lda, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(567, 0.9915344)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = np.zeros(shape=(len(train_texts), train_labels.shape[1]))\n",
    "for i, doc in enumerate(corpus):\n",
    "    for idx, val in lda[doc]:\n",
    "        train_embs[i, idx] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDACorrectionNet(nn.Module):\n",
    "    def __init__(self, num_labels, bottlenack_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_labels, bottlenack_size, dtype=float)\n",
    "        self.act = torch.sigmoid\n",
    "        self.linear2 = nn.Linear(bottlenack_size, num_labels, dtype=float)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.linear1(input)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorNetLDACorrectionNet(nn.Module):\n",
    "    def __init__(self, num_labels, bottlenack_size):\n",
    "        super().__init__()\n",
    "        self.lda_corect_net = LDACorrectionNet(num_labels, bottlenack_size)\n",
    "        self.cor_net = CorNet(num_labels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_logits = self.lda_corect_net(input)\n",
    "        raw_logits = raw_logits.float()\n",
    "        corr_logits = self.cor_net(raw_logits)\n",
    "\n",
    "        return corr_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetLDACorrectionNet, \n",
    "              bottlenack_size=300, num_labels=train_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MultiLabelDataset(train_embs, train_labels),\n",
    "                          8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_docs = [d.doc2bow(text) for text in val_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embs = np.zeros(shape=(len(val_text), train_labels.shape[1]))\n",
    "for i, doc in enumerate(val_docs):\n",
    "    for idx, val in lda[doc]:\n",
    "        val_embs[i, idx] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(MultiLabelDataset(val_embs, val_labels),\n",
    "                          8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/PapersTagsPrediction/src/deepxml/optimizers.py:97: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1578.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0623080 valid loss: 0.0283165 P@5: 0.05114 N@5: 0.06821 early stop: 0\n",
      "0 1600 train loss: 0.0287368 valid loss: 0.0282236 P@5: 0.07169 N@5: 0.09652 early stop: 0\n",
      "0 2400 train loss: 0.0279387 valid loss: 0.0282496 P@5: 0.06986 N@5: 0.09825 early stop: 0\n",
      "0 3200 train loss: 0.0284423 valid loss: 0.0283268 P@5: 0.07397 N@5: 0.10650 early stop: 0\n",
      "1 56 train loss: 0.0287314 valid loss: 0.0285081 P@5: 0.06301 N@5: 0.09176 early stop: 0\n",
      "1 856 train loss: 0.0280990 valid loss: 0.0281776 P@5: 0.06804 N@5: 0.09079 early stop: 0\n",
      "1 1656 train loss: 0.0283931 valid loss: 0.0282530 P@5: 0.06575 N@5: 0.09510 early stop: 0\n",
      "1 2456 train loss: 0.0278713 valid loss: 0.0283207 P@5: 0.05662 N@5: 0.07781 early stop: 0\n",
      "1 3256 train loss: 0.0282697 valid loss: 0.0282807 P@5: 0.05114 N@5: 0.08279 early stop: 0\n",
      "2 112 train loss: 0.0279791 valid loss: 0.0283045 P@5: 0.07169 N@5: 0.10222 early stop: 0\n",
      "2 912 train loss: 0.0278194 valid loss: 0.0279128 P@5: 0.06530 N@5: 0.10215 early stop: 0\n",
      "2 1712 train loss: 0.0283910 valid loss: 0.0279586 P@5: 0.06621 N@5: 0.10954 early stop: 0\n",
      "2 2512 train loss: 0.0285922 valid loss: 0.0280977 P@5: 0.05662 N@5: 0.09297 early stop: 0\n",
      "2 3312 train loss: 0.0276127 valid loss: 0.0282514 P@5: 0.07443 N@5: 0.11129 early stop: 0\n",
      "3 168 train loss: 0.0281830 valid loss: 0.0280491 P@5: 0.06712 N@5: 0.10491 early stop: 0\n",
      "3 968 train loss: 0.0278573 valid loss: 0.0278476 P@5: 0.07991 N@5: 0.13450 early stop: 0\n",
      "3 1768 train loss: 0.0282161 valid loss: 0.0280478 P@5: 0.07763 N@5: 0.10596 early stop: 0\n",
      "3 2568 train loss: 0.0272631 valid loss: 0.0279480 P@5: 0.07306 N@5: 0.11414 early stop: 0\n",
      "3 3368 train loss: 0.0283143 valid loss: 0.0279431 P@5: 0.07352 N@5: 0.10844 early stop: 0\n",
      "4 224 train loss: 0.0276366 valid loss: 0.0279556 P@5: 0.07169 N@5: 0.10205 early stop: 0\n",
      "4 1024 train loss: 0.0268173 valid loss: 0.0280680 P@5: 0.06621 N@5: 0.09212 early stop: 0\n",
      "4 1824 train loss: 0.0277956 valid loss: 0.0277138 P@5: 0.07352 N@5: 0.11559 early stop: 0\n",
      "4 2624 train loss: 0.0282638 valid loss: 0.0277224 P@5: 0.08721 N@5: 0.13632 early stop: 0\n",
      "4 3424 train loss: 0.0279153 valid loss: 0.0275746 P@5: 0.08584 N@5: 0.13770 early stop: 0\n",
      "5 280 train loss: 0.0274498 valid loss: 0.0278540 P@5: 0.07306 N@5: 0.11001 early stop: 0\n",
      "5 1080 train loss: 0.0270074 valid loss: 0.0275725 P@5: 0.08858 N@5: 0.13534 early stop: 0\n",
      "5 1880 train loss: 0.0274149 valid loss: 0.0275421 P@5: 0.08858 N@5: 0.14370 early stop: 0\n",
      "5 2680 train loss: 0.0275888 valid loss: 0.0275173 P@5: 0.09041 N@5: 0.13432 early stop: 0\n",
      "5 3480 train loss: 0.0276389 valid loss: 0.0274402 P@5: 0.09315 N@5: 0.15056 early stop: 0\n",
      "6 336 train loss: 0.0271568 valid loss: 0.0271854 P@5: 0.10685 N@5: 0.18823 early stop: 0\n",
      "6 1136 train loss: 0.0271682 valid loss: 0.0273271 P@5: 0.09543 N@5: 0.16382 early stop: 0\n",
      "6 1936 train loss: 0.0269257 valid loss: 0.0269986 P@5: 0.09498 N@5: 0.17097 early stop: 0\n",
      "6 2736 train loss: 0.0270857 valid loss: 0.0270802 P@5: 0.10274 N@5: 0.16840 early stop: 0\n",
      "6 3536 train loss: 0.0270761 valid loss: 0.0267685 P@5: 0.10320 N@5: 0.18230 early stop: 0\n",
      "7 392 train loss: 0.0262283 valid loss: 0.0266402 P@5: 0.11416 N@5: 0.18390 early stop: 0\n",
      "7 1192 train loss: 0.0266625 valid loss: 0.0265393 P@5: 0.11735 N@5: 0.19305 early stop: 0\n",
      "7 1992 train loss: 0.0262941 valid loss: 0.0264751 P@5: 0.11644 N@5: 0.19198 early stop: 0\n",
      "7 2792 train loss: 0.0262924 valid loss: 0.0262606 P@5: 0.12511 N@5: 0.20349 early stop: 0\n",
      "7 3592 train loss: 0.0261392 valid loss: 0.0260433 P@5: 0.12237 N@5: 0.20730 early stop: 0\n",
      "8 448 train loss: 0.0254768 valid loss: 0.0260396 P@5: 0.12420 N@5: 0.20915 early stop: 0\n",
      "8 1248 train loss: 0.0262212 valid loss: 0.0258170 P@5: 0.13105 N@5: 0.21324 early stop: 0\n",
      "8 2048 train loss: 0.0253397 valid loss: 0.0256361 P@5: 0.13836 N@5: 0.22705 early stop: 0\n",
      "8 2848 train loss: 0.0255733 valid loss: 0.0255561 P@5: 0.13379 N@5: 0.22124 early stop: 0\n",
      "8 3648 train loss: 0.0251574 valid loss: 0.0254896 P@5: 0.13516 N@5: 0.22271 early stop: 0\n",
      "9 504 train loss: 0.0252608 valid loss: 0.0252935 P@5: 0.13288 N@5: 0.22172 early stop: 0\n",
      "9 1304 train loss: 0.0249297 valid loss: 0.0252418 P@5: 0.14658 N@5: 0.24115 early stop: 0\n",
      "9 2104 train loss: 0.0250190 valid loss: 0.0253910 P@5: 0.15479 N@5: 0.23840 early stop: 0\n",
      "9 2904 train loss: 0.0244607 valid loss: 0.0249951 P@5: 0.14840 N@5: 0.24515 early stop: 0\n",
      "9 3704 train loss: 0.0250627 valid loss: 0.0253537 P@5: 0.14110 N@5: 0.23302 early stop: 0\n",
      "10 560 train loss: 0.0244584 valid loss: 0.0250219 P@5: 0.14155 N@5: 0.22370 early stop: 0\n",
      "10 1360 train loss: 0.0243162 valid loss: 0.0247851 P@5: 0.14338 N@5: 0.23452 early stop: 0\n",
      "10 2160 train loss: 0.0242824 valid loss: 0.0249129 P@5: 0.14064 N@5: 0.22647 early stop: 0\n",
      "10 2960 train loss: 0.0244476 valid loss: 0.0248218 P@5: 0.15388 N@5: 0.24760 early stop: 0\n",
      "10 3760 train loss: 0.0242607 valid loss: 0.0246848 P@5: 0.15297 N@5: 0.24515 early stop: 0\n",
      "11 616 train loss: 0.0246729 valid loss: 0.0244649 P@5: 0.15571 N@5: 0.25965 early stop: 0\n",
      "11 1416 train loss: 0.0238234 valid loss: 0.0247302 P@5: 0.15434 N@5: 0.25703 early stop: 0\n",
      "11 2216 train loss: 0.0241263 valid loss: 0.0246032 P@5: 0.16119 N@5: 0.25223 early stop: 0\n",
      "11 3016 train loss: 0.0235806 valid loss: 0.0244238 P@5: 0.15434 N@5: 0.24985 early stop: 0\n",
      "11 3816 train loss: 0.0235243 valid loss: 0.0242153 P@5: 0.16301 N@5: 0.26647 early stop: 0\n",
      "12 672 train loss: 0.0234590 valid loss: 0.0244306 P@5: 0.15982 N@5: 0.25805 early stop: 0\n",
      "12 1472 train loss: 0.0233053 valid loss: 0.0244290 P@5: 0.15616 N@5: 0.25009 early stop: 0\n",
      "12 2272 train loss: 0.0238335 valid loss: 0.0240218 P@5: 0.17123 N@5: 0.28050 early stop: 0\n",
      "12 3072 train loss: 0.0237684 valid loss: 0.0240085 P@5: 0.17489 N@5: 0.28429 early stop: 0\n",
      "12 3872 train loss: 0.0231940 valid loss: 0.0242638 P@5: 0.16438 N@5: 0.26485 early stop: 0\n",
      "13 728 train loss: 0.0238767 valid loss: 0.0241748 P@5: 0.17032 N@5: 0.27173 early stop: 0\n",
      "13 1528 train loss: 0.0230292 valid loss: 0.0238231 P@5: 0.17991 N@5: 0.29439 early stop: 0\n",
      "13 2328 train loss: 0.0230334 valid loss: 0.0242129 P@5: 0.16621 N@5: 0.26365 early stop: 0\n",
      "13 3128 train loss: 0.0229812 valid loss: 0.0239689 P@5: 0.17763 N@5: 0.28859 early stop: 0\n",
      "13 3928 train loss: 0.0231326 valid loss: 0.0239286 P@5: 0.16712 N@5: 0.26383 early stop: 0\n",
      "14 784 train loss: 0.0222549 valid loss: 0.0235756 P@5: 0.18265 N@5: 0.29550 early stop: 0\n",
      "14 1584 train loss: 0.0230727 valid loss: 0.0238941 P@5: 0.18630 N@5: 0.29567 early stop: 0\n",
      "14 2384 train loss: 0.0229358 valid loss: 0.0236616 P@5: 0.18037 N@5: 0.29270 early stop: 0\n",
      "14 3184 train loss: 0.0224615 valid loss: 0.0237309 P@5: 0.17671 N@5: 0.29023 early stop: 0\n",
      "15 40 train loss: 0.0227913 valid loss: 0.0236422 P@5: 0.18356 N@5: 0.30043 early stop: 0\n",
      "15 840 train loss: 0.0222393 valid loss: 0.0236141 P@5: 0.18402 N@5: 0.29836 early stop: 0\n",
      "15 1640 train loss: 0.0229637 valid loss: 0.0236271 P@5: 0.18493 N@5: 0.29666 early stop: 0\n",
      "15 2440 train loss: 0.0228226 valid loss: 0.0237443 P@5: 0.18128 N@5: 0.29864 early stop: 0\n",
      "15 3240 train loss: 0.0221713 valid loss: 0.0234648 P@5: 0.18219 N@5: 0.30004 early stop: 0\n",
      "16 96 train loss: 0.0227257 valid loss: 0.0236099 P@5: 0.17580 N@5: 0.29103 early stop: 0\n",
      "16 896 train loss: 0.0219637 valid loss: 0.0234273 P@5: 0.18356 N@5: 0.29492 early stop: 0\n",
      "16 1696 train loss: 0.0221653 valid loss: 0.0233275 P@5: 0.19680 N@5: 0.31737 early stop: 0\n",
      "16 2496 train loss: 0.0222487 valid loss: 0.0234934 P@5: 0.19087 N@5: 0.31710 early stop: 0\n",
      "16 3296 train loss: 0.0224896 valid loss: 0.0233300 P@5: 0.19543 N@5: 0.32328 early stop: 0\n",
      "17 152 train loss: 0.0218530 valid loss: 0.0233432 P@5: 0.18402 N@5: 0.30064 early stop: 0\n",
      "17 952 train loss: 0.0216087 valid loss: 0.0232807 P@5: 0.19680 N@5: 0.31922 early stop: 0\n",
      "17 1752 train loss: 0.0215536 valid loss: 0.0234574 P@5: 0.20091 N@5: 0.32258 early stop: 0\n",
      "17 2552 train loss: 0.0222088 valid loss: 0.0232288 P@5: 0.18904 N@5: 0.30539 early stop: 0\n",
      "17 3352 train loss: 0.0219478 valid loss: 0.0231772 P@5: 0.19361 N@5: 0.31339 early stop: 0\n",
      "18 208 train loss: 0.0225910 valid loss: 0.0231954 P@5: 0.19269 N@5: 0.31805 early stop: 0\n",
      "18 1008 train loss: 0.0216905 valid loss: 0.0230201 P@5: 0.19680 N@5: 0.32027 early stop: 0\n",
      "18 1808 train loss: 0.0219059 valid loss: 0.0231808 P@5: 0.20183 N@5: 0.32354 early stop: 0\n",
      "18 2608 train loss: 0.0215177 valid loss: 0.0231381 P@5: 0.19772 N@5: 0.31742 early stop: 0\n",
      "18 3408 train loss: 0.0220085 valid loss: 0.0232717 P@5: 0.19635 N@5: 0.32654 early stop: 0\n",
      "19 264 train loss: 0.0208830 valid loss: 0.0233451 P@5: 0.19452 N@5: 0.32314 early stop: 0\n",
      "19 1064 train loss: 0.0214321 valid loss: 0.0228724 P@5: 0.20137 N@5: 0.33188 early stop: 0\n",
      "19 1864 train loss: 0.0209427 valid loss: 0.0233257 P@5: 0.19772 N@5: 0.31676 early stop: 0\n",
      "19 2664 train loss: 0.0220243 valid loss: 0.0231207 P@5: 0.19269 N@5: 0.32062 early stop: 0\n",
      "19 3464 train loss: 0.0217298 valid loss: 0.0230868 P@5: 0.20411 N@5: 0.32733 early stop: 0\n",
      "20 320 train loss: 0.0208261 valid loss: 0.0234337 P@5: 0.18950 N@5: 0.30736 early stop: 0\n",
      "20 1120 train loss: 0.0211451 valid loss: 0.0232024 P@5: 0.20274 N@5: 0.32873 early stop: 0\n",
      "20 1920 train loss: 0.0210532 valid loss: 0.0232279 P@5: 0.19178 N@5: 0.31130 early stop: 0\n",
      "20 2720 train loss: 0.0214876 valid loss: 0.0230174 P@5: 0.19498 N@5: 0.32528 early stop: 0\n",
      "20 3520 train loss: 0.0211753 valid loss: 0.0230689 P@5: 0.19406 N@5: 0.31511 early stop: 0\n",
      "21 376 train loss: 0.0211767 valid loss: 0.0229514 P@5: 0.20274 N@5: 0.33018 early stop: 0\n",
      "21 1176 train loss: 0.0199421 valid loss: 0.0228520 P@5: 0.20137 N@5: 0.32672 early stop: 0\n",
      "21 1976 train loss: 0.0207586 valid loss: 0.0229535 P@5: 0.19954 N@5: 0.32849 early stop: 0\n",
      "21 2776 train loss: 0.0207123 valid loss: 0.0227364 P@5: 0.20639 N@5: 0.34061 early stop: 0\n",
      "21 3576 train loss: 0.0212979 valid loss: 0.0226654 P@5: 0.20913 N@5: 0.33414 early stop: 0\n",
      "22 432 train loss: 0.0215751 valid loss: 0.0227584 P@5: 0.20228 N@5: 0.33602 early stop: 0\n",
      "22 1232 train loss: 0.0203682 valid loss: 0.0229983 P@5: 0.20091 N@5: 0.32715 early stop: 0\n",
      "22 2032 train loss: 0.0208051 valid loss: 0.0228426 P@5: 0.19817 N@5: 0.32294 early stop: 0\n",
      "22 2832 train loss: 0.0203560 valid loss: 0.0227648 P@5: 0.21096 N@5: 0.34205 early stop: 0\n",
      "22 3632 train loss: 0.0212922 valid loss: 0.0226190 P@5: 0.21279 N@5: 0.34081 early stop: 0\n",
      "23 488 train loss: 0.0209197 valid loss: 0.0227114 P@5: 0.21142 N@5: 0.34177 early stop: 0\n",
      "23 1288 train loss: 0.0209338 valid loss: 0.0225979 P@5: 0.21142 N@5: 0.33996 early stop: 0\n",
      "23 2088 train loss: 0.0202409 valid loss: 0.0225981 P@5: 0.20457 N@5: 0.33254 early stop: 0\n",
      "23 2888 train loss: 0.0200124 valid loss: 0.0227486 P@5: 0.20639 N@5: 0.33026 early stop: 0\n",
      "23 3688 train loss: 0.0202707 valid loss: 0.0225431 P@5: 0.21507 N@5: 0.34523 early stop: 0\n",
      "24 544 train loss: 0.0202334 valid loss: 0.0231920 P@5: 0.19315 N@5: 0.31139 early stop: 0\n",
      "24 1344 train loss: 0.0195403 valid loss: 0.0223476 P@5: 0.21781 N@5: 0.35737 early stop: 0\n",
      "24 2144 train loss: 0.0204861 valid loss: 0.0227036 P@5: 0.20183 N@5: 0.33429 early stop: 0\n",
      "24 2944 train loss: 0.0203918 valid loss: 0.0228882 P@5: 0.20639 N@5: 0.34077 early stop: 0\n",
      "24 3744 train loss: 0.0202257 valid loss: 0.0226207 P@5: 0.21461 N@5: 0.34273 early stop: 0\n",
      "25 600 train loss: 0.0202787 valid loss: 0.0234171 P@5: 0.19315 N@5: 0.30998 early stop: 0\n",
      "25 1400 train loss: 0.0199931 valid loss: 0.0224360 P@5: 0.21598 N@5: 0.35053 early stop: 0\n",
      "25 2200 train loss: 0.0194051 valid loss: 0.0226997 P@5: 0.21187 N@5: 0.34353 early stop: 0\n",
      "25 3000 train loss: 0.0201322 valid loss: 0.0227561 P@5: 0.21096 N@5: 0.34152 early stop: 0\n",
      "25 3800 train loss: 0.0204746 valid loss: 0.0225581 P@5: 0.21598 N@5: 0.35418 early stop: 0\n",
      "26 656 train loss: 0.0192356 valid loss: 0.0225858 P@5: 0.21644 N@5: 0.35041 early stop: 0\n",
      "26 1456 train loss: 0.0195626 valid loss: 0.0225085 P@5: 0.21735 N@5: 0.34497 early stop: 0\n",
      "26 2256 train loss: 0.0201633 valid loss: 0.0226191 P@5: 0.21416 N@5: 0.34610 early stop: 0\n",
      "26 3056 train loss: 0.0201422 valid loss: 0.0226549 P@5: 0.21050 N@5: 0.34251 early stop: 0\n",
      "26 3856 train loss: 0.0196475 valid loss: 0.0228949 P@5: 0.20868 N@5: 0.34070 early stop: 0\n",
      "27 712 train loss: 0.0193002 valid loss: 0.0224966 P@5: 0.21872 N@5: 0.35719 early stop: 0\n",
      "27 1512 train loss: 0.0193764 valid loss: 0.0225135 P@5: 0.21050 N@5: 0.34615 early stop: 0\n",
      "27 2312 train loss: 0.0197447 valid loss: 0.0224519 P@5: 0.21689 N@5: 0.35720 early stop: 0\n",
      "27 3112 train loss: 0.0197481 valid loss: 0.0223522 P@5: 0.21826 N@5: 0.36039 early stop: 0\n",
      "27 3912 train loss: 0.0195875 valid loss: 0.0225994 P@5: 0.21096 N@5: 0.34519 early stop: 0\n",
      "28 768 train loss: 0.0196580 valid loss: 0.0225151 P@5: 0.21598 N@5: 0.35135 early stop: 0\n",
      "28 1568 train loss: 0.0191427 valid loss: 0.0225862 P@5: 0.21598 N@5: 0.35000 early stop: 0\n",
      "28 2368 train loss: 0.0191870 valid loss: 0.0224035 P@5: 0.22192 N@5: 0.35618 early stop: 0\n",
      "28 3168 train loss: 0.0192017 valid loss: 0.0225888 P@5: 0.21050 N@5: 0.34683 early stop: 0\n",
      "29 24 train loss: 0.0194348 valid loss: 0.0224455 P@5: 0.21872 N@5: 0.35559 early stop: 0\n",
      "29 824 train loss: 0.0184892 valid loss: 0.0225541 P@5: 0.21781 N@5: 0.35526 early stop: 0\n",
      "29 1624 train loss: 0.0190981 valid loss: 0.0228178 P@5: 0.20959 N@5: 0.33903 early stop: 0\n",
      "29 2424 train loss: 0.0195256 valid loss: 0.0227922 P@5: 0.21689 N@5: 0.34899 early stop: 0\n",
      "29 3224 train loss: 0.0187840 valid loss: 0.0227252 P@5: 0.21005 N@5: 0.34575 early stop: 0\n",
      "30 80 train loss: 0.0195694 valid loss: 0.0224440 P@5: 0.21781 N@5: 0.35085 early stop: 0\n",
      "30 880 train loss: 0.0187463 valid loss: 0.0225410 P@5: 0.21598 N@5: 0.35610 early stop: 0\n",
      "30 1680 train loss: 0.0190041 valid loss: 0.0224191 P@5: 0.22146 N@5: 0.35363 early stop: 0\n",
      "30 2480 train loss: 0.0181104 valid loss: 0.0222146 P@5: 0.22694 N@5: 0.37044 early stop: 0\n",
      "30 3280 train loss: 0.0190704 valid loss: 0.0225717 P@5: 0.20868 N@5: 0.34034 early stop: 0\n",
      "31 136 train loss: 0.0192180 valid loss: 0.0225954 P@5: 0.22055 N@5: 0.35913 early stop: 0\n",
      "31 936 train loss: 0.0186085 valid loss: 0.0229865 P@5: 0.20868 N@5: 0.33992 early stop: 0\n",
      "31 1736 train loss: 0.0181085 valid loss: 0.0223794 P@5: 0.21963 N@5: 0.35788 early stop: 0\n",
      "31 2536 train loss: 0.0189270 valid loss: 0.0225594 P@5: 0.21644 N@5: 0.35588 early stop: 0\n",
      "31 3336 train loss: 0.0184219 valid loss: 0.0223387 P@5: 0.22557 N@5: 0.36494 early stop: 0\n",
      "32 192 train loss: 0.0188002 valid loss: 0.0224759 P@5: 0.20868 N@5: 0.34229 early stop: 0\n",
      "32 992 train loss: 0.0184261 valid loss: 0.0226064 P@5: 0.21689 N@5: 0.35593 early stop: 0\n",
      "32 1792 train loss: 0.0181104 valid loss: 0.0223921 P@5: 0.21872 N@5: 0.36498 early stop: 0\n",
      "32 2592 train loss: 0.0179172 valid loss: 0.0223636 P@5: 0.22785 N@5: 0.37020 early stop: 0\n",
      "32 3392 train loss: 0.0191104 valid loss: 0.0226055 P@5: 0.22100 N@5: 0.36137 early stop: 0\n",
      "33 248 train loss: 0.0183096 valid loss: 0.0223324 P@5: 0.21781 N@5: 0.36190 early stop: 0\n",
      "33 1048 train loss: 0.0180005 valid loss: 0.0227422 P@5: 0.21963 N@5: 0.35221 early stop: 0\n",
      "33 1848 train loss: 0.0177778 valid loss: 0.0222947 P@5: 0.22192 N@5: 0.36577 early stop: 0\n",
      "33 2648 train loss: 0.0183220 valid loss: 0.0223914 P@5: 0.21963 N@5: 0.35476 early stop: 0\n",
      "33 3448 train loss: 0.0184355 valid loss: 0.0223921 P@5: 0.22420 N@5: 0.36196 early stop: 0\n",
      "34 304 train loss: 0.0187338 valid loss: 0.0227268 P@5: 0.21689 N@5: 0.35664 early stop: 0\n",
      "34 1104 train loss: 0.0174218 valid loss: 0.0224518 P@5: 0.22237 N@5: 0.36293 early stop: 0\n",
      "34 1904 train loss: 0.0181558 valid loss: 0.0227707 P@5: 0.21918 N@5: 0.35704 early stop: 0\n",
      "34 2704 train loss: 0.0182762 valid loss: 0.0227170 P@5: 0.21370 N@5: 0.35166 early stop: 0\n",
      "34 3504 train loss: 0.0182436 valid loss: 0.0225292 P@5: 0.21963 N@5: 0.36119 early stop: 0\n",
      "35 360 train loss: 0.0174876 valid loss: 0.0227242 P@5: 0.21507 N@5: 0.35891 early stop: 0\n",
      "35 1160 train loss: 0.0175973 valid loss: 0.0224029 P@5: 0.22055 N@5: 0.36029 early stop: 0\n",
      "35 1960 train loss: 0.0170519 valid loss: 0.0226969 P@5: 0.21781 N@5: 0.35934 early stop: 0\n",
      "35 2760 train loss: 0.0182004 valid loss: 0.0228053 P@5: 0.21370 N@5: 0.34623 early stop: 0\n",
      "35 3560 train loss: 0.0179913 valid loss: 0.0223339 P@5: 0.22648 N@5: 0.37539 early stop: 0\n",
      "36 416 train loss: 0.0177188 valid loss: 0.0224365 P@5: 0.22785 N@5: 0.37515 early stop: 0\n",
      "36 1216 train loss: 0.0174081 valid loss: 0.0224137 P@5: 0.22055 N@5: 0.36481 early stop: 0\n",
      "36 2016 train loss: 0.0171082 valid loss: 0.0224685 P@5: 0.22192 N@5: 0.36816 early stop: 0\n",
      "36 2816 train loss: 0.0171324 valid loss: 0.0223880 P@5: 0.22968 N@5: 0.37226 early stop: 0\n",
      "36 3616 train loss: 0.0181242 valid loss: 0.0224041 P@5: 0.22785 N@5: 0.36502 early stop: 0\n",
      "37 472 train loss: 0.0177736 valid loss: 0.0228009 P@5: 0.22329 N@5: 0.36609 early stop: 0\n",
      "37 1272 train loss: 0.0171904 valid loss: 0.0222193 P@5: 0.22922 N@5: 0.37631 early stop: 0\n",
      "37 2072 train loss: 0.0171410 valid loss: 0.0221877 P@5: 0.23196 N@5: 0.37923 early stop: 0\n",
      "37 2872 train loss: 0.0171192 valid loss: 0.0223628 P@5: 0.22603 N@5: 0.37284 early stop: 0\n",
      "37 3672 train loss: 0.0178761 valid loss: 0.0226689 P@5: 0.21872 N@5: 0.36084 early stop: 0\n",
      "38 528 train loss: 0.0166878 valid loss: 0.0228268 P@5: 0.21735 N@5: 0.35711 early stop: 0\n",
      "38 1328 train loss: 0.0173899 valid loss: 0.0222781 P@5: 0.23196 N@5: 0.38085 early stop: 0\n",
      "38 2128 train loss: 0.0172202 valid loss: 0.0222914 P@5: 0.23242 N@5: 0.37572 early stop: 0\n",
      "38 2928 train loss: 0.0170441 valid loss: 0.0223192 P@5: 0.22740 N@5: 0.37288 early stop: 0\n",
      "38 3728 train loss: 0.0171059 valid loss: 0.0227420 P@5: 0.22009 N@5: 0.36248 early stop: 0\n",
      "39 584 train loss: 0.0165067 valid loss: 0.0225290 P@5: 0.22831 N@5: 0.37648 early stop: 0\n",
      "39 1384 train loss: 0.0167776 valid loss: 0.0226446 P@5: 0.22192 N@5: 0.37205 early stop: 0\n",
      "39 2184 train loss: 0.0173130 valid loss: 0.0224344 P@5: 0.22968 N@5: 0.36848 early stop: 0\n",
      "39 2984 train loss: 0.0163592 valid loss: 0.0224642 P@5: 0.22329 N@5: 0.37305 early stop: 0\n",
      "39 3784 train loss: 0.0175168 valid loss: 0.0222014 P@5: 0.22557 N@5: 0.37166 early stop: 0\n",
      "40 640 train loss: 0.0170711 valid loss: 0.0228720 P@5: 0.21826 N@5: 0.35804 early stop: 0\n",
      "40 1440 train loss: 0.0160972 valid loss: 0.0224774 P@5: 0.21461 N@5: 0.36365 early stop: 0\n",
      "40 2240 train loss: 0.0162766 valid loss: 0.0224487 P@5: 0.22420 N@5: 0.37006 early stop: 0\n",
      "40 3040 train loss: 0.0170556 valid loss: 0.0225223 P@5: 0.21963 N@5: 0.35625 early stop: 0\n",
      "40 3840 train loss: 0.0170208 valid loss: 0.0224120 P@5: 0.21918 N@5: 0.36810 early stop: 0\n",
      "41 696 train loss: 0.0160129 valid loss: 0.0223510 P@5: 0.22922 N@5: 0.37496 early stop: 0\n",
      "41 1496 train loss: 0.0160516 valid loss: 0.0225461 P@5: 0.22420 N@5: 0.37212 early stop: 0\n",
      "41 2296 train loss: 0.0164976 valid loss: 0.0228252 P@5: 0.22055 N@5: 0.36085 early stop: 0\n",
      "41 3096 train loss: 0.0167635 valid loss: 0.0223585 P@5: 0.23105 N@5: 0.38438 early stop: 0\n",
      "41 3896 train loss: 0.0168748 valid loss: 0.0222451 P@5: 0.23059 N@5: 0.38247 early stop: 0\n",
      "42 752 train loss: 0.0153769 valid loss: 0.0226901 P@5: 0.22055 N@5: 0.36591 early stop: 0\n",
      "42 1552 train loss: 0.0165493 valid loss: 0.0224105 P@5: 0.22557 N@5: 0.38530 early stop: 0\n",
      "42 2352 train loss: 0.0159228 valid loss: 0.0222876 P@5: 0.23014 N@5: 0.38111 early stop: 0\n",
      "42 3152 train loss: 0.0164090 valid loss: 0.0227003 P@5: 0.22283 N@5: 0.36910 early stop: 0\n",
      "43 8 train loss: 0.0170046 valid loss: 0.0227342 P@5: 0.22283 N@5: 0.36667 early stop: 0\n",
      "43 808 train loss: 0.0156437 valid loss: 0.0225819 P@5: 0.22557 N@5: 0.36636 early stop: 0\n",
      "43 1608 train loss: 0.0161663 valid loss: 0.0226124 P@5: 0.22968 N@5: 0.37875 early stop: 0\n",
      "43 2408 train loss: 0.0157448 valid loss: 0.0224513 P@5: 0.23059 N@5: 0.37961 early stop: 0\n",
      "43 3208 train loss: 0.0164940 valid loss: 0.0227544 P@5: 0.22100 N@5: 0.36058 early stop: 0\n",
      "44 64 train loss: 0.0163878 valid loss: 0.0224325 P@5: 0.22968 N@5: 0.38028 early stop: 0\n",
      "44 864 train loss: 0.0155165 valid loss: 0.0224960 P@5: 0.22511 N@5: 0.37170 early stop: 0\n",
      "44 1664 train loss: 0.0160842 valid loss: 0.0225615 P@5: 0.22420 N@5: 0.37158 early stop: 0\n",
      "44 2464 train loss: 0.0157997 valid loss: 0.0230267 P@5: 0.22100 N@5: 0.36101 early stop: 0\n",
      "44 3264 train loss: 0.0159119 valid loss: 0.0224280 P@5: 0.23470 N@5: 0.38400 early stop: 0\n",
      "45 120 train loss: 0.0161463 valid loss: 0.0227566 P@5: 0.22466 N@5: 0.37459 early stop: 0\n",
      "45 920 train loss: 0.0159359 valid loss: 0.0229714 P@5: 0.22100 N@5: 0.36224 early stop: 0\n",
      "45 1720 train loss: 0.0153816 valid loss: 0.0229071 P@5: 0.22557 N@5: 0.36893 early stop: 0\n",
      "45 2520 train loss: 0.0156380 valid loss: 0.0226076 P@5: 0.23379 N@5: 0.38540 early stop: 0\n",
      "45 3320 train loss: 0.0153486 valid loss: 0.0230616 P@5: 0.21872 N@5: 0.36417 early stop: 0\n",
      "46 176 train loss: 0.0158587 valid loss: 0.0226517 P@5: 0.22922 N@5: 0.37965 early stop: 0\n",
      "46 976 train loss: 0.0149680 valid loss: 0.0228003 P@5: 0.22648 N@5: 0.36840 early stop: 0\n",
      "46 1776 train loss: 0.0154863 valid loss: 0.0233287 P@5: 0.21689 N@5: 0.35177 early stop: 0\n",
      "46 2576 train loss: 0.0152478 valid loss: 0.0228893 P@5: 0.22466 N@5: 0.37602 early stop: 0\n",
      "46 3376 train loss: 0.0161537 valid loss: 0.0225723 P@5: 0.22557 N@5: 0.37606 early stop: 0\n",
      "47 232 train loss: 0.0158179 valid loss: 0.0226527 P@5: 0.22374 N@5: 0.37307 early stop: 0\n",
      "47 1032 train loss: 0.0148625 valid loss: 0.0226800 P@5: 0.22785 N@5: 0.38166 early stop: 0\n",
      "47 1832 train loss: 0.0149639 valid loss: 0.0227519 P@5: 0.22694 N@5: 0.37647 early stop: 0\n",
      "47 2632 train loss: 0.0158395 valid loss: 0.0228054 P@5: 0.22466 N@5: 0.37214 early stop: 0\n",
      "47 3432 train loss: 0.0153035 valid loss: 0.0226605 P@5: 0.22785 N@5: 0.37900 early stop: 0\n",
      "48 288 train loss: 0.0152687 valid loss: 0.0231167 P@5: 0.22922 N@5: 0.37231 early stop: 0\n",
      "48 1088 train loss: 0.0148074 valid loss: 0.0231433 P@5: 0.22192 N@5: 0.36297 early stop: 0\n",
      "48 1888 train loss: 0.0154387 valid loss: 0.0225518 P@5: 0.23425 N@5: 0.38493 early stop: 0\n",
      "48 2688 train loss: 0.0156029 valid loss: 0.0230366 P@5: 0.21689 N@5: 0.36164 early stop: 0\n",
      "48 3488 train loss: 0.0155407 valid loss: 0.0228166 P@5: 0.22511 N@5: 0.36728 early stop: 0\n",
      "49 344 train loss: 0.0139336 valid loss: 0.0229662 P@5: 0.22831 N@5: 0.37456 early stop: 0\n",
      "49 1144 train loss: 0.0147413 valid loss: 0.0230402 P@5: 0.22283 N@5: 0.36951 early stop: 0\n",
      "49 1944 train loss: 0.0149603 valid loss: 0.0234558 P@5: 0.21644 N@5: 0.35437 early stop: 0\n",
      "49 2744 train loss: 0.0149871 valid loss: 0.0228299 P@5: 0.22785 N@5: 0.37710 early stop: 0\n",
      "49 3544 train loss: 0.0157040 valid loss: 0.0227366 P@5: 0.23196 N@5: 0.38129 early stop: 0\n",
      "50 400 train loss: 0.0149705 valid loss: 0.0229931 P@5: 0.23288 N@5: 0.38544 early stop: 0\n",
      "50 1200 train loss: 0.0147463 valid loss: 0.0230377 P@5: 0.23151 N@5: 0.38025 early stop: 0\n",
      "50 2000 train loss: 0.0148301 valid loss: 0.0227831 P@5: 0.22968 N@5: 0.38486 early stop: 0\n",
      "50 2800 train loss: 0.0143627 valid loss: 0.0235697 P@5: 0.22237 N@5: 0.36171 early stop: 0\n",
      "50 3600 train loss: 0.0154212 valid loss: 0.0230352 P@5: 0.23105 N@5: 0.38302 early stop: 0\n",
      "51 456 train loss: 0.0140643 valid loss: 0.0230150 P@5: 0.23151 N@5: 0.37875 early stop: 0\n",
      "51 1256 train loss: 0.0139803 valid loss: 0.0231911 P@5: 0.22466 N@5: 0.36910 early stop: 0\n",
      "51 2056 train loss: 0.0143793 valid loss: 0.0231578 P@5: 0.23151 N@5: 0.37686 early stop: 0\n",
      "51 2856 train loss: 0.0148056 valid loss: 0.0232555 P@5: 0.22466 N@5: 0.37866 early stop: 0\n",
      "51 3656 train loss: 0.0151921 valid loss: 0.0230434 P@5: 0.23470 N@5: 0.37445 early stop: 0\n",
      "52 512 train loss: 0.0147690 valid loss: 0.0230204 P@5: 0.23059 N@5: 0.37571 early stop: 0\n",
      "52 1312 train loss: 0.0135843 valid loss: 0.0232853 P@5: 0.22740 N@5: 0.37810 early stop: 0\n",
      "52 2112 train loss: 0.0139231 valid loss: 0.0232342 P@5: 0.22694 N@5: 0.37043 early stop: 0\n",
      "52 2912 train loss: 0.0148811 valid loss: 0.0231344 P@5: 0.22831 N@5: 0.37032 early stop: 0\n",
      "52 3712 train loss: 0.0149969 valid loss: 0.0232497 P@5: 0.22420 N@5: 0.37384 early stop: 0\n",
      "53 568 train loss: 0.0135150 valid loss: 0.0229752 P@5: 0.23059 N@5: 0.38308 early stop: 0\n",
      "53 1368 train loss: 0.0140432 valid loss: 0.0232426 P@5: 0.22329 N@5: 0.37537 early stop: 0\n",
      "53 2168 train loss: 0.0141368 valid loss: 0.0235550 P@5: 0.22329 N@5: 0.36959 early stop: 0\n",
      "53 2968 train loss: 0.0142191 valid loss: 0.0235729 P@5: 0.21826 N@5: 0.35660 early stop: 0\n",
      "53 3768 train loss: 0.0149011 valid loss: 0.0231065 P@5: 0.22785 N@5: 0.37740 early stop: 0\n",
      "54 624 train loss: 0.0134128 valid loss: 0.0235699 P@5: 0.22694 N@5: 0.36904 early stop: 0\n",
      "54 1424 train loss: 0.0139063 valid loss: 0.0232359 P@5: 0.22785 N@5: 0.37458 early stop: 0\n",
      "54 2224 train loss: 0.0140874 valid loss: 0.0233136 P@5: 0.22922 N@5: 0.38202 early stop: 0\n",
      "54 3024 train loss: 0.0143373 valid loss: 0.0229678 P@5: 0.22831 N@5: 0.38188 early stop: 0\n",
      "54 3824 train loss: 0.0145078 valid loss: 0.0230911 P@5: 0.23242 N@5: 0.38322 early stop: 0\n",
      "55 680 train loss: 0.0133104 valid loss: 0.0233393 P@5: 0.22877 N@5: 0.37779 early stop: 0\n",
      "55 1480 train loss: 0.0141321 valid loss: 0.0236941 P@5: 0.22283 N@5: 0.36420 early stop: 0\n",
      "55 2280 train loss: 0.0139285 valid loss: 0.0235760 P@5: 0.22922 N@5: 0.38080 early stop: 0\n",
      "55 3080 train loss: 0.0137663 valid loss: 0.0231681 P@5: 0.22329 N@5: 0.37700 early stop: 0\n",
      "55 3880 train loss: 0.0141126 valid loss: 0.0232996 P@5: 0.23059 N@5: 0.38241 early stop: 0\n",
      "56 736 train loss: 0.0133340 valid loss: 0.0237113 P@5: 0.22329 N@5: 0.37020 early stop: 0\n",
      "56 1536 train loss: 0.0134322 valid loss: 0.0234893 P@5: 0.22968 N@5: 0.37905 early stop: 0\n",
      "56 2336 train loss: 0.0139741 valid loss: 0.0235335 P@5: 0.22648 N@5: 0.36629 early stop: 0\n",
      "56 3136 train loss: 0.0133323 valid loss: 0.0233735 P@5: 0.23333 N@5: 0.38525 early stop: 0\n",
      "56 3936 train loss: 0.0141784 valid loss: 0.0238605 P@5: 0.21918 N@5: 0.35800 early stop: 0\n",
      "57 792 train loss: 0.0133879 valid loss: 0.0238259 P@5: 0.23059 N@5: 0.37921 early stop: 0\n",
      "57 1592 train loss: 0.0132477 valid loss: 0.0236981 P@5: 0.23105 N@5: 0.38279 early stop: 0\n",
      "57 2392 train loss: 0.0133537 valid loss: 0.0241037 P@5: 0.21507 N@5: 0.35818 early stop: 0\n",
      "57 3192 train loss: 0.0134160 valid loss: 0.0236990 P@5: 0.23379 N@5: 0.36887 early stop: 0\n",
      "58 48 train loss: 0.0138479 valid loss: 0.0239173 P@5: 0.22511 N@5: 0.36997 early stop: 0\n",
      "58 848 train loss: 0.0123292 valid loss: 0.0236943 P@5: 0.22785 N@5: 0.37507 early stop: 0\n",
      "58 1648 train loss: 0.0135474 valid loss: 0.0237548 P@5: 0.22146 N@5: 0.36721 early stop: 0\n",
      "58 2448 train loss: 0.0133032 valid loss: 0.0239210 P@5: 0.22740 N@5: 0.37604 early stop: 0\n",
      "58 3248 train loss: 0.0134334 valid loss: 0.0241724 P@5: 0.21735 N@5: 0.35516 early stop: 0\n",
      "59 104 train loss: 0.0139246 valid loss: 0.0235794 P@5: 0.23288 N@5: 0.37472 early stop: 0\n",
      "59 904 train loss: 0.0127435 valid loss: 0.0237576 P@5: 0.23105 N@5: 0.38026 early stop: 0\n",
      "59 1704 train loss: 0.0127981 valid loss: 0.0240430 P@5: 0.22511 N@5: 0.37159 early stop: 0\n",
      "59 2504 train loss: 0.0131220 valid loss: 0.0239283 P@5: 0.22877 N@5: 0.38316 early stop: 0\n",
      "59 3304 train loss: 0.0135287 valid loss: 0.0236361 P@5: 0.23151 N@5: 0.38188 early stop: 0\n",
      "60 160 train loss: 0.0128868 valid loss: 0.0239464 P@5: 0.22511 N@5: 0.36596 early stop: 0\n",
      "60 960 train loss: 0.0119097 valid loss: 0.0240463 P@5: 0.23607 N@5: 0.39013 early stop: 0\n",
      "60 1760 train loss: 0.0126889 valid loss: 0.0240453 P@5: 0.23744 N@5: 0.38277 early stop: 0\n",
      "60 2560 train loss: 0.0132741 valid loss: 0.0241867 P@5: 0.22557 N@5: 0.36692 early stop: 0\n",
      "60 3360 train loss: 0.0133948 valid loss: 0.0246125 P@5: 0.21689 N@5: 0.35702 early stop: 0\n",
      "61 216 train loss: 0.0133330 valid loss: 0.0241082 P@5: 0.22648 N@5: 0.37559 early stop: 0\n",
      "61 1016 train loss: 0.0121465 valid loss: 0.0245498 P@5: 0.21598 N@5: 0.35313 early stop: 0\n",
      "61 1816 train loss: 0.0123789 valid loss: 0.0242739 P@5: 0.22877 N@5: 0.37841 early stop: 0\n",
      "61 2616 train loss: 0.0131213 valid loss: 0.0242317 P@5: 0.22557 N@5: 0.37125 early stop: 0\n",
      "61 3416 train loss: 0.0126257 valid loss: 0.0244846 P@5: 0.22055 N@5: 0.37145 early stop: 0\n",
      "62 272 train loss: 0.0125341 valid loss: 0.0242561 P@5: 0.22968 N@5: 0.37821 early stop: 0\n",
      "62 1072 train loss: 0.0120758 valid loss: 0.0242402 P@5: 0.23059 N@5: 0.38200 early stop: 0\n",
      "62 1872 train loss: 0.0125712 valid loss: 0.0244817 P@5: 0.22283 N@5: 0.36621 early stop: 0\n",
      "62 2672 train loss: 0.0129657 valid loss: 0.0239664 P@5: 0.23470 N@5: 0.38635 early stop: 0\n",
      "62 3472 train loss: 0.0128580 valid loss: 0.0245770 P@5: 0.21872 N@5: 0.36197 early stop: 0\n",
      "63 328 train loss: 0.0125660 valid loss: 0.0243675 P@5: 0.22831 N@5: 0.37518 early stop: 0\n",
      "63 1128 train loss: 0.0115522 valid loss: 0.0242283 P@5: 0.22922 N@5: 0.38263 early stop: 0\n",
      "63 1928 train loss: 0.0124280 valid loss: 0.0244596 P@5: 0.23059 N@5: 0.38258 early stop: 0\n",
      "63 2728 train loss: 0.0123853 valid loss: 0.0244761 P@5: 0.22785 N@5: 0.37295 early stop: 0\n",
      "63 3528 train loss: 0.0125194 valid loss: 0.0244112 P@5: 0.22740 N@5: 0.37512 early stop: 0\n",
      "64 384 train loss: 0.0122771 valid loss: 0.0244951 P@5: 0.22009 N@5: 0.36717 early stop: 0\n",
      "64 1184 train loss: 0.0117259 valid loss: 0.0246837 P@5: 0.22740 N@5: 0.37196 early stop: 0\n",
      "64 1984 train loss: 0.0119108 valid loss: 0.0246381 P@5: 0.22557 N@5: 0.37440 early stop: 0\n",
      "64 2784 train loss: 0.0123954 valid loss: 0.0246076 P@5: 0.22511 N@5: 0.37006 early stop: 0\n",
      "64 3584 train loss: 0.0125265 valid loss: 0.0245907 P@5: 0.22557 N@5: 0.36756 early stop: 0\n",
      "65 440 train loss: 0.0119334 valid loss: 0.0245506 P@5: 0.22420 N@5: 0.36965 early stop: 0\n",
      "65 1240 train loss: 0.0119857 valid loss: 0.0251875 P@5: 0.21644 N@5: 0.35598 early stop: 0\n",
      "65 2040 train loss: 0.0116857 valid loss: 0.0245859 P@5: 0.23059 N@5: 0.37659 early stop: 0\n",
      "65 2840 train loss: 0.0118952 valid loss: 0.0246608 P@5: 0.23196 N@5: 0.38300 early stop: 0\n",
      "65 3640 train loss: 0.0124846 valid loss: 0.0247248 P@5: 0.22740 N@5: 0.37281 early stop: 0\n",
      "66 496 train loss: 0.0116577 valid loss: 0.0249065 P@5: 0.22740 N@5: 0.37369 early stop: 0\n",
      "66 1296 train loss: 0.0113463 valid loss: 0.0251884 P@5: 0.22968 N@5: 0.37141 early stop: 0\n",
      "66 2096 train loss: 0.0118667 valid loss: 0.0246719 P@5: 0.23562 N@5: 0.38435 early stop: 0\n",
      "66 2896 train loss: 0.0122268 valid loss: 0.0252486 P@5: 0.22466 N@5: 0.36755 early stop: 0\n",
      "66 3696 train loss: 0.0123934 valid loss: 0.0248006 P@5: 0.22785 N@5: 0.36463 early stop: 0\n",
      "67 552 train loss: 0.0108766 valid loss: 0.0251285 P@5: 0.22648 N@5: 0.37408 early stop: 0\n",
      "67 1352 train loss: 0.0112395 valid loss: 0.0250864 P@5: 0.22877 N@5: 0.38268 early stop: 0\n",
      "67 2152 train loss: 0.0116529 valid loss: 0.0253743 P@5: 0.22648 N@5: 0.37334 early stop: 0\n",
      "67 2952 train loss: 0.0115348 valid loss: 0.0251994 P@5: 0.22466 N@5: 0.36763 early stop: 0\n",
      "67 3752 train loss: 0.0123657 valid loss: 0.0252098 P@5: 0.22374 N@5: 0.35747 early stop: 0\n",
      "68 608 train loss: 0.0110319 valid loss: 0.0254483 P@5: 0.22146 N@5: 0.36508 early stop: 0\n",
      "68 1408 train loss: 0.0108952 valid loss: 0.0258948 P@5: 0.22511 N@5: 0.37267 early stop: 0\n",
      "68 2208 train loss: 0.0118154 valid loss: 0.0251997 P@5: 0.22557 N@5: 0.36840 early stop: 0\n",
      "68 3008 train loss: 0.0117748 valid loss: 0.0247646 P@5: 0.22694 N@5: 0.37654 early stop: 0\n",
      "68 3808 train loss: 0.0118996 valid loss: 0.0255621 P@5: 0.21324 N@5: 0.35031 early stop: 0\n",
      "69 664 train loss: 0.0106220 valid loss: 0.0254464 P@5: 0.23288 N@5: 0.37620 early stop: 0\n",
      "69 1464 train loss: 0.0114616 valid loss: 0.0254283 P@5: 0.22831 N@5: 0.37475 early stop: 0\n",
      "69 2264 train loss: 0.0112146 valid loss: 0.0251828 P@5: 0.23333 N@5: 0.38280 early stop: 0\n",
      "69 3064 train loss: 0.0116088 valid loss: 0.0259219 P@5: 0.23105 N@5: 0.37949 early stop: 0\n",
      "69 3864 train loss: 0.0114256 valid loss: 0.0254342 P@5: 0.22466 N@5: 0.36692 early stop: 0\n",
      "70 720 train loss: 0.0105288 valid loss: 0.0256682 P@5: 0.22557 N@5: 0.37187 early stop: 0\n",
      "70 1520 train loss: 0.0105883 valid loss: 0.0259168 P@5: 0.21963 N@5: 0.35334 early stop: 0\n",
      "70 2320 train loss: 0.0112360 valid loss: 0.0259110 P@5: 0.22511 N@5: 0.36986 early stop: 0\n",
      "70 3120 train loss: 0.0111081 valid loss: 0.0259114 P@5: 0.22968 N@5: 0.36680 early stop: 0\n",
      "70 3920 train loss: 0.0118371 valid loss: 0.0258572 P@5: 0.21872 N@5: 0.35939 early stop: 0\n",
      "71 776 train loss: 0.0104966 valid loss: 0.0255381 P@5: 0.22192 N@5: 0.36987 early stop: 0\n",
      "71 1576 train loss: 0.0101085 valid loss: 0.0259316 P@5: 0.22283 N@5: 0.36909 early stop: 0\n",
      "71 2376 train loss: 0.0108898 valid loss: 0.0257601 P@5: 0.21918 N@5: 0.36858 early stop: 0\n",
      "71 3176 train loss: 0.0113376 valid loss: 0.0259841 P@5: 0.22740 N@5: 0.37677 early stop: 0\n",
      "72 32 train loss: 0.0116447 valid loss: 0.0255966 P@5: 0.23288 N@5: 0.38696 early stop: 0\n",
      "72 832 train loss: 0.0101792 valid loss: 0.0258087 P@5: 0.22055 N@5: 0.36896 early stop: 0\n",
      "72 1632 train loss: 0.0104965 valid loss: 0.0258973 P@5: 0.22922 N@5: 0.37596 early stop: 0\n",
      "72 2432 train loss: 0.0110465 valid loss: 0.0258800 P@5: 0.22877 N@5: 0.37604 early stop: 0\n",
      "72 3232 train loss: 0.0108560 valid loss: 0.0263512 P@5: 0.22603 N@5: 0.37159 early stop: 0\n",
      "73 88 train loss: 0.0105935 valid loss: 0.0260863 P@5: 0.22557 N@5: 0.37585 early stop: 0\n",
      "73 888 train loss: 0.0102366 valid loss: 0.0260469 P@5: 0.22192 N@5: 0.36846 early stop: 0\n",
      "73 1688 train loss: 0.0103501 valid loss: 0.0261383 P@5: 0.22603 N@5: 0.37144 early stop: 0\n",
      "73 2488 train loss: 0.0102649 valid loss: 0.0262090 P@5: 0.22877 N@5: 0.37090 early stop: 0\n",
      "73 3288 train loss: 0.0105369 valid loss: 0.0262971 P@5: 0.22511 N@5: 0.36759 early stop: 0\n",
      "74 144 train loss: 0.0106368 valid loss: 0.0262717 P@5: 0.22374 N@5: 0.36677 early stop: 0\n",
      "74 944 train loss: 0.0094963 valid loss: 0.0268256 P@5: 0.22740 N@5: 0.36964 early stop: 0\n",
      "74 1744 train loss: 0.0102531 valid loss: 0.0260819 P@5: 0.22740 N@5: 0.37428 early stop: 0\n",
      "74 2544 train loss: 0.0105472 valid loss: 0.0269287 P@5: 0.23425 N@5: 0.38163 early stop: 0\n",
      "74 3344 train loss: 0.0104415 valid loss: 0.0262254 P@5: 0.22557 N@5: 0.37043 early stop: 0\n",
      "75 200 train loss: 0.0109434 valid loss: 0.0265453 P@5: 0.23014 N@5: 0.37569 early stop: 0\n",
      "75 1000 train loss: 0.0096246 valid loss: 0.0264610 P@5: 0.22511 N@5: 0.37339 early stop: 0\n",
      "75 1800 train loss: 0.0102720 valid loss: 0.0268094 P@5: 0.21598 N@5: 0.36615 early stop: 0\n",
      "75 2600 train loss: 0.0097924 valid loss: 0.0270305 P@5: 0.21826 N@5: 0.36296 early stop: 0\n",
      "75 3400 train loss: 0.0102306 valid loss: 0.0269035 P@5: 0.22603 N@5: 0.37851 early stop: 0\n",
      "76 256 train loss: 0.0104505 valid loss: 0.0265290 P@5: 0.22466 N@5: 0.37527 early stop: 0\n",
      "76 1056 train loss: 0.0090509 valid loss: 0.0276803 P@5: 0.22192 N@5: 0.36327 early stop: 0\n",
      "76 1856 train loss: 0.0100207 valid loss: 0.0272406 P@5: 0.22009 N@5: 0.36108 early stop: 0\n",
      "76 2656 train loss: 0.0098250 valid loss: 0.0267563 P@5: 0.23105 N@5: 0.38064 early stop: 0\n",
      "76 3456 train loss: 0.0108483 valid loss: 0.0268450 P@5: 0.22877 N@5: 0.37378 early stop: 0\n",
      "77 312 train loss: 0.0096139 valid loss: 0.0265441 P@5: 0.23014 N@5: 0.37560 early stop: 0\n",
      "77 1112 train loss: 0.0091184 valid loss: 0.0269069 P@5: 0.22055 N@5: 0.36878 early stop: 0\n",
      "77 1912 train loss: 0.0097897 valid loss: 0.0277238 P@5: 0.22009 N@5: 0.36714 early stop: 0\n",
      "77 2712 train loss: 0.0101691 valid loss: 0.0272595 P@5: 0.21918 N@5: 0.36563 early stop: 0\n",
      "77 3512 train loss: 0.0101740 valid loss: 0.0273547 P@5: 0.23151 N@5: 0.38000 early stop: 0\n",
      "78 368 train loss: 0.0095436 valid loss: 0.0270901 P@5: 0.22100 N@5: 0.36403 early stop: 0\n",
      "78 1168 train loss: 0.0092548 valid loss: 0.0269755 P@5: 0.23242 N@5: 0.38051 early stop: 0\n",
      "78 1968 train loss: 0.0094883 valid loss: 0.0272743 P@5: 0.22511 N@5: 0.37504 early stop: 0\n",
      "78 2768 train loss: 0.0098178 valid loss: 0.0271240 P@5: 0.22009 N@5: 0.36729 early stop: 0\n",
      "78 3568 train loss: 0.0097549 valid loss: 0.0273556 P@5: 0.22648 N@5: 0.37313 early stop: 0\n",
      "79 424 train loss: 0.0093960 valid loss: 0.0277057 P@5: 0.22009 N@5: 0.36055 early stop: 0\n",
      "79 1224 train loss: 0.0088078 valid loss: 0.0274252 P@5: 0.21872 N@5: 0.37350 early stop: 0\n",
      "79 2024 train loss: 0.0095063 valid loss: 0.0277320 P@5: 0.21872 N@5: 0.35231 early stop: 0\n",
      "79 2824 train loss: 0.0096745 valid loss: 0.0274488 P@5: 0.22009 N@5: 0.37267 early stop: 0\n",
      "79 3624 train loss: 0.0098368 valid loss: 0.0275681 P@5: 0.22694 N@5: 0.36554 early stop: 0\n",
      "80 480 train loss: 0.0091358 valid loss: 0.0278612 P@5: 0.21963 N@5: 0.35842 early stop: 0\n",
      "80 1280 train loss: 0.0090943 valid loss: 0.0284519 P@5: 0.22055 N@5: 0.36166 early stop: 0\n",
      "80 2080 train loss: 0.0090875 valid loss: 0.0276215 P@5: 0.22420 N@5: 0.37390 early stop: 0\n",
      "80 2880 train loss: 0.0092633 valid loss: 0.0280701 P@5: 0.22420 N@5: 0.36934 early stop: 0\n",
      "80 3680 train loss: 0.0094732 valid loss: 0.0282612 P@5: 0.20959 N@5: 0.33805 early stop: 0\n",
      "81 536 train loss: 0.0091039 valid loss: 0.0284699 P@5: 0.21507 N@5: 0.35169 early stop: 0\n",
      "81 1336 train loss: 0.0081280 valid loss: 0.0282477 P@5: 0.21872 N@5: 0.35952 early stop: 0\n",
      "81 2136 train loss: 0.0088369 valid loss: 0.0281539 P@5: 0.22968 N@5: 0.37467 early stop: 0\n",
      "81 2936 train loss: 0.0094865 valid loss: 0.0280432 P@5: 0.22237 N@5: 0.37319 early stop: 0\n",
      "81 3736 train loss: 0.0097049 valid loss: 0.0282232 P@5: 0.21644 N@5: 0.36172 early stop: 0\n",
      "82 592 train loss: 0.0089583 valid loss: 0.0286600 P@5: 0.22283 N@5: 0.37030 early stop: 0\n",
      "82 1392 train loss: 0.0086837 valid loss: 0.0288203 P@5: 0.21826 N@5: 0.35622 early stop: 0\n",
      "82 2192 train loss: 0.0092656 valid loss: 0.0280972 P@5: 0.22466 N@5: 0.37709 early stop: 0\n",
      "82 2992 train loss: 0.0087523 valid loss: 0.0286418 P@5: 0.22511 N@5: 0.36904 early stop: 0\n",
      "82 3792 train loss: 0.0092681 valid loss: 0.0281781 P@5: 0.22511 N@5: 0.37321 early stop: 0\n",
      "83 648 train loss: 0.0083642 valid loss: 0.0290770 P@5: 0.21644 N@5: 0.35455 early stop: 0\n",
      "83 1448 train loss: 0.0087126 valid loss: 0.0287276 P@5: 0.21918 N@5: 0.36174 early stop: 0\n",
      "83 2248 train loss: 0.0086055 valid loss: 0.0288520 P@5: 0.22329 N@5: 0.36757 early stop: 0\n",
      "83 3048 train loss: 0.0085612 valid loss: 0.0290141 P@5: 0.22055 N@5: 0.36599 early stop: 0\n",
      "83 3848 train loss: 0.0089671 valid loss: 0.0291958 P@5: 0.21096 N@5: 0.35164 early stop: 0\n",
      "84 704 train loss: 0.0080026 valid loss: 0.0288872 P@5: 0.22603 N@5: 0.37868 early stop: 0\n",
      "84 1504 train loss: 0.0088386 valid loss: 0.0291824 P@5: 0.22192 N@5: 0.36640 early stop: 0\n",
      "84 2304 train loss: 0.0086343 valid loss: 0.0287414 P@5: 0.22146 N@5: 0.36542 early stop: 0\n",
      "84 3104 train loss: 0.0088233 valid loss: 0.0298519 P@5: 0.21689 N@5: 0.35563 early stop: 0\n",
      "84 3904 train loss: 0.0084989 valid loss: 0.0288521 P@5: 0.22283 N@5: 0.36832 early stop: 0\n",
      "85 760 train loss: 0.0074635 valid loss: 0.0292064 P@5: 0.22146 N@5: 0.36760 early stop: 0\n",
      "85 1560 train loss: 0.0082153 valid loss: 0.0296402 P@5: 0.22237 N@5: 0.35793 early stop: 0\n",
      "85 2360 train loss: 0.0085823 valid loss: 0.0296441 P@5: 0.21187 N@5: 0.35174 early stop: 0\n",
      "85 3160 train loss: 0.0086116 valid loss: 0.0290809 P@5: 0.21918 N@5: 0.36735 early stop: 0\n",
      "86 16 train loss: 0.0088890 valid loss: 0.0298210 P@5: 0.21050 N@5: 0.34851 early stop: 0\n",
      "86 816 train loss: 0.0074849 valid loss: 0.0291641 P@5: 0.22237 N@5: 0.36455 early stop: 0\n",
      "86 1616 train loss: 0.0083907 valid loss: 0.0297473 P@5: 0.21507 N@5: 0.34767 early stop: 0\n",
      "86 2416 train loss: 0.0076880 valid loss: 0.0292421 P@5: 0.22740 N@5: 0.37690 early stop: 0\n",
      "86 3216 train loss: 0.0086978 valid loss: 0.0303062 P@5: 0.21553 N@5: 0.35698 early stop: 0\n",
      "87 72 train loss: 0.0085501 valid loss: 0.0298234 P@5: 0.21963 N@5: 0.35765 early stop: 0\n",
      "87 872 train loss: 0.0075401 valid loss: 0.0294185 P@5: 0.22329 N@5: 0.37014 early stop: 0\n",
      "87 1672 train loss: 0.0079433 valid loss: 0.0295713 P@5: 0.22329 N@5: 0.37285 early stop: 0\n",
      "87 2472 train loss: 0.0079617 valid loss: 0.0299767 P@5: 0.21781 N@5: 0.36569 early stop: 0\n",
      "87 3272 train loss: 0.0080974 valid loss: 0.0297387 P@5: 0.21598 N@5: 0.35370 early stop: 0\n",
      "88 128 train loss: 0.0089912 valid loss: 0.0303920 P@5: 0.21324 N@5: 0.34609 early stop: 0\n",
      "88 928 train loss: 0.0076742 valid loss: 0.0302500 P@5: 0.21553 N@5: 0.35420 early stop: 0\n",
      "88 1728 train loss: 0.0079504 valid loss: 0.0302135 P@5: 0.22009 N@5: 0.36324 early stop: 0\n",
      "88 2528 train loss: 0.0077011 valid loss: 0.0301820 P@5: 0.21918 N@5: 0.35614 early stop: 0\n",
      "88 3328 train loss: 0.0085623 valid loss: 0.0302792 P@5: 0.21507 N@5: 0.35232 early stop: 0\n",
      "89 184 train loss: 0.0079595 valid loss: 0.0300050 P@5: 0.22237 N@5: 0.36137 early stop: 0\n",
      "89 984 train loss: 0.0071570 valid loss: 0.0302009 P@5: 0.21781 N@5: 0.36228 early stop: 0\n",
      "89 1784 train loss: 0.0078312 valid loss: 0.0298903 P@5: 0.22055 N@5: 0.36493 early stop: 0\n",
      "89 2584 train loss: 0.0079218 valid loss: 0.0302779 P@5: 0.22146 N@5: 0.36568 early stop: 0\n",
      "89 3384 train loss: 0.0079226 valid loss: 0.0304623 P@5: 0.21416 N@5: 0.35247 early stop: 0\n",
      "90 240 train loss: 0.0080359 valid loss: 0.0306066 P@5: 0.21689 N@5: 0.35530 early stop: 0\n",
      "90 1040 train loss: 0.0070653 valid loss: 0.0305407 P@5: 0.22283 N@5: 0.36276 early stop: 0\n",
      "90 1840 train loss: 0.0070840 valid loss: 0.0312568 P@5: 0.22283 N@5: 0.36853 early stop: 0\n",
      "90 2640 train loss: 0.0078799 valid loss: 0.0304892 P@5: 0.22237 N@5: 0.36282 early stop: 0\n",
      "90 3440 train loss: 0.0080507 valid loss: 0.0309495 P@5: 0.21553 N@5: 0.35136 early stop: 0\n",
      "91 296 train loss: 0.0074207 valid loss: 0.0303786 P@5: 0.21416 N@5: 0.35762 early stop: 0\n",
      "91 1096 train loss: 0.0069365 valid loss: 0.0308668 P@5: 0.21918 N@5: 0.36116 early stop: 0\n",
      "91 1896 train loss: 0.0071028 valid loss: 0.0316267 P@5: 0.22055 N@5: 0.35992 early stop: 0\n",
      "91 2696 train loss: 0.0077210 valid loss: 0.0318080 P@5: 0.20685 N@5: 0.33367 early stop: 0\n",
      "91 3496 train loss: 0.0076511 valid loss: 0.0309550 P@5: 0.22009 N@5: 0.35865 early stop: 0\n",
      "92 352 train loss: 0.0070373 valid loss: 0.0316762 P@5: 0.21507 N@5: 0.36028 early stop: 0\n",
      "92 1152 train loss: 0.0070247 valid loss: 0.0311094 P@5: 0.21050 N@5: 0.34558 early stop: 0\n",
      "92 1952 train loss: 0.0075302 valid loss: 0.0316160 P@5: 0.21872 N@5: 0.35687 early stop: 0\n",
      "92 2752 train loss: 0.0071162 valid loss: 0.0319841 P@5: 0.21324 N@5: 0.35674 early stop: 0\n",
      "92 3552 train loss: 0.0075409 valid loss: 0.0310939 P@5: 0.21507 N@5: 0.35963 early stop: 0\n",
      "93 408 train loss: 0.0075287 valid loss: 0.0319351 P@5: 0.21644 N@5: 0.35687 early stop: 0\n",
      "93 1208 train loss: 0.0068255 valid loss: 0.0315265 P@5: 0.22009 N@5: 0.36714 early stop: 0\n",
      "93 2008 train loss: 0.0071043 valid loss: 0.0319266 P@5: 0.22009 N@5: 0.36037 early stop: 0\n",
      "93 2808 train loss: 0.0071731 valid loss: 0.0315315 P@5: 0.21918 N@5: 0.36074 early stop: 0\n",
      "93 3608 train loss: 0.0072177 valid loss: 0.0325834 P@5: 0.20502 N@5: 0.34207 early stop: 0\n",
      "94 464 train loss: 0.0066754 valid loss: 0.0319952 P@5: 0.21644 N@5: 0.35511 early stop: 0\n",
      "94 1264 train loss: 0.0064766 valid loss: 0.0324282 P@5: 0.21096 N@5: 0.34688 early stop: 0\n",
      "94 2064 train loss: 0.0064477 valid loss: 0.0324213 P@5: 0.22420 N@5: 0.37533 early stop: 0\n",
      "94 2864 train loss: 0.0073887 valid loss: 0.0320167 P@5: 0.21689 N@5: 0.35415 early stop: 0\n",
      "94 3664 train loss: 0.0079846 valid loss: 0.0319003 P@5: 0.21553 N@5: 0.35704 early stop: 0\n",
      "95 520 train loss: 0.0065070 valid loss: 0.0316504 P@5: 0.21872 N@5: 0.35705 early stop: 0\n",
      "95 1320 train loss: 0.0064093 valid loss: 0.0314868 P@5: 0.21370 N@5: 0.35222 early stop: 0\n",
      "95 2120 train loss: 0.0067792 valid loss: 0.0319691 P@5: 0.21918 N@5: 0.36221 early stop: 0\n",
      "95 2920 train loss: 0.0066629 valid loss: 0.0321876 P@5: 0.22055 N@5: 0.36241 early stop: 0\n",
      "95 3720 train loss: 0.0070227 valid loss: 0.0316613 P@5: 0.22100 N@5: 0.36266 early stop: 0\n",
      "96 576 train loss: 0.0066421 valid loss: 0.0322429 P@5: 0.21644 N@5: 0.35258 early stop: 0\n",
      "96 1376 train loss: 0.0062423 valid loss: 0.0327400 P@5: 0.21689 N@5: 0.35545 early stop: 0\n",
      "96 2176 train loss: 0.0064316 valid loss: 0.0328812 P@5: 0.21461 N@5: 0.35552 early stop: 0\n",
      "96 2976 train loss: 0.0074320 valid loss: 0.0321502 P@5: 0.20822 N@5: 0.33929 early stop: 0\n",
      "96 3776 train loss: 0.0074854 valid loss: 0.0322516 P@5: 0.22329 N@5: 0.36325 early stop: 0\n",
      "97 632 train loss: 0.0060053 valid loss: 0.0332655 P@5: 0.21279 N@5: 0.35012 early stop: 0\n",
      "97 1432 train loss: 0.0062659 valid loss: 0.0332027 P@5: 0.21416 N@5: 0.35144 early stop: 0\n",
      "97 2232 train loss: 0.0062600 valid loss: 0.0329746 P@5: 0.22009 N@5: 0.35466 early stop: 0\n",
      "97 3032 train loss: 0.0067560 valid loss: 0.0338503 P@5: 0.21005 N@5: 0.35007 early stop: 0\n",
      "97 3832 train loss: 0.0069642 valid loss: 0.0333962 P@5: 0.22055 N@5: 0.36198 early stop: 0\n",
      "98 688 train loss: 0.0057148 valid loss: 0.0330624 P@5: 0.21324 N@5: 0.34809 early stop: 0\n",
      "98 1488 train loss: 0.0059522 valid loss: 0.0335339 P@5: 0.21005 N@5: 0.34239 early stop: 0\n",
      "98 2288 train loss: 0.0065072 valid loss: 0.0333099 P@5: 0.21598 N@5: 0.35818 early stop: 0\n",
      "98 3088 train loss: 0.0062602 valid loss: 0.0329570 P@5: 0.21781 N@5: 0.36002 early stop: 0\n",
      "98 3888 train loss: 0.0071536 valid loss: 0.0346433 P@5: 0.20822 N@5: 0.34257 early stop: 0\n",
      "99 744 train loss: 0.0055717 valid loss: 0.0331385 P@5: 0.22557 N@5: 0.36132 early stop: 0\n",
      "99 1544 train loss: 0.0060260 valid loss: 0.0332385 P@5: 0.21872 N@5: 0.36324 early stop: 0\n",
      "99 2344 train loss: 0.0063421 valid loss: 0.0340254 P@5: 0.22055 N@5: 0.35720 early stop: 0\n",
      "99 3144 train loss: 0.0065525 valid loss: 0.0333899 P@5: 0.21781 N@5: 0.36190 early stop: 0\n",
      "99 3944 train loss: 0.0065943 valid loss: 0.0336974 P@5: 0.21553 N@5: 0.35694 early stop: 0\n"
     ]
    }
   ],
   "source": [
    "model.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "val_res = model.predict(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3881278538812785,\n",
       " 0.21552511415525114,\n",
       " 0.14132420091324202,\n",
       " 0.3881278538812785,\n",
       " 0.35693699381598476,\n",
       " 0.4062039630858612]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(val_res[1], val_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4269406392694064,\n",
       " 0.2324200913242009,\n",
       " 0.1506849315068493,\n",
       " 0.4269406392694064,\n",
       " 0.3950904912715572,\n",
       " 0.44634322386189507]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(val_res[1], val_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
