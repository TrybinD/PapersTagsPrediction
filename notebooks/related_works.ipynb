{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/daniil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from navec import Navec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from deepxml.attentionxml import AttentionXML, CorNetAttentionXML\n",
    "from deepxml.data_utils import Tokenizer\n",
    "from deepxml.dataset import MultiLabelDataset\n",
    "from deepxml.evaluation import get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10\n",
    "from deepxml.meshprobenet import MeSHProbeNet, CorNetMeSHProbeNet\n",
    "from deepxml.models import Model\n",
    "from deepxml.xmlcnn import CorNetXMLCNN, XMLCNN\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/habr_posts_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>807711</td>\n",
       "      <td>Kaspersky_Lab</td>\n",
       "      <td>Security Week 2416: уязвимость в серверных мат...</td>\n",
       "      <td>[Блог компании «Лаборатория Касперского», Инфо...</td>\n",
       "      <td>На прошлой неделе исследователи компании Binar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>807709</td>\n",
       "      <td>markshevchenko</td>\n",
       "      <td>Вычислительные выражения: Подробнее про типы-о...</td>\n",
       "      <td>[.NET, Функциональное программирование, F#]</td>\n",
       "      <td>В предыдущем посте мы познакомились с концепци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>807707</td>\n",
       "      <td>ru_vds</td>\n",
       "      <td>Угадай местоположение льдины с арктическим ЦОД...</td>\n",
       "      <td>[Блог компании RUVDS.com, Хостинг, Системное а...</td>\n",
       "      <td>Как вы наверняка знаете, 12 апреля RUVDS успеш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>807705</td>\n",
       "      <td>shaddyk</td>\n",
       "      <td>Запустили проект с НСИС по повышению качества ...</td>\n",
       "      <td>[Блог компании HFLabs, Открытые данные, IT-ком...</td>\n",
       "      <td>НСИС — оператор единой автоматизированной инфо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>807703</td>\n",
       "      <td>VokaMut</td>\n",
       "      <td>Тестируем AI на создании прикладного приложения</td>\n",
       "      <td>[Веб-разработка, Искусственный интеллект, Natu...</td>\n",
       "      <td>Всем привет, я Григорий Тумаков, CTO в Моризо ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id          author                                              title  \\\n",
       "0   807711   Kaspersky_Lab  Security Week 2416: уязвимость в серверных мат...   \n",
       "1   807709  markshevchenko  Вычислительные выражения: Подробнее про типы-о...   \n",
       "2   807707          ru_vds  Угадай местоположение льдины с арктическим ЦОД...   \n",
       "3   807705         shaddyk  Запустили проект с НСИС по повышению качества ...   \n",
       "4   807703         VokaMut    Тестируем AI на создании прикладного приложения   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [Блог компании «Лаборатория Касперского», Инфо...   \n",
       "1        [.NET, Функциональное программирование, F#]   \n",
       "2  [Блог компании RUVDS.com, Хостинг, Системное а...   \n",
       "3  [Блог компании HFLabs, Открытые данные, IT-ком...   \n",
       "4  [Веб-разработка, Искусственный интеллект, Natu...   \n",
       "\n",
       "                                                text  \n",
       "0  На прошлой неделе исследователи компании Binar...  \n",
       "1  В предыдущем посте мы познакомились с концепци...  \n",
       "2  Как вы наверняка знаете, 12 апреля RUVDS успеш...  \n",
       "3  НСИС — оператор единой автоматизированной инфо...  \n",
       "4  Всем привет, я Григорий Тумаков, CTO в Моризо ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(language=\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.build_vocab(train_df[\"text\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting token to id: 100%|██████████| 2800/2800 [00:00<00:00, 6916.21it/s]\n",
      "Converting token to id: 100%|██████████| 700/700 [00:00<00:00, 5612.18it/s]\n",
      "Converting token to id: 100%|██████████| 876/876 [00:00<00:00, 7435.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens = tokenizer(train_df[\"text\"].to_list())\n",
    "val_tokens = tokenizer(val_df[\"text\"].to_list())\n",
    "test_tokens = tokenizer(test_df[\"text\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/programming/PapersTagsPrediction/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['Haskell', 'I2P', 'MongoDB', 'MySQL', 'NestJS', 'Блог компании Garage Eight', 'Блог компании RDP', 'Блог компании SL Soft', 'Блог компании SOFTPOINT', 'Блог компании VAS Experts', 'Блог компании Xeovo VPN', 'Блог компании Леруа Мерлен', 'Блог компании ООО «СМАРТС-Кванттелеком»', 'Блог компании Самолет', 'Блог компании Страховой Дом ВСК', 'Кодобред', 'Медгаджеты'] will be ignored\n",
      "  warnings.warn(\n",
      "/home/daniil/programming/PapersTagsPrediction/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['ASP', 'CGI (графика)', 'IIS', 'Lua', 'MongoDB', 'MySQL', 'UEFI', 'Блог компании DataLine', 'Блог компании Deiteriy Lab', 'Блог компании Garage Eight', 'Блог компании Headz.io', 'Блог компании ITT Solutions', 'Блог компании Monq', 'Блог компании PERCo', 'Блог компании Sapiens solutions', 'Блог компании Start X (EX Антифишинг)', 'Блог компании documentat.io', 'Блог компании Ænix', 'Блог компании АйПиМатика', 'Блог компании ЕАЕ-Консалт', 'Блог компании ИТМО', 'Блог компании Лига Цифровой Экономики', 'Блог компании МосТрансПроект', 'Блог компании Окама', 'Блог компании РТЛабс', 'Блог компании Самолет', 'Блог компании Северсталь', 'Блог компании Ситидрайв', 'Блог компании Сравни', 'Блог компании Страховой Дом ВСК', 'Верстка писем', 'Графические оболочки', 'Медгаджеты', 'Типографика'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "train_labels = mlb.fit_transform(train_df[\"tags\"].to_list())\n",
    "val_labels = mlb.transform(val_df[\"tags\"].to_list())\n",
    "test_labels = mlb.transform(test_df[\"tags\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(MultiLabelDataset(train_tokens, train_labels),\n",
    "                          8, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(MultiLabelDataset(val_tokens, val_labels),\n",
    "                        8, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(MultiLabelDataset(test_tokens, test_labels),\n",
    "                         8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "navec = Navec.load(\"../data/navec_hudlit_v1_12B_500K_300d_100q.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = np.zeros(shape=(len(tokenizer.vocab), 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in tqdm(tokenizer.vocab.items()):\n",
    "    words_embeddings[idx] = navec.get(word, navec[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\"dynamic_pool_length\": 8,\n",
    "                \"bottleneck_dim\": 100,\n",
    "                \"num_filters\": 64,\n",
    "                \"dropout\": 0.5,\n",
    "                \"emb_trainable\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=XMLCNN,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab),\n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.1425434 valid loss: 0.0325206 P@5: 0.08343 N@5: 0.12186 early stop: 0\n",
      "0 1600 train loss: 0.0382701 valid loss: 0.0317664 P@5: 0.06943 N@5: 0.10260 early stop: 0\n",
      "0 2400 train loss: 0.0362993 valid loss: 0.0305024 P@5: 0.08943 N@5: 0.12659 early stop: 0\n",
      "1 400 train loss: 0.0346231 valid loss: 0.0304501 P@5: 0.08086 N@5: 0.12645 early stop: 0\n",
      "1 1200 train loss: 0.0341054 valid loss: 0.0318948 P@5: 0.09229 N@5: 0.13734 early stop: 0\n",
      "1 2000 train loss: 0.0341609 valid loss: 0.0304062 P@5: 0.09371 N@5: 0.13955 early stop: 0\n",
      "1 2800 train loss: 0.0333776 valid loss: 0.0312565 P@5: 0.09400 N@5: 0.15221 early stop: 0\n",
      "2 800 train loss: 0.0330080 valid loss: 0.0301471 P@5: 0.10343 N@5: 0.15320 early stop: 0\n",
      "2 1600 train loss: 0.0326168 valid loss: 0.0294227 P@5: 0.09886 N@5: 0.15501 early stop: 0\n",
      "2 2400 train loss: 0.0320364 valid loss: 0.0296667 P@5: 0.09743 N@5: 0.15105 early stop: 0\n",
      "3 400 train loss: 0.0328258 valid loss: 0.0297506 P@5: 0.10600 N@5: 0.16906 early stop: 0\n",
      "3 1200 train loss: 0.0317927 valid loss: 0.0299372 P@5: 0.10429 N@5: 0.16946 early stop: 0\n",
      "3 2000 train loss: 0.0317563 valid loss: 0.0291671 P@5: 0.11200 N@5: 0.17286 early stop: 0\n",
      "3 2800 train loss: 0.0311177 valid loss: 0.0292213 P@5: 0.10971 N@5: 0.17220 early stop: 0\n",
      "4 800 train loss: 0.0306284 valid loss: 0.0288993 P@5: 0.11514 N@5: 0.18110 early stop: 0\n",
      "4 1600 train loss: 0.0308670 valid loss: 0.0288097 P@5: 0.12000 N@5: 0.19015 early stop: 0\n",
      "4 2400 train loss: 0.0307845 valid loss: 0.0283307 P@5: 0.12057 N@5: 0.18941 early stop: 0\n",
      "5 400 train loss: 0.0308758 valid loss: 0.0289364 P@5: 0.11914 N@5: 0.19759 early stop: 0\n",
      "5 1200 train loss: 0.0301016 valid loss: 0.0280735 P@5: 0.12571 N@5: 0.19972 early stop: 0\n",
      "5 2000 train loss: 0.0295652 valid loss: 0.0279834 P@5: 0.12343 N@5: 0.20511 early stop: 0\n",
      "5 2800 train loss: 0.0308360 valid loss: 0.0278908 P@5: 0.11971 N@5: 0.19933 early stop: 0\n",
      "6 800 train loss: 0.0301902 valid loss: 0.0284295 P@5: 0.12971 N@5: 0.20346 early stop: 0\n",
      "6 1600 train loss: 0.0287887 valid loss: 0.0278949 P@5: 0.13571 N@5: 0.22423 early stop: 0\n",
      "6 2400 train loss: 0.0301806 valid loss: 0.0277788 P@5: 0.13971 N@5: 0.22277 early stop: 0\n",
      "7 400 train loss: 0.0294216 valid loss: 0.0285345 P@5: 0.13200 N@5: 0.21455 early stop: 0\n",
      "7 1200 train loss: 0.0295784 valid loss: 0.0279310 P@5: 0.13286 N@5: 0.21011 early stop: 0\n",
      "7 2000 train loss: 0.0288458 valid loss: 0.0273170 P@5: 0.14886 N@5: 0.24008 early stop: 0\n",
      "7 2800 train loss: 0.0288260 valid loss: 0.0271745 P@5: 0.15286 N@5: 0.24355 early stop: 0\n",
      "8 800 train loss: 0.0281166 valid loss: 0.0267325 P@5: 0.15486 N@5: 0.24208 early stop: 0\n",
      "8 1600 train loss: 0.0282525 valid loss: 0.0266050 P@5: 0.16029 N@5: 0.25580 early stop: 0\n",
      "8 2400 train loss: 0.0284733 valid loss: 0.0265965 P@5: 0.15686 N@5: 0.24648 early stop: 0\n",
      "9 400 train loss: 0.0272023 valid loss: 0.0263185 P@5: 0.17029 N@5: 0.26738 early stop: 0\n",
      "9 1200 train loss: 0.0282239 valid loss: 0.0269447 P@5: 0.16886 N@5: 0.26163 early stop: 0\n",
      "9 2000 train loss: 0.0273207 valid loss: 0.0264286 P@5: 0.17029 N@5: 0.27056 early stop: 0\n",
      "9 2800 train loss: 0.0276603 valid loss: 0.0262595 P@5: 0.17857 N@5: 0.27899 early stop: 0\n",
      "CPU times: user 3min 34s, sys: 446 ms, total: 3min 35s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2465753424657534,\n",
       " 0.16552511415525115,\n",
       " 0.11883561643835616,\n",
       " 0.2465753424657534,\n",
       " 0.2595369389003099,\n",
       " 0.3134625093429662]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    with open(Path(f\"../data/models/{name}.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"XMLCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_935303/1504342484.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df,\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"XMLCNN\"]+metrics+[\"3min 36s\"]+[\"40 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetXMLCNN,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab),\n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0683950 valid loss: 0.0307038 P@5: 0.07000 N@5: 0.10285 early stop: 0\n",
      "0 1600 train loss: 0.0310858 valid loss: 0.0300879 P@5: 0.08714 N@5: 0.12623 early stop: 0\n",
      "0 2400 train loss: 0.0302093 valid loss: 0.0298995 P@5: 0.07486 N@5: 0.11478 early stop: 0\n",
      "1 400 train loss: 0.0299098 valid loss: 0.0295354 P@5: 0.09629 N@5: 0.13964 early stop: 0\n",
      "1 1200 train loss: 0.0298271 valid loss: 0.0290523 P@5: 0.10086 N@5: 0.15748 early stop: 0\n",
      "1 2000 train loss: 0.0292776 valid loss: 0.0285338 P@5: 0.11600 N@5: 0.17862 early stop: 0\n",
      "1 2800 train loss: 0.0289089 valid loss: 0.0281247 P@5: 0.12343 N@5: 0.19264 early stop: 0\n",
      "2 800 train loss: 0.0276769 valid loss: 0.0275592 P@5: 0.14429 N@5: 0.23023 early stop: 0\n",
      "2 1600 train loss: 0.0279593 valid loss: 0.0269707 P@5: 0.15400 N@5: 0.24929 early stop: 0\n",
      "2 2400 train loss: 0.0269973 valid loss: 0.0264142 P@5: 0.15914 N@5: 0.25806 early stop: 0\n",
      "3 400 train loss: 0.0266423 valid loss: 0.0258331 P@5: 0.18343 N@5: 0.29126 early stop: 0\n",
      "3 1200 train loss: 0.0252123 valid loss: 0.0254591 P@5: 0.19086 N@5: 0.29756 early stop: 0\n",
      "3 2000 train loss: 0.0256862 valid loss: 0.0252172 P@5: 0.18914 N@5: 0.30604 early stop: 0\n",
      "3 2800 train loss: 0.0260131 valid loss: 0.0247637 P@5: 0.20286 N@5: 0.32200 early stop: 0\n",
      "4 800 train loss: 0.0239361 valid loss: 0.0243698 P@5: 0.20314 N@5: 0.33017 early stop: 0\n",
      "4 1600 train loss: 0.0240488 valid loss: 0.0244458 P@5: 0.20571 N@5: 0.32891 early stop: 0\n",
      "4 2400 train loss: 0.0240522 valid loss: 0.0242992 P@5: 0.21229 N@5: 0.34207 early stop: 0\n",
      "5 400 train loss: 0.0233198 valid loss: 0.0240704 P@5: 0.21314 N@5: 0.34044 early stop: 0\n",
      "5 1200 train loss: 0.0222717 valid loss: 0.0240602 P@5: 0.21200 N@5: 0.33711 early stop: 0\n",
      "5 2000 train loss: 0.0223328 valid loss: 0.0234780 P@5: 0.22629 N@5: 0.36261 early stop: 0\n",
      "5 2800 train loss: 0.0228160 valid loss: 0.0234478 P@5: 0.21800 N@5: 0.35574 early stop: 0\n",
      "6 800 train loss: 0.0213344 valid loss: 0.0231982 P@5: 0.23457 N@5: 0.37509 early stop: 0\n",
      "6 1600 train loss: 0.0213892 valid loss: 0.0231701 P@5: 0.23000 N@5: 0.36818 early stop: 0\n",
      "6 2400 train loss: 0.0214872 valid loss: 0.0231011 P@5: 0.22857 N@5: 0.37216 early stop: 0\n",
      "7 400 train loss: 0.0205980 valid loss: 0.0229350 P@5: 0.23229 N@5: 0.37636 early stop: 0\n",
      "7 1200 train loss: 0.0205105 valid loss: 0.0228272 P@5: 0.23514 N@5: 0.37752 early stop: 0\n",
      "7 2000 train loss: 0.0194090 valid loss: 0.0226615 P@5: 0.24314 N@5: 0.39027 early stop: 0\n",
      "7 2800 train loss: 0.0204239 valid loss: 0.0228943 P@5: 0.23400 N@5: 0.37615 early stop: 0\n",
      "8 800 train loss: 0.0188849 valid loss: 0.0226585 P@5: 0.24200 N@5: 0.39135 early stop: 0\n",
      "8 1600 train loss: 0.0194662 valid loss: 0.0225679 P@5: 0.24771 N@5: 0.39922 early stop: 0\n",
      "8 2400 train loss: 0.0195410 valid loss: 0.0226939 P@5: 0.24200 N@5: 0.37981 early stop: 0\n",
      "9 400 train loss: 0.0192223 valid loss: 0.0229119 P@5: 0.24114 N@5: 0.38727 early stop: 0\n",
      "9 1200 train loss: 0.0185868 valid loss: 0.0227494 P@5: 0.24314 N@5: 0.38706 early stop: 0\n",
      "9 2000 train loss: 0.0189744 valid loss: 0.0229246 P@5: 0.24486 N@5: 0.39352 early stop: 0\n",
      "9 2800 train loss: 0.0185146 valid loss: 0.0225984 P@5: 0.24857 N@5: 0.39306 early stop: 0\n",
      "CPU times: user 3min 36s, sys: 355 ms, total: 3min 37s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4463470319634703,\n",
       " 0.23493150684931507,\n",
       " 0.154337899543379,\n",
       " 0.4463470319634703,\n",
       " 0.3954260144172659,\n",
       " 0.4465979302296313]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"CorNetXMLCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetXMLCNN\"]+metrics+[\"3min 37s\"] + [\"43 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\"hidden_size\": 300,\n",
    "                \"n_layers\": 2,\n",
    "                \"n_probes\": 5,\n",
    "                \"dropout\": 0.5,\n",
    "                \"emb_trainable\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=MeSHProbeNet,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab),\n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0602205 valid loss: 0.0310268 P@5: 0.06886 N@5: 0.10071 early stop: 0\n",
      "0 1600 train loss: 0.0311347 valid loss: 0.0307127 P@5: 0.07600 N@5: 0.11368 early stop: 0\n",
      "0 2400 train loss: 0.0303034 valid loss: 0.0306265 P@5: 0.05000 N@5: 0.08104 early stop: 0\n",
      "1 400 train loss: 0.0302248 valid loss: 0.0301865 P@5: 0.08314 N@5: 0.12906 early stop: 0\n",
      "1 1200 train loss: 0.0302712 valid loss: 0.0294630 P@5: 0.09943 N@5: 0.14848 early stop: 0\n",
      "1 2000 train loss: 0.0295120 valid loss: 0.0290530 P@5: 0.10571 N@5: 0.16264 early stop: 0\n",
      "1 2800 train loss: 0.0287745 valid loss: 0.0288528 P@5: 0.11000 N@5: 0.16318 early stop: 0\n",
      "2 800 train loss: 0.0282127 valid loss: 0.0283309 P@5: 0.11171 N@5: 0.18130 early stop: 0\n",
      "2 1600 train loss: 0.0649474 valid loss: 0.0280779 P@5: 0.12743 N@5: 0.19698 early stop: 0\n",
      "2 2400 train loss: 0.0272808 valid loss: 0.0277157 P@5: 0.12629 N@5: 0.19919 early stop: 0\n",
      "3 400 train loss: 0.0279123 valid loss: 0.0275076 P@5: 0.13000 N@5: 0.19700 early stop: 0\n",
      "3 1200 train loss: 0.0272449 valid loss: 0.0271044 P@5: 0.13886 N@5: 0.22288 early stop: 0\n",
      "3 2000 train loss: 0.0269994 valid loss: 0.0266189 P@5: 0.15343 N@5: 0.23781 early stop: 0\n",
      "3 2800 train loss: 0.0253933 valid loss: 0.0263184 P@5: 0.16257 N@5: 0.26256 early stop: 0\n",
      "4 800 train loss: 0.0254349 valid loss: 0.0260521 P@5: 0.16657 N@5: 0.26115 early stop: 0\n",
      "4 1600 train loss: 0.0256229 valid loss: 0.0257958 P@5: 0.17286 N@5: 0.27420 early stop: 0\n",
      "4 2400 train loss: 0.0249591 valid loss: 0.0253442 P@5: 0.17714 N@5: 0.28611 early stop: 0\n",
      "5 400 train loss: 0.0250814 valid loss: 0.0252355 P@5: 0.17914 N@5: 0.28831 early stop: 0\n",
      "5 1200 train loss: 0.0248111 valid loss: 0.0250729 P@5: 0.18257 N@5: 0.30056 early stop: 0\n",
      "5 2000 train loss: 0.0243499 valid loss: 0.0249460 P@5: 0.17686 N@5: 0.28387 early stop: 0\n",
      "5 2800 train loss: 0.0237920 valid loss: 0.0245508 P@5: 0.20086 N@5: 0.32712 early stop: 0\n",
      "6 800 train loss: 0.0229036 valid loss: 0.0245009 P@5: 0.19714 N@5: 0.31898 early stop: 0\n",
      "6 1600 train loss: 0.0233262 valid loss: 0.0242229 P@5: 0.20371 N@5: 0.33128 early stop: 0\n",
      "6 2400 train loss: 0.0230447 valid loss: 0.0241654 P@5: 0.20571 N@5: 0.33714 early stop: 0\n",
      "7 400 train loss: 0.0225252 valid loss: 0.0240473 P@5: 0.20571 N@5: 0.34115 early stop: 0\n",
      "7 1200 train loss: 0.0219023 valid loss: 0.0235029 P@5: 0.22057 N@5: 0.35625 early stop: 0\n",
      "7 2000 train loss: 0.0220423 valid loss: 0.0234415 P@5: 0.22286 N@5: 0.36733 early stop: 0\n",
      "7 2800 train loss: 0.0218183 valid loss: 0.0236015 P@5: 0.21314 N@5: 0.34625 early stop: 0\n",
      "8 800 train loss: 0.0209838 valid loss: 0.0234475 P@5: 0.21829 N@5: 0.35472 early stop: 0\n",
      "8 1600 train loss: 0.0207269 valid loss: 0.0227624 P@5: 0.23657 N@5: 0.38829 early stop: 0\n",
      "8 2400 train loss: 0.0209543 valid loss: 0.0229664 P@5: 0.23057 N@5: 0.37708 early stop: 0\n",
      "9 400 train loss: 0.0196579 valid loss: 0.0231100 P@5: 0.23457 N@5: 0.37424 early stop: 0\n",
      "9 1200 train loss: 0.0191644 valid loss: 0.0229273 P@5: 0.23457 N@5: 0.38348 early stop: 0\n",
      "9 2000 train loss: 0.0195371 valid loss: 0.0230281 P@5: 0.22543 N@5: 0.36832 early stop: 0\n",
      "9 2800 train loss: 0.0200028 valid loss: 0.0226610 P@5: 0.24200 N@5: 0.39440 early stop: 0\n",
      "CPU times: user 29min 54s, sys: 9min 56s, total: 39min 51s\n",
      "Wall time: 39min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.408675799086758,\n",
       " 0.22808219178082192,\n",
       " 0.1545662100456621,\n",
       " 0.408675799086758,\n",
       " 0.3824655151875472,\n",
       " 0.4403073827378376]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"MeSHProbeNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"MeSHProbeNet\"]+metrics+[\"39min 53s\"]+[\"313 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetMeSHProbeNet,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab),\n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0549944 valid loss: 0.0309916 P@5: 0.06029 N@5: 0.07631 early stop: 0\n",
      "0 1600 train loss: 0.0314117 valid loss: 0.0305396 P@5: 0.07514 N@5: 0.11204 early stop: 0\n",
      "0 2400 train loss: 0.0308404 valid loss: 0.0303925 P@5: 0.06943 N@5: 0.09779 early stop: 0\n",
      "1 400 train loss: 0.0302049 valid loss: 0.0296810 P@5: 0.09086 N@5: 0.14447 early stop: 0\n",
      "1 1200 train loss: 0.0289069 valid loss: 0.0292488 P@5: 0.11114 N@5: 0.17299 early stop: 0\n",
      "1 2000 train loss: 0.0302046 valid loss: 0.0291146 P@5: 0.09714 N@5: 0.14324 early stop: 0\n",
      "1 2800 train loss: 0.0288426 valid loss: 0.0282682 P@5: 0.11143 N@5: 0.17320 early stop: 0\n",
      "2 800 train loss: 0.0277113 valid loss: 0.0277409 P@5: 0.13086 N@5: 0.20136 early stop: 0\n",
      "2 1600 train loss: 0.0275913 valid loss: 0.0269092 P@5: 0.15600 N@5: 0.24277 early stop: 0\n",
      "2 2400 train loss: 0.0269427 valid loss: 0.0268199 P@5: 0.14657 N@5: 0.22285 early stop: 0\n",
      "3 400 train loss: 0.0269015 valid loss: 0.0266107 P@5: 0.15400 N@5: 0.23918 early stop: 0\n",
      "3 1200 train loss: 0.0254485 valid loss: 0.0258893 P@5: 0.16457 N@5: 0.26539 early stop: 0\n",
      "3 2000 train loss: 0.0255065 valid loss: 0.0255196 P@5: 0.18286 N@5: 0.28319 early stop: 0\n",
      "3 2800 train loss: 0.0250439 valid loss: 0.0251589 P@5: 0.18514 N@5: 0.28826 early stop: 0\n",
      "4 800 train loss: 0.0248003 valid loss: 0.0249478 P@5: 0.18829 N@5: 0.29449 early stop: 0\n",
      "4 1600 train loss: 0.0242089 valid loss: 0.0243293 P@5: 0.19514 N@5: 0.31298 early stop: 0\n",
      "4 2400 train loss: 0.0235500 valid loss: 0.0241298 P@5: 0.20600 N@5: 0.33811 early stop: 0\n",
      "5 400 train loss: 0.0228553 valid loss: 0.0239762 P@5: 0.20514 N@5: 0.33759 early stop: 0\n",
      "5 1200 train loss: 0.0219525 valid loss: 0.0235533 P@5: 0.21743 N@5: 0.35540 early stop: 0\n",
      "5 2000 train loss: 0.0230426 valid loss: 0.0233977 P@5: 0.22086 N@5: 0.35801 early stop: 0\n",
      "5 2800 train loss: 0.0220562 valid loss: 0.0232106 P@5: 0.22571 N@5: 0.37457 early stop: 0\n",
      "6 800 train loss: 0.0208445 valid loss: 0.0231917 P@5: 0.23086 N@5: 0.37032 early stop: 0\n",
      "6 1600 train loss: 0.0206306 valid loss: 0.0228503 P@5: 0.23657 N@5: 0.38876 early stop: 0\n",
      "6 2400 train loss: 0.0204745 valid loss: 0.0224718 P@5: 0.24257 N@5: 0.39986 early stop: 0\n",
      "7 400 train loss: 0.0195688 valid loss: 0.0230063 P@5: 0.23714 N@5: 0.38645 early stop: 0\n",
      "7 1200 train loss: 0.0190012 valid loss: 0.0225874 P@5: 0.24429 N@5: 0.40104 early stop: 0\n",
      "7 2000 train loss: 0.0195405 valid loss: 0.0221708 P@5: 0.25229 N@5: 0.40618 early stop: 0\n",
      "7 2800 train loss: 0.0190793 valid loss: 0.0221101 P@5: 0.25143 N@5: 0.41004 early stop: 0\n",
      "8 800 train loss: 0.0166996 valid loss: 0.0228072 P@5: 0.24657 N@5: 0.40017 early stop: 0\n",
      "8 1600 train loss: 0.0172764 valid loss: 0.0225796 P@5: 0.24486 N@5: 0.39729 early stop: 0\n",
      "8 2400 train loss: 0.0168309 valid loss: 0.0220018 P@5: 0.25486 N@5: 0.41379 early stop: 0\n",
      "9 400 train loss: 0.0157057 valid loss: 0.0230567 P@5: 0.25371 N@5: 0.41367 early stop: 0\n",
      "9 1200 train loss: 0.0148762 valid loss: 0.0229635 P@5: 0.25657 N@5: 0.41437 early stop: 0\n",
      "9 2000 train loss: 0.0146771 valid loss: 0.0226251 P@5: 0.25429 N@5: 0.41387 early stop: 0\n",
      "9 2800 train loss: 0.0158395 valid loss: 0.0225300 P@5: 0.26257 N@5: 0.42923 early stop: 0\n",
      "CPU times: user 19min 4s, sys: 7min 4s, total: 26min 8s\n",
      "Wall time: 26min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4520547945205479,\n",
       " 0.24885844748858446,\n",
       " 0.1636986301369863,\n",
       " 0.4520547945205479,\n",
       " 0.4144862814755454,\n",
       " 0.4701097685721493]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"CorNetMeSHProbeNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetMeSHProbeNet\"]+metrics+[\"26min 9s\"]+[\"317 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\"hidden_size\": 256,\n",
    "                \"layers_num\": 1,\n",
    "                \"linear_size\": [256],\n",
    "                \"dropout\": 0.5,\n",
    "                \"emb_trainable\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=AttentionXML,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab),\n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0571790 valid loss: 0.0304442 P@5: 0.07829 N@5: 0.11631 early stop: 0\n",
      "0 1600 train loss: 0.0301680 valid loss: 0.0299156 P@5: 0.07600 N@5: 0.11359 early stop: 0\n",
      "0 2400 train loss: 0.0298240 valid loss: 0.0296295 P@5: 0.08429 N@5: 0.12316 early stop: 0\n",
      "1 400 train loss: 0.0291400 valid loss: 0.0297673 P@5: 0.09086 N@5: 0.13023 early stop: 0\n",
      "1 1200 train loss: 0.0295991 valid loss: 0.0291480 P@5: 0.10257 N@5: 0.14387 early stop: 0\n",
      "1 2000 train loss: 0.0289475 valid loss: 0.0287991 P@5: 0.11286 N@5: 0.16845 early stop: 0\n",
      "1 2800 train loss: 0.0280205 valid loss: 0.0280255 P@5: 0.11914 N@5: 0.19280 early stop: 0\n",
      "2 800 train loss: 0.0272995 valid loss: 0.0275432 P@5: 0.13571 N@5: 0.21224 early stop: 0\n",
      "2 1600 train loss: 0.0274761 valid loss: 0.0269058 P@5: 0.15000 N@5: 0.23798 early stop: 0\n",
      "2 2400 train loss: 0.0263523 valid loss: 0.0264282 P@5: 0.16143 N@5: 0.25232 early stop: 0\n",
      "3 400 train loss: 0.0263287 valid loss: 0.0266501 P@5: 0.17571 N@5: 0.26852 early stop: 0\n",
      "3 1200 train loss: 0.0247666 valid loss: 0.0257849 P@5: 0.17371 N@5: 0.27399 early stop: 0\n",
      "3 2000 train loss: 0.0251389 valid loss: 0.0259587 P@5: 0.17743 N@5: 0.28857 early stop: 0\n",
      "3 2800 train loss: 0.0250312 valid loss: 0.0250998 P@5: 0.19514 N@5: 0.31955 early stop: 0\n",
      "4 800 train loss: 0.0239283 valid loss: 0.0250596 P@5: 0.20400 N@5: 0.32694 early stop: 0\n",
      "4 1600 train loss: 0.0239669 valid loss: 0.0245639 P@5: 0.20571 N@5: 0.33359 early stop: 0\n",
      "4 2400 train loss: 0.0232014 valid loss: 0.0244298 P@5: 0.21057 N@5: 0.33512 early stop: 0\n",
      "5 400 train loss: 0.0222724 valid loss: 0.0240690 P@5: 0.22000 N@5: 0.35694 early stop: 0\n",
      "5 1200 train loss: 0.0226393 valid loss: 0.0236527 P@5: 0.22114 N@5: 0.35756 early stop: 0\n",
      "5 2000 train loss: 0.0217103 valid loss: 0.0238003 P@5: 0.22886 N@5: 0.36859 early stop: 0\n",
      "5 2800 train loss: 0.0224452 valid loss: 0.0236422 P@5: 0.24029 N@5: 0.38279 early stop: 0\n",
      "6 800 train loss: 0.0202905 valid loss: 0.0237666 P@5: 0.23543 N@5: 0.38139 early stop: 0\n",
      "6 1600 train loss: 0.0206239 valid loss: 0.0241624 P@5: 0.23429 N@5: 0.38544 early stop: 0\n",
      "6 2400 train loss: 0.0210495 valid loss: 0.0232204 P@5: 0.24800 N@5: 0.39934 early stop: 0\n",
      "7 400 train loss: 0.0198545 valid loss: 0.0233229 P@5: 0.24371 N@5: 0.39026 early stop: 0\n",
      "7 1200 train loss: 0.0195552 valid loss: 0.0237678 P@5: 0.25171 N@5: 0.40716 early stop: 0\n",
      "7 2000 train loss: 0.0191607 valid loss: 0.0237379 P@5: 0.24914 N@5: 0.41030 early stop: 0\n",
      "7 2800 train loss: 0.0191670 valid loss: 0.0228855 P@5: 0.24914 N@5: 0.40548 early stop: 0\n",
      "8 800 train loss: 0.0173439 valid loss: 0.0233348 P@5: 0.24600 N@5: 0.40381 early stop: 0\n",
      "8 1600 train loss: 0.0175719 valid loss: 0.0231161 P@5: 0.25857 N@5: 0.41526 early stop: 0\n",
      "8 2400 train loss: 0.0184333 valid loss: 0.0230380 P@5: 0.26257 N@5: 0.42703 early stop: 0\n",
      "9 400 train loss: 0.0173142 valid loss: 0.0245319 P@5: 0.26029 N@5: 0.42981 early stop: 0\n",
      "9 1200 train loss: 0.0162946 valid loss: 0.0240413 P@5: 0.24829 N@5: 0.41102 early stop: 0\n",
      "9 2000 train loss: 0.0166701 valid loss: 0.0236068 P@5: 0.26257 N@5: 0.42820 early stop: 0\n",
      "9 2800 train loss: 0.0165514 valid loss: 0.0235241 P@5: 0.26314 N@5: 0.43418 early stop: 0\n",
      "CPU times: user 8min 36s, sys: 2min 10s, total: 10min 46s\n",
      "Wall time: 10min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4737442922374429,\n",
       " 0.24794520547945206,\n",
       " 0.1552511415525114,\n",
       " 0.4737442922374429,\n",
       " 0.4208846112050576,\n",
       " 0.46421247403766547]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"AttentionXML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"AttentionXML\"]+metrics+[\"10min 47s\"]+[\"84 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(network=CorNetAttentionXML,\n",
    "              **model_config, labels_num=train_labels.shape[1], emb_size=300, vocab_size=len(tokenizer.vocab), \n",
    "              emb_init=words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 800 train loss: 0.0528414 valid loss: 0.0304479 P@5: 0.07829 N@5: 0.11598 early stop: 0\n",
      "0 1600 train loss: 0.0301051 valid loss: 0.0297904 P@5: 0.08000 N@5: 0.11930 early stop: 0\n",
      "0 2400 train loss: 0.0303168 valid loss: 0.0295530 P@5: 0.08657 N@5: 0.12231 early stop: 0\n",
      "1 400 train loss: 0.0292458 valid loss: 0.0293464 P@5: 0.09743 N@5: 0.13797 early stop: 0\n",
      "1 1200 train loss: 0.0288400 valid loss: 0.0288343 P@5: 0.10886 N@5: 0.16221 early stop: 0\n",
      "1 2000 train loss: 0.0291567 valid loss: 0.0285651 P@5: 0.11371 N@5: 0.17429 early stop: 0\n",
      "1 2800 train loss: 0.0274044 valid loss: 0.0278816 P@5: 0.13200 N@5: 0.20033 early stop: 0\n",
      "2 800 train loss: 0.0272165 valid loss: 0.0274133 P@5: 0.14229 N@5: 0.22571 early stop: 0\n",
      "2 1600 train loss: 0.0268046 valid loss: 0.0268896 P@5: 0.15143 N@5: 0.23941 early stop: 0\n",
      "2 2400 train loss: 0.0263644 valid loss: 0.0267172 P@5: 0.15029 N@5: 0.23674 early stop: 0\n",
      "3 400 train loss: 0.0263225 valid loss: 0.0260991 P@5: 0.16800 N@5: 0.26342 early stop: 0\n",
      "3 1200 train loss: 0.0243771 valid loss: 0.0256168 P@5: 0.17657 N@5: 0.28155 early stop: 0\n",
      "3 2000 train loss: 0.0255318 valid loss: 0.0252472 P@5: 0.18971 N@5: 0.30049 early stop: 0\n",
      "3 2800 train loss: 0.0246922 valid loss: 0.0247860 P@5: 0.20000 N@5: 0.31716 early stop: 0\n",
      "4 800 train loss: 0.0228194 valid loss: 0.0246630 P@5: 0.20457 N@5: 0.33146 early stop: 0\n",
      "4 1600 train loss: 0.0235033 valid loss: 0.0243235 P@5: 0.21543 N@5: 0.34491 early stop: 0\n",
      "4 2400 train loss: 0.0238964 valid loss: 0.0242070 P@5: 0.21457 N@5: 0.34684 early stop: 0\n",
      "5 400 train loss: 0.0225397 valid loss: 0.0238036 P@5: 0.21686 N@5: 0.35522 early stop: 0\n",
      "5 1200 train loss: 0.0218977 valid loss: 0.0238207 P@5: 0.22371 N@5: 0.36125 early stop: 0\n",
      "5 2000 train loss: 0.0214043 valid loss: 0.0232933 P@5: 0.23714 N@5: 0.37953 early stop: 0\n",
      "5 2800 train loss: 0.0216215 valid loss: 0.0235874 P@5: 0.23200 N@5: 0.37518 early stop: 0\n",
      "6 800 train loss: 0.0201443 valid loss: 0.0230256 P@5: 0.24571 N@5: 0.40189 early stop: 0\n",
      "6 1600 train loss: 0.0196261 valid loss: 0.0231244 P@5: 0.24057 N@5: 0.39254 early stop: 0\n",
      "6 2400 train loss: 0.0205204 valid loss: 0.0226781 P@5: 0.24857 N@5: 0.41418 early stop: 0\n",
      "7 400 train loss: 0.0191545 valid loss: 0.0229869 P@5: 0.24714 N@5: 0.40636 early stop: 0\n",
      "7 1200 train loss: 0.0178755 valid loss: 0.0231136 P@5: 0.25657 N@5: 0.42057 early stop: 0\n",
      "7 2000 train loss: 0.0187902 valid loss: 0.0227423 P@5: 0.25886 N@5: 0.41449 early stop: 0\n",
      "7 2800 train loss: 0.0185820 valid loss: 0.0228188 P@5: 0.25829 N@5: 0.42148 early stop: 0\n",
      "8 800 train loss: 0.0165187 valid loss: 0.0231068 P@5: 0.25286 N@5: 0.40875 early stop: 0\n",
      "8 1600 train loss: 0.0163792 valid loss: 0.0234666 P@5: 0.24800 N@5: 0.40371 early stop: 0\n",
      "8 2400 train loss: 0.0168873 valid loss: 0.0227729 P@5: 0.25886 N@5: 0.42322 early stop: 0\n",
      "9 400 train loss: 0.0159303 valid loss: 0.0236439 P@5: 0.26143 N@5: 0.42966 early stop: 0\n",
      "9 1200 train loss: 0.0149755 valid loss: 0.0239414 P@5: 0.25943 N@5: 0.41639 early stop: 0\n",
      "9 2000 train loss: 0.0149733 valid loss: 0.0243321 P@5: 0.26457 N@5: 0.43169 early stop: 0\n",
      "9 2800 train loss: 0.0157102 valid loss: 0.0232951 P@5: 0.25857 N@5: 0.42456 early stop: 0\n",
      "CPU times: user 8min 37s, sys: 2min 10s, total: 10min 47s\n",
      "Wall time: 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_loader, val_loader, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    }
   ],
   "source": [
    "test_res = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4452054794520548,\n",
       " 0.2458904109589041,\n",
       " 0.15924657534246575,\n",
       " 0.4452054794520548,\n",
       " 0.40768301065668583,\n",
       " 0.4597818721359381]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [metric(test_res[1], test_labels) for metric in [get_p_1, get_p_5, get_p_10, get_n_1, get_n_5, get_n_10]]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"CorNetAttentionXML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, \n",
    "                        pd.DataFrame([[\"CorNetAttentionXML\"]+metrics+[\"10min 48s\"]+[\"88 Mb\"]], \n",
    "                                     columns=[\"model_name\", \"P@1\", \"P@5\", \"P@10\", \"N@1\", \"N@5\", \"N@10\", \"time\", \"size\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@10</th>\n",
       "      <th>N@1</th>\n",
       "      <th>N@5</th>\n",
       "      <th>N@10</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XMLCNN</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.165525</td>\n",
       "      <td>0.118836</td>\n",
       "      <td>0.246575</td>\n",
       "      <td>0.259537</td>\n",
       "      <td>0.313463</td>\n",
       "      <td>3min 36s</td>\n",
       "      <td>40 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CorNetXMLCNN</td>\n",
       "      <td>0.446347</td>\n",
       "      <td>0.234932</td>\n",
       "      <td>0.154338</td>\n",
       "      <td>0.446347</td>\n",
       "      <td>0.395426</td>\n",
       "      <td>0.446598</td>\n",
       "      <td>2min 21s</td>\n",
       "      <td>43 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MeSHProbeNet</td>\n",
       "      <td>0.408676</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.154566</td>\n",
       "      <td>0.408676</td>\n",
       "      <td>0.382466</td>\n",
       "      <td>0.440307</td>\n",
       "      <td>39min 53s</td>\n",
       "      <td>313 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CorNetMeSHProbeNet</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.248858</td>\n",
       "      <td>0.163699</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.414486</td>\n",
       "      <td>0.470110</td>\n",
       "      <td>26min 9s</td>\n",
       "      <td>317 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AttentionXML</td>\n",
       "      <td>0.473744</td>\n",
       "      <td>0.247945</td>\n",
       "      <td>0.155251</td>\n",
       "      <td>0.473744</td>\n",
       "      <td>0.420885</td>\n",
       "      <td>0.464212</td>\n",
       "      <td>10min 47s</td>\n",
       "      <td>84 Mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CorNetAttentionXML</td>\n",
       "      <td>0.445205</td>\n",
       "      <td>0.245890</td>\n",
       "      <td>0.159247</td>\n",
       "      <td>0.445205</td>\n",
       "      <td>0.407683</td>\n",
       "      <td>0.459782</td>\n",
       "      <td>10min 48s</td>\n",
       "      <td>88 Mb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name       P@1       P@5      P@10       N@1       N@5  \\\n",
       "0              XMLCNN  0.246575  0.165525  0.118836  0.246575  0.259537   \n",
       "1        CorNetXMLCNN  0.446347  0.234932  0.154338  0.446347  0.395426   \n",
       "2        MeSHProbeNet  0.408676  0.228082  0.154566  0.408676  0.382466   \n",
       "3  CorNetMeSHProbeNet  0.452055  0.248858  0.163699  0.452055  0.414486   \n",
       "4        AttentionXML  0.473744  0.247945  0.155251  0.473744  0.420885   \n",
       "5  CorNetAttentionXML  0.445205  0.245890  0.159247  0.445205  0.407683   \n",
       "\n",
       "       N@10       time    size  \n",
       "0  0.313463   3min 36s   40 Mb  \n",
       "1  0.446598   2min 21s   43 Mb  \n",
       "2  0.440307  39min 53s  313 Mb  \n",
       "3  0.470110   26min 9s  317 Mb  \n",
       "4  0.464212  10min 47s   84 Mb  \n",
       "5  0.459782  10min 48s   88 Mb  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
